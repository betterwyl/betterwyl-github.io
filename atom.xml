<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Betterwyl</title>
  
  <subtitle>Be patient, brave and honesty</subtitle>
  <link href="https://betterwyl.github.io/atom.xml" rel="self"/>
  
  <link href="https://betterwyl.github.io/"/>
  <updated>2023-04-30T07:14:06.273Z</updated>
  <id>https://betterwyl.github.io/</id>
  
  <author>
    <name>wyl</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper reading</title>
    <link href="https://betterwyl.github.io/2023/12/16/radarwork/"/>
    <id>https://betterwyl.github.io/2023/12/16/radarwork/</id>
    <published>2023-12-16T05:45:43.000Z</published>
    <updated>2023-04-30T07:14:06.273Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Graph-Convolutional-Networks-for-3D-Object-Detection-on-Radar-Data【2021ICCVW】"><a href="#Graph-Convolutional-Networks-for-3D-Object-Detection-on-Radar-Data【2021ICCVW】" class="headerlink" title="Graph Convolutional Networks for 3D Object Detection on Radar Data【2021ICCVW】"></a>Graph Convolutional Networks for 3D Object Detection on Radar Data【2021ICCVW】</h2><p>核心思想：radar data →GNN </p><p>创新点：</p><p>1 state-of-the-art 使用GNN 提高了10%</p><p>2 第一个在距离超过50米至100米的radar data上评估</p><p>Related work：</p><p>1文章说在雷达张量上回归2D或3D对象领域的工作相当稀少。【？】</p><p>2 图在这方面的应用很少 但是图研究本身很火热。</p><p>使用FMCW Radar</p><p>使用恒虚警率（CFAR）算法从该张量中提取点云</p><p>极坐标→笛卡尔坐标转换【Vehicle Detection With Automotive Radar Using Deep Learning on</p><p>Range-Azimuth-Doppler Tensors 这篇文章也是基于这个方法的，但是表明了有一个问题：网络开始时将range-beam-Doppler tensor转换为笛卡尔会导致目标检测性能的显著降低。】</p><p>极坐标雷达数据如果直接输入也无法获得关于不同距离的单元之间各种不同笛卡尔距离的信息。</p><p><strong>Method</strong></p><p>实验baseline RT-Net：</p><p>input：polar range-beam-Doppler tensor </p><p>backbone：【ResNet】</p><p>关于节点和边的设置：</p><p><img src="/2023/12/16/radarwork/image-20221201215913364.png" alt="image-20221201215913364"></p><p>设计的GTR-Net：</p><p>两个连续GCN卷积层 输出转换回三维张量并使用金字塔ResNet进一步处理</p><p>【权重设置了对照 ：①全部设置1；②根据笛卡尔空间的欧式距离计算权重】</p><p><img src="/2023/12/16/radarwork/image-20221201215931855.png" alt="image-20221201215931855"></p><p>数据集是自采的，简单、中等、困难这个等级也是作者自己分类的</p><p><img src="/2023/12/16/radarwork/image-20221201215949505.png" alt="image-20221201215949505"></p><h2 id="Radar-PointGNN-Graph-Based-Object-Recognition-for-Unstructured-Radar-Point-cloud-Data【2021IEEE-RadarConference】荷兰-代尔夫特理工大学"><a href="#Radar-PointGNN-Graph-Based-Object-Recognition-for-Unstructured-Radar-Point-cloud-Data【2021IEEE-RadarConference】荷兰-代尔夫特理工大学" class="headerlink" title="Radar-PointGNN: Graph Based Object Recognition for Unstructured Radar Point-cloud Data【2021IEEE RadarConference】荷兰 代尔夫特理工大学"></a>Radar-PointGNN: Graph Based Object Recognition for Unstructured Radar Point-cloud Data【2021IEEE RadarConference】荷兰 代尔夫特理工大学</h2><p>数据集： nuScenes </p><p>创新点：</p><p>1 在公共数据集nuScenes上评估的雷达数据的第一个已发布的多目标识别方法。</p><p>2 Radar domain GCN </p><p><strong>Method</strong>：</p><p><strong>预处理：</strong></p><p><strong>建图</strong>【顶点：radar detection points 边：点距离小于1m  感觉建图这边是可以优化的地方】</p><p> <img src="/2023/12/16/radarwork/image-20221201220035671.png" alt="image-20221201220035671">    </p><p>有点类似于<strong>数据增强</strong>的处理：平移和旋转前五个雷达帧的雷达测量值，增加输入点云的密度。</p><p><img src="/2023/12/16/radarwork/image-20221201220044846.png" alt="image-20221201220044846"></p><p><strong>预测decoder</strong>:</p><p><strong>Loss</strong>：</p><p>分类损失=杂波预测任务和类别预测任务的交叉熵损失之和</p><p>定位损失【只计算有正确分的】=标量预测的Huber损失和定向预测的交叉熵</p><p>添加一个正则项L2</p><p>NMS：他说很少发现相交的，因此把阈值设置为0.01。【？】</p><h2 id="Improved-Orientation-Estimation-and-Detection-with-Hybrid-Object-Detection-Networks-for-Automotive-Radar"><a href="#Improved-Orientation-Estimation-and-Detection-with-Hybrid-Object-Detection-Networks-for-Automotive-Radar" class="headerlink" title="Improved Orientation Estimation and Detection with Hybrid Object Detection Networks for Automotive Radar"></a>Improved Orientation Estimation and Detection with Hybrid Object Detection Networks for Automotive Radar</h2><p>创新点：</p><p>将点邻域的特征学习与基于网格的雷达目标检测网络相结合的空白</p><p>核心部分：</p><p>Point Feature Extractor </p><p>GraphPillars：GNN思想。Form a graph：1m or 2m neighborhood </p><p>KPConvs:【这是CVPR2019的工作】</p><p>把Resnet中的2D卷积换成KPConv【去看一下】</p><p>Grid Render</p><p>PointPillars的思想。投影到笛卡尔BEV网格上，其单元格包含这些点的特征。当多个点落在同一个单元中时，这些点的特征向量使用均值池聚合。</p><p>Backbone：Resnet+FPN</p><p>Detection Head：CNN</p><p>我的想法：这篇文章使用了KPConv可变形卷积和GNN的思想，和之前看过的3DGCN的有异曲同工之妙，处理这种稀疏性数据是比固定卷积更有灵活性的。在Grid部分用了PointPillars。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>My note on OpenPCDet&amp;MMDet3D</title>
    <link href="https://betterwyl.github.io/2023/04/30/openpcdet/"/>
    <id>https://betterwyl.github.io/2023/04/30/openpcdet/</id>
    <published>2023-04-30T07:45:43.000Z</published>
    <updated>2023-04-30T07:12:02.786Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="OpenPCDet"><a href="#OpenPCDet" class="headerlink" title="OpenPCDet"></a>OpenPCDet</h1><p>在PCDet中搭建3D目标检测框架只需要写config文件将所需模块定义清楚，然后PCDet将自动根据模块间的拓扑顺序组合为3D目标检测框架，来进行训练和测试</p><p><img src="/2023/04/30/openpcdet/image-20230426220506879.png" alt="STRUCTURE"></p><p>数据处理流程</p><h3 id="Dataset-Preparation-以KITTI数据集为例："><a href="#Dataset-Preparation-以KITTI数据集为例：" class="headerlink" title="Dataset Preparation  以KITTI数据集为例："></a>Dataset Preparation  以KITTI数据集为例：</h3><p>更改数据集配置文件： tools/cfgs/dataset_configs/kitti_dataset.yaml</p><p>生成the data infos：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml</span><br></pre></td></tr></table></figure><h3 id="Pretrained-Models"><a href="#Pretrained-Models" class="headerlink" title="Pretrained Models"></a>Pretrained Models</h3><h3 id="Training-amp-Testing"><a href="#Training-amp-Testing" class="headerlink" title="Training &amp; Testing"></a>Training &amp; Testing</h3><p>使用预训练模型测试和使用多GPUs测试</p><ul><li><p>要测试特定训练设置的所有已保存检查点并在 Tensorboard 上绘制性能曲线，请添加参数：<code>--eval_all</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125; --eval_all</span><br></pre></td></tr></table></figure></li><li><p>要使用多个 GPU 进行测试，要执行以下操作：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sh scripts&#x2F;dist_test.sh $&#123;NUM_GPUS&#125; \</span><br><span class="line">    --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125;</span><br><span class="line"></span><br><span class="line"># or</span><br><span class="line"></span><br><span class="line">sh scripts&#x2F;slurm_test_mgpu.sh $&#123;PARTITION&#125; $&#123;NUM_GPUS&#125; \</span><br><span class="line">    --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125;</span><br></pre></td></tr></table></figure><p>使用多GPUs训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sh scripts&#x2F;dist_train.sh $&#123;NUM_GPUS&#125; --cfg_file $&#123;CONFIG_FILE&#125;</span><br><span class="line"></span><br><span class="line"># or </span><br><span class="line"></span><br><span class="line">sh scripts&#x2F;slurm_train.sh $&#123;PARTITION&#125; $&#123;JOB_NAME&#125; $&#123;NUM_GPUS&#125; --cfg_file $&#123;CONFIG_FILE&#125;</span><br></pre></td></tr></table></figure><h3 id="以VOD数据集为例子PP-Radar复现"><a href="#以VOD数据集为例子PP-Radar复现" class="headerlink" title="以VOD数据集为例子PP-Radar复现"></a>以VOD数据集为例子PP-Radar复现</h3><p>按照openPCDet框架中数据集中Kitti的格式，生成XXXXinfo.pkl 文件</p><p>需要更改两个cfg文件：一个是数据集的cfg文件，一个是模型的cfg文件【采用的是PointPillar Config】。</p><p>在PillarVFE需要进行更改：原因是因为在点云中没有RCS和Doppler特征。</p><p>在tensorboard进行查看：</p><p><img src="/2023/04/30/openpcdet/image-20230430150428353.png" alt="可视化"></p><p>结果：</p><p><img src="/2023/04/30/openpcdet/image-20230430150323635.png" alt="result"></p><h1 id="MMDet3D"><a href="#MMDet3D" class="headerlink" title="MMDet3D"></a>MMDet3D</h1><h2 id="work-flow"><a href="#work-flow" class="headerlink" title="work flow"></a>work flow</h2><p><img src="/2023/04/30/openpcdet/v2-7ecc8e5e19c59a3e6682c5e3cdc34918_r.jpg" alt="workflow"></p><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p> stream: batch →  backbone → neck → head [cls + reg]→  gt bbox encoder→ loss</p><h3 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h3><p>路径：mmdetection3d/mmdet3d/models/backbones</p><p>对backbone进行扩展，可以继承上述网络，然后通过注册器机制注册使用。通过 MMCV 中的注册器机制，你可以通过 dict 形式的配置来实例化任何已经注册的类</p><h3 id="LEARN-ABOUT-CONFIGS"><a href="#LEARN-ABOUT-CONFIGS" class="headerlink" title="LEARN  ABOUT  CONFIGS"></a>LEARN  ABOUT  CONFIGS</h3><p>路径：mmdetection3d/configs/_base_</p><h3 id="把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集-dataset-，模型-model-，训练策略-schedule：Optimization-config-和运行时的默认设置-default-runtime：Hook-config"><a href="#把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集-dataset-，模型-model-，训练策略-schedule：Optimization-config-和运行时的默认设置-default-runtime：Hook-config" class="headerlink" title="把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集 (dataset)，模型 (model)，训练策略 (schedule：Optimization config) 和运行时的默认设置 (default runtime：Hook config)"></a>把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集 (dataset)，模型 (model)，训练策略 (schedule：Optimization config) 和运行时的默认设置 (default runtime：Hook config)</h3><p>configs文件的命名风格：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;model&#125;_[model setting]_&#123;backbone&#125;_&#123;neck&#125;_[norm setting]_[misc]_[gpu x batch_per_gpu]_&#123;schedule&#125;_&#123;dataset&#125;</span><br><span class="line">*&#123;xxx&#125; 是被要求填写的字段而 [yyy] 是可选的</span><br><span class="line">    &#123;model&#125;：模型种类，例如 hv_pointpillars (Hard Voxelization PointPillars)、VoteNet 等。</span><br><span class="line">    [model setting]：某些模型的特殊设定。</span><br><span class="line">    &#123;backbone&#125;： 主干网络种类例如 regnet-400mf、regnet-1.6gf 等。</span><br><span class="line">    &#123;neck&#125;：模型颈部的种类包括 fpn、secfpn 等。</span><br><span class="line">    [norm_setting]：如无特殊声明，默认使用 bn (Batch Normalization)，其他类型可以有 gn (Group Normalization)、sbn (Synchronized Batch Normalization) 等。 gn-head&#x2F;gn-neck 表示 GN 仅应用于网络的头部或颈部，而 gn-all 表示 GN 用于整个模型，例如主干网络、颈部和头部。</span><br><span class="line">    [misc]：模型中各式各样的设置&#x2F;插件，例如 strong-aug 意味着在训练过程中使用更强的数据增广策略。</span><br><span class="line">    [batch_per_gpu x gpu]：每个 GPU 的样本数和 GPU 数量，默认使用 4x8。</span><br><span class="line">    &#123;schedule&#125;：训练方案，选项是 1x、2x、20e 等。 1x 和 2x 分别代表训练 12 和 24 轮。 20e 在级联模型中使用，表示训练 20 轮。 对于 1x&#x2F;2x，初始学习率在第 8&#x2F;16 和第 11&#x2F;22 轮衰减 10 倍；对于 20e，初始学习率在第 16 和第 19 轮衰减 10 倍。</span><br><span class="line">    &#123;dataset&#125;：数据集，例如 nus-3d、kitti-3d、lyft-3d、scannet-3d、sunrgbd-3d 等。 当某一数据集存在多种设定时，我们也标记下所使用的类别数量，例如 kitti-3d-3class 和 kitti-3d-car 分别意味着在 KITTI 的所有三类上和单独车这一类上进行训练。</span><br></pre></td></tr></table></figure><h3 id="DATASET-PREPARATION"><a href="#DATASET-PREPARATION" class="headerlink" title="DATASET PREPARATION"></a>DATASET PREPARATION</h3><p>离线转换的方法将其转换为 KITTI数据集的格式，因此只需要在转换后修改配置文件中的数据标注文件的路径和标注数据所包含类别；对于那些与现有数据格式相似的新数据集，如 Lyft 数据集和 nuScenes 数据集，我们建议直接调用数据转换器和现有的数据集类别信息，在这个过程中，可以考虑通过继承的方式来减少实施数据转换的负担。</p><p>当现有数据集与新数据集存在差异时，可以通过定义一个从现有数据集类继承而来的新数据集类来处理具体的差异；最后，用户需要进一步修改配置文件来调用新的数据集。【waymo例子】</p><p>路径：mmdetection3d/mmdet3d/datasets</p><p>自定义数据集：将标注信息重新组织成一个 pickle 文件格式的字典列表 标注框的标注信息会被存储在 <code>annotation.pkl</code> 文件中 在 <code>mmdet3d/datasets/my_dataset.py</code> 中创建一个新的数据集类来进行数据的加载。</p><p>统合数据集或者修改数据集的分布，并应用到模型的训练中。 </p><ul><li><code>RepeatDataset</code>：简单地重复整个数据集</li><li><code>ClassBalancedDataset</code>：以类别平衡的方式重复数据集</li><li><code>ConcatDataset</code>：拼接多个数据集</li></ul><h2 id="CUSTOMIZE-DATA-PIPELINES"><a href="#CUSTOMIZE-DATA-PIPELINES" class="headerlink" title="CUSTOMIZE DATA PIPELINES"></a>CUSTOMIZE DATA PIPELINES</h2><p><img src="/2023/04/30/openpcdet/image-20230426213830604-16828380822496.png" alt></p><p>数据加载、预处理、格式化、测试时的数据增强</p><h2 id="CUSTOMIZE-Model"><a href="#CUSTOMIZE-Model" class="headerlink" title="CUSTOMIZE Model"></a>CUSTOMIZE Model</h2><p>通常把模型的各个组成成分分成6种类型：</p><ul><li>编码器（encoder）：包括 voxel layer、voxel encoder 和 middle encoder 等进入 backbone 前所使用的基于 voxel 的方法，如 HardVFE 和 PointPillarsScatter。</li><li>骨干网络（backbone）：通常采用 FCN 网络来提取特征图，如 ResNet 和 SECOND。</li><li>颈部网络（neck）：位于 backbones 和 heads 之间的组成模块，如 FPN 和 SECONDFPN。</li><li>检测头（head）：用于特定任务的组成模块，如检测框的预测和掩码的预测。</li><li>RoI 提取器（RoI extractor）：用于从特征图中提取 RoI 特征的组成模块，如 H3DRoIHead 和 PartAggregationROIHead。</li><li>损失函数（loss）：heads 中用于计算损失函数的组成模块，如 FocalLoss、L1Loss 和 GHMLoss。</li></ul><p>针对每个组成成分进行添加和修改。</p><h2 id="INFERENCE"><a href="#INFERENCE" class="headerlink" title="INFERENCE"></a>INFERENCE</h2><ul><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id2" target="_blank" rel="noopener">自定义优化器设置</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id9" target="_blank" rel="noopener">自定义训练规程</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id10" target="_blank" rel="noopener">自定义工作流</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id11" target="_blank" rel="noopener">自定义钩子</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>Motivation</title>
    <link href="https://betterwyl.github.io/2023/04/02/motivation%20pose%20estimation/"/>
    <id>https://betterwyl.github.io/2023/04/02/motivation%20pose%20estimation/</id>
    <published>2023-04-02T06:45:43.000Z</published>
    <updated>2023-04-02T14:16:15.172Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="3D-Pose-estimation-based-graph"><a href="#3D-Pose-estimation-based-graph" class="headerlink" title="3D Pose-estimation based graph"></a>3D Pose-estimation based graph</h1><p>[noticeable]backbone: </p><p>2D-HPE hourglass (NOTICE: which applicated in RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization )</p><p>每个Hourglass module的结构都包含一个bottom-up过程和一个top-down过程，前者通过卷积和pooling将图片从高分辨率降到低分辨率，后者通过upsampling将图片从低分辨率回复到高分辨率。</p><p>基于Hourglass 的改进：Hourglass+Associative Embedding 在多人姿态估计中，先检测身体部位，然后把他们分组给不同的个人。→检测和分组同时进行（Associative Embedding ）</p><p>coordinate/heatmap</p><p>3D-HPE CNN+GCN</p><h2 id="Graph-based-Related-work"><a href="#Graph-based-Related-work" class="headerlink" title="Graph based Related work"></a>Graph based Related work</h2><h2 id="Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks-ICCV2019"><a href="#Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks-ICCV2019" class="headerlink" title="Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks (ICCV2019)"></a>Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks (ICCV2019)</h2><p>code：<a href="https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks" target="_blank" rel="noopener">https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks</a></p><p>Spatial-temporal Graph Construction：一个序列T帧，一帧有M个body joints。Vertices=MT</p><p>local-global：多尺度特征 GCN-based Local-to-global Prediction 【这里很奇怪 代码里没有用到nonlocal3Dblock】</p><p>use different kernels for different neighboring nodes。</p><h2 id="Graph-Stacked-Hourglass-Network-CVPR-2021"><a href="#Graph-Stacked-Hourglass-Network-CVPR-2021" class="headerlink" title="Graph Stacked Hourglass Network (CVPR 2021)"></a>Graph Stacked Hourglass Network (CVPR 2021)</h2><p>问题：  图卷积（只能在一个单一尺度上对特征进行处理，难以提取表征空间的局部和全局空间信息，限制了模型的表征能力，没有利用模型的深度特点）。</p><p>在HourGlassNet 上改进。【HG可看作是conv-deconv或者encoder-decoder的结构】</p><p>downsampling  【 pooling 】 and upsampling 【unpooling 】</p><p>Graph design：residual connections   补充Graph U-Nets</p><p>引入PreAggr 。堆叠了四个 前面multi-scale【spatial aspect of the graph】，后面multi-level【SE block+semantic information】。【保证输入输出通道大小都为64】</p><h2 id="A-Graph-Attention-Spatio-temporal-Convolutional-Network-for-3D-Human-Pose-Estimation-in-Video-ICRA2021"><a href="#A-Graph-Attention-Spatio-temporal-Convolutional-Network-for-3D-Human-Pose-Estimation-in-Video-ICRA2021" class="headerlink" title="A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video(ICRA2021)"></a>A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video(ICRA2021)</h2><p>关键点：图注意时空卷积网络，spectral-based  </p><p>使用方法：图注意力和Semantic graph convolutional networks for 3d human pose regression的工作</p><p>Temporal 改变：1D卷积–2D卷积 基于《3d human pose<br>estimation in video with temporal convolutions and semi-supervised<br>training》方法</p><p>Spatio方法：</p><p><strong>Local Attention Graph</strong> ：每一个node 都有C 个通道特征，经过L个layer。即CL</p><p> two novel convolution kernels【基于身体结构对称性假设】。[noticeable]：one is symmetric kernel </p><p>！each of these two convolution kernels are applied to two distinct GCNs  </p><p><strong>Global Attention Graph</strong>  ：针对于没有直接相连的joint，存在一种sub-segments  关系。【推：没有直接相连的node ，非局部关系→！这么说是可以解决遮挡问题的】</p><h2 id="Modulated-Graph-Convolutional-Network-for-3D-Human-Pose-Estimation-ICCV-2021"><a href="#Modulated-Graph-Convolutional-Network-for-3D-Human-Pose-Estimation-ICCV-2021" class="headerlink" title="Modulated Graph Convolutional Network for 3D Human Pose Estimation (ICCV 2021)"></a>Modulated Graph Convolutional Network for 3D Human Pose Estimation (ICCV 2021)</h2><p>调整GCN中的图结构：使用共享权重而不增加模型参数。</p><h2 id="Semantic-Graph-Convolutional-Networks-for-3D-Human-Pose-Regression-CVPR-2019"><a href="#Semantic-Graph-Convolutional-Networks-for-3D-Human-Pose-Regression-CVPR-2019" class="headerlink" title="Semantic Graph Convolutional Networks for 3D Human Pose Regression (CVPR 2019)"></a>Semantic Graph Convolutional Networks for 3D Human Pose Regression (CVPR 2019)</h2><p>node的<strong>局部和全局关系</strong></p><p>…更新中</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>coding指南 2</title>
    <link href="https://betterwyl.github.io/2023/04/02/debug%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/"/>
    <id>https://betterwyl.github.io/2023/04/02/debug%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/</id>
    <published>2023-04-02T06:45:43.000Z</published>
    <updated>2023-04-02T14:46:19.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Point-GNN"><a href="#Point-GNN" class="headerlink" title="Point-GNN"></a>Point-GNN</h1><p>1.os.path.join 路径要写完全了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATASET_DIR=<span class="string">"/dataset/KITTI"</span></span><br><span class="line">DATASET_SPLIT_FILE = os.path.join(DATASET_DIR,<span class="string">'3DOP_splits/'</span>+train_config[<span class="string">'train_dataset'</span>])</span><br><span class="line">print(DATASET_SPLIT_FILE)</span><br><span class="line">/dataset/KITTI/<span class="number">3</span>DOP_splits/train_car.txt</span><br></pre></td></tr></table></figure><p>2.数据增强tips: * *  kwargs和 * args :</p><p>可以设置选择data augment的模式：如下就是选择random_rotation_all模式对应的操作。</p><ul><li><p>args传的是【非键值对】可变数量的参数*<em>列表 *</em></p><ul><li><ul><li>kwargs 传递的就是具体方法涉及的参数信息<strong>不定长度</strong>【键值对】。</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">aug_method = aug_method_map[aug_config[<span class="string">'method_name'</span>]]</span><br><span class="line">cam_rgb_points, labels = aug_method(</span><br><span class="line">    cam_rgb_points, labels, **aug_config[<span class="string">'method_kwargs'</span>])</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> 测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"data_aug_configs"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"method_kwargs"</span>: &#123;</span><br><span class="line">                <span class="string">"expend_factor"</span>: [</span><br><span class="line">                    <span class="number">1.0</span>,</span><br><span class="line">                    <span class="number">1.0</span>,</span><br><span class="line">                    <span class="number">1.0</span></span><br><span class="line">                ],</span><br><span class="line">                <span class="string">"method_name"</span>: <span class="string">"normal"</span>,</span><br><span class="line">                <span class="string">"yaw_std"</span>: <span class="number">0.39269908169872414</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"method_name"</span>: <span class="string">"random_rotation_all"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        ...]</span><br><span class="line">aug_method_map = &#123;</span><br><span class="line">    <span class="string">'random_jitter'</span>: random_jitter,</span><br><span class="line">    <span class="string">'random_box_rotation'</span>: random_box_rotation,</span><br><span class="line">    <span class="string">'random_box_shift'</span>: random_box_shift,</span><br><span class="line">    <span class="string">'random_transition'</span>: random_transition,</span><br><span class="line">    <span class="string">'remove_background'</span>: remove_background,</span><br><span class="line">    <span class="string">'random_rotation_all'</span>: random_rotation_all,</span><br><span class="line">    <span class="string">'random_flip_all'</span>: random_flip_all,...&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_rotation_all</span><span class="params">(cam_rgb_points, labels, method_name=<span class="string">'normal'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    yaw_std=<span class="number">0.3</span>, expend_factor=<span class="params">(<span class="number">1.0</span>, <span class="number">1.1</span>, <span class="number">1.1</span>)</span>)</span>:</span></span><br><span class="line">    xyz = cam_rgb_points.xyz</span><br><span class="line">    <span class="keyword">if</span> method_name == <span class="string">'normal'</span>:</span><br><span class="line">        delta_yaw = np.random.normal(scale=yaw_std)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> method_name == <span class="string">'uniform'</span>:</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>3.Points = namedtuple(‘Points’, [‘xyz’, ‘attr’])</p><p>velo points[x,y,z,flection] velopoints2camera  points xyz in camera with rgb[x,y,z,flection,RGB]</p><p>4.建图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph_generate_fn= get_graph_generate_fn(config[<span class="string">'graph_gen_method'</span>])</span><br><span class="line"></span><br><span class="line">(vertex_coord_list, keypoint_indices_list, edges_list) = \</span><br><span class="line">        graph_generate_fn(cam_rgb_points.xyz, **config[<span class="string">'graph_gen_kwargs'</span>])</span><br></pre></td></tr></table></figure><p>gen_multi_level_local_graph_v3 </p><p>voxel采样：multi_layer_downsampling </p><p>采样后 得到vertex+keypoint_indices 相当于node</p><p>create edges 重要参数 graph_level</p><ul><li><input checked disabled type="checkbox"> 自己采样测试 ：结果1024→816；1000→802</li></ul><p>遍历采样后的每一个点，找邻居值【使用kneighbors】</p><p>5.数据变化过程：以car为例 input：3260samples</p><p>分成两个batch [m,n]【猜测是并行加载数据的操作，看不懂为啥这边要分两个】</p><p>batch_list += [fetch_data(dataset, m或n, train_config, config)] </p><p>input_v, vertex_coord_list, keypoint_indices_list, edges_list, \</p><p>​      cls_labels, encoded_boxes, valid_boxes = batch_data(batch_list)</p><p>cam_rgb_points：[x,y,z,flection,RGB] </p><p>input_v = cam_rgb_points.attr.flection</p><p>假设graph level=2</p><p>points_xyz = vertex_coord_list[0]  </p><p>keypoint_indices_list=特征点的index</p><ul><li><input checked disabled type="checkbox"> 结果分析：</li></ul><p>共有37255个point 【含有xyz坐标：vertex_coord_list[0]  】下采样得到3091个点【含有xyz坐标：points_xyz = vertex_coord_list[1]】</p><p>下采样的点在原始point中的index：keypoint_indices_list: 3,6,19…</p><p>vertex_coord_list [ 0 ]  [ 3 ]==→ vertex_coord_list[ 1 ] [ 0 ]</p><p>【torch.equal(vertex_coord_list[1],vertex_coord_list[2]) =True 为什么是一样的】</p><p>valid_boxes：flag ：bool 表明采样后的 points 在一个 3D box中 比如sum(valid_boxes)=145，有145个点在框中，当然是越大越好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="code已复现备注" scheme="https://betterwyl.github.io/tags/code%E5%B7%B2%E5%A4%8D%E7%8E%B0%E5%A4%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>CS224W</title>
    <link href="https://betterwyl.github.io/2023/03/30/CS224/"/>
    <id>https://betterwyl.github.io/2023/03/30/CS224/</id>
    <published>2023-03-30T12:45:43.000Z</published>
    <updated>2023-04-05T13:33:06.802Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><h3 id="1-1intro"><a href="#1-1intro" class="headerlink" title="1.1intro"></a>1.1intro</h3><p>图的表示学习：大致来说就是将原始的结点（或链接、或图）表示为向量（嵌入<strong>embedding</strong>），图中相似的结点会被embed得靠近（指同一实体，在结点空间上相似，在向量空间上就也应当相似）。</p><h3 id="1-2Applications-of-Graph-ML"><a href="#1-2Applications-of-Graph-ML" class="headerlink" title="1.2Applications of Graph ML"></a>1.2Applications of Graph ML</h3><p>node level：</p><p>protein folding：给定氨基酸序列计算预测蛋白质的3D结构。模拟蛋白质位置，预测3Dshape。</p><p>edge level：</p><p>recommender systems ：pinsage图像+图结构，更好的推荐 ；任务目标是使相似结点嵌入之间的距离比不相似结点嵌入之间的距离更小。</p><p>药物组合副作用：预测引擎，蛋白质相互作用网络，预测缺失的边缘。</p><p>背景：很多人需要同时吃多种药来治疗多种病症。任务：输入一对药物，预测其有害副作用。</p><p>subgraph level：</p><p>Google map: traffic prediction测一段路程的长度、耗时等：将路段建模成图，在每个子图上建立预测模型</p><p>graph level：</p><p>用GNN的图分类任务来从一系列备选图（分子被表示为图，结点是原子，边是化学键）中预测最有可能是抗生素的分子；</p><p>预测材料变形：用GNN来预测粒子的下一步活动（组成一个新位置、新图</p><h3 id="1-3design-choice："><a href="#1-3design-choice：" class="headerlink" title="1.3design choice："></a>1.3design choice：</h3><p>Bipartite Graph→Folded/Projected Bipartite Graphs</p><p>Representing Graphs：Edge list【难表示】；Adjacency list【对图的分析和操作更方便】</p><p>结点和边的属性：Weight (e.g., frequency of communication)；Ranking (best friend, second best friend…)；Type (friend, relative, co-worker)；Sign: Friend vs. Foe, Trust vs. Distrust；Properties depending on the structure of the rest of the graph: Number of common friends</p><p>Self-edges (self-loops)自环 / Multigraph</p><p><img src="/2023/03/30/CS224/image-20230405203011209.png" alt="Self-edges (self-loops) / Multigraph"></p><p>multigraph有时也可被视作是weighted graph，就是说将多边的地方视作一条边的权重（在邻接矩阵上可看出效果是一样的）。但有时也可能就是想要分别处理每一条边，这些边上可能有不同的property和attribute</p><p>Connectivity</p><p><img src="/2023/03/30/CS224/image-20230403192951728.png" alt="Connectivity"></p><p>connected：任意两个结点都有路径相通 strongly connected components：互相可以访问</p><p>disconnected：由2至多个connected components构成。<br>最大的子连接图：giant component<br>isolated node<br>这种图的邻接矩阵可以写成block-diagonal的形式，数字只在connected components之中出现。【聚类中的块对角个数=类别数】</p><h2 id="2-Feature-Engineering-for-ML-in-Graphs"><a href="#2-Feature-Engineering-for-ML-in-Graphs" class="headerlink" title="2.Feature Engineering for ML in Graphs"></a>2.Feature Engineering for ML in Graphs</h2><p>传统pipelines【2 steps】：设计并获取所有训练数据上结点/边/图的特征→分类器/模型→新结点作出预测</p><p>Features: d-dimensional vectors<br>Objects: Nodes, edges, sets of nodes, entire graphs<br>Objective function: What task are we aiming to solve?</p><h3 id="2-1Node-Level-Feature"><a href="#2-1Node-Level-Feature" class="headerlink" title="2.1Node Level Feature"></a>2.1Node Level Feature</h3><p>1.Importance-based features考虑了结点的重要性；</p><p>2.Structure-based features捕获节点附近的拓扑属性</p><p><strong>eigenvector centrality</strong>：如果结点邻居重要，那么结点本身也重要。与link的数量无关，与link的重要性有关。</p><p><img src="/2023/03/30/CS224/image-20230403200310561.png" alt="eigenvector centrality"></p><p><strong>betweenness centrality</strong>：桥梁！如果一个结点处在很多结点对的最短路径上，那么这个结点是重要的。</p><p><img src="/2023/03/30/CS224/image-20230403200532146.png" alt="betweenness centrality"></p><p><strong>closeness centrality</strong>：交通枢纽！到其他结点的路径长度最小。越居中越短。</p><p><img src="/2023/03/30/CS224/image-20230403200844882.png" alt="closeness centrality"></p><p><strong>clustering coefficient</strong>聚类系数：邻居的联系程度</p><p>第1个例子：6 / 6<br>第2个例子：3 / 6<br>第3个例子：0 / 6 </p><p><img src="/2023/03/30/CS224/image-20230403204703668.png" alt="eg"></p><p><strong>三角形/三元组</strong>：结点自我网络。共友也会互相认识。</p><p>三角形可以拓展到某些预定义的子图pre-specified subgraph</p><p><strong>graphlets</strong></p><p>2-5个节点的graphlets，可以得到一个长度为73个坐标coordinate的向量GDV，描述该点的局部拓扑结构topology of node’s neighborhood，可以捕获距离为4 hops的互联性interconnectivities。<br>相比节点度数或clustering coefficient，GDV能够描述两个节点之间更详细的节点局部拓扑结构相似性local topological similarity。</p><p><img src="/2023/03/30/CS224/image-20230403213451852.png" alt="graphlets"></p><h3 id="2-2Link-Level-Feature"><a href="#2-2Link-Level-Feature" class="headerlink" title="2.2Link Level Feature"></a>2.2Link Level Feature</h3><p>link level prediction：根据网络现有的link预测新的link</p><p>eg基于相似性：算两点间的相似性得分（如用共同邻居衡量相似性），然后将点对进行排序，得分最高的n组点对就是预测结果，与真实值作比</p><p>测试阶段：评估无edge的node pairs→rank k→prediction</p><p>链接预测任务的两种类型：</p><p>随机缺失边【第一种假设可以以蛋白质之间的交互作用举例，缺失的是研究者还没有发现的交互作用。新链接的发现会受到已发现链接的影响。在网络中有些部分被研究得更彻底，有些部分就几乎没有什么了解，不同部分的发现难度不同】；随时间推演边【以社交网络举例，随着时间流转，人们认识更多朋友。】</p><p><strong>distance-based feature</strong></p><p><img src="/2023/03/30/CS224/image-20230403215243135.png" alt="distance-based feature"></p><p><strong>local neighborhood overlap</strong></p><p><img src="/2023/03/30/CS224/image-20230403215255078.png" alt="local neighborhood overlap"></p><p>common neighbors： 度数高将表现结果更高。</p><p>Adamic-Adar index：有点像信息熵，度数低的共友比名人共友价值大。</p><p>如果没有共友的情况，则值为0。【缺点：两个点未来仍有可能被连接起来。相距两跳的结点、没有邻居的结点？解决方案global neighborhood overlap】</p><p><strong>global neighborhood overlap</strong></p><p>获得一对结点的得分。</p><p>Katz index：计算node pairs之间所有长度路径的数量 → 对邻接矩阵求幂</p><p>邻接矩阵：代表u和v之间长度为1的路径的数量</p><p>分解路径：将长度为1转化，通过桥接方法。对起始节点和相邻结点求和，到目标结点。【归纳法】</p><p><img src="/2023/03/30/CS224/image-20230403220039044.png" alt="Katz compute"></p><p>观察其对角线，其实就是邻居的数量。</p><p><img src="/2023/03/30/CS224/image-20230403220152827.png" alt="katz"></p><p>比较长的距离以比较小的权重。</p><p>2.3Graph Level Feature </p><p><strong>核方法</strong>：</p><p><img src="/2023/03/30/CS224/image-20230403223205107.png" alt="KERNEL"></p><p>eg. BoW for graph：将图表示成一个向量，每个元素代表对应something出现的次数（这个something可以是node, degree, graphlet, color）node degrees</p><p><img src="/2023/03/30/CS224/image-20230404113506849.png" alt="BOW"></p><p>eg. Graphlet kernel【侧重于node 和 edge，即结构 】直接点积两个图的graphlet count vector得到相似性。对于图尺寸相差较大的情况需进行归一化。计算代价高，k-size对应n^k</p><p><img src="/2023/03/30/CS224/image-20230404113417281.png" alt="Graphlet kernel"></p><p>eg. Weisfeiler-Lehman Kernel ：建立特征描述子，邻域结构叠代结点度。</p><p>color refinement：</p><p><img src="/2023/03/30/CS224/image-20230404114000690.png" alt="hash"></p><p>把邻居颜色聚集起来，使用hash，重新标记颜色。【优化是线性的】</p><p>进行K次迭代，整个迭代过程中颜色出现的次数作为Weisfeiler-Lehman graph feature。上述向量点积计算相似性，得到WL kernel。</p><h2 id="3-Node-Embeddings"><a href="#3-Node-Embeddings" class="headerlink" title="3.Node Embeddings"></a>3.Node Embeddings</h2><h3 id="3-1encoder-decoder"><a href="#3-1encoder-decoder" class="headerlink" title="3.1encoder-decoder"></a>3.1encoder-decoder</h3><p>graph representation learning：学习到图数据用于机器学习的、与下游任务无关的特征，我们希望这个向量能够抓住数据的结构信息。</p><p>将图数据紧密地嵌入到嵌入空间中→自动对结构信息进行编码用于下游任务。</p><p>eg. node embedding：deep walk</p><p><strong>Encoder and Decoder</strong>：对结点进行编码，保证其在嵌入空间的相似性。在二维的相似性【例如距离】，可反应图的结构。</p><p>Encoder：将节点映射为embedding<br>定义一个衡量节点相似度的函数（如衡量在原网络中的节点相似度）<br>Decoder DEC：将embedding对映射为相似度得分。</p><p><img src="/2023/03/30/CS224/image-20230404122425624.png" alt="encoder+decoder"></p><p>ENC(v)=Z · v </p><p><img src="/2023/03/30/CS224/image-20230404123051920.png" alt="embedding"></p><p>→ Z(d,v)每个节点都有一个列来表示该结点的嵌入。</p><p>→v是一个其他元素都为0，对应节点位置的元素为1的向量。</p><p>【缺陷：参数过多，难scale up到大型图；Z的大小取决于结点的数量】</p><p>DEC：节点相似的不同定义：边；邻居；相似的structural roles</p><p>random walks：无监督/自监督→使用结点embedding时，并没有使用结点的label和功能，目标只学习了网络的相似性。</p><h3 id="3-2Random-Walk-Approaches-for-Node-Embeddings"><a href="#3-2Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="3.2Random Walk Approaches for Node Embeddings"></a>3.2Random Walk Approaches for Node Embeddings</h3><p>goal：每个结点的嵌入向量z</p><p>随机游走的相似性：使用概率表示，从结点u开始随机走动得到v的概率</p><p><img src="/2023/03/30/CS224/image-20230404141113888.png" alt="random walk"></p><p>random walk：从某一结点开始，每一步按照概率选一个邻居，停止后得到随机游走序列。→点u和v在一次随机游走中出现的概率【相似的网络邻居，彼此之间很近，之间可能有多条路径】</p><p><img src="/2023/03/30/CS224/image-20230404144913123.png" alt="finally"></p><p>复杂度O(V^2)。改进：softmax归一化的分母。将抽出k个结点作为负样本代替所有结点作为负样本。【k值大相当于加入了更多数据，方差会变小，偏差比较大。】</p><p><img src="/2023/03/30/CS224/image-20230404145538706.png" alt="negative sampling"></p><p>random walk策略</p><p>eg.deep walk：一阶随机游走</p><p>eg.node2vec</p><p>灵活的网络邻居【local(BFS) and global(DFS)】→丰富的结点嵌入【biased walk】：引入二阶随机数</p><p>超参数：p：return parameter【记住游走的来源】；q：DFS和BFS的ratio</p><p>例如保持相同的为走到S2，向前迈进为S3/S4。p小则返回，q小则探索。</p><p><img src="/2023/03/30/CS224/image-20230404153209529.png" alt="visualization"></p><p>node2vec在节点分类任务上表现更好，不同的方法在不同数据的链接预测任务上表现不同。</p><h3 id="3-3-Embedding-Entire-Graphs"><a href="#3-3-Embedding-Entire-Graphs" class="headerlink" title="3.3 Embedding Entire Graphs"></a>3.3 Embedding Entire Graphs</h3><p>聚合/加权平均结点的嵌入；</p><p>introduce a virtual node【虚拟结点将连接到网络中所有其他结点】→嵌入子图/整个图→得到全图的信息；</p><p>anonymous walk embeddings：以节点第一次出现的序号（是第几个出现的节点）作为索引。拜访了不同的结点，但用相同的顺序获得相同的anonymous表示。</p><h2 id="4-GNN"><a href="#4-GNN" class="headerlink" title="4.GNN"></a>4.GNN</h2><p>结点嵌入任务目的：在于将节点映射到d维向量，使得在图中相似的节点在向量域中也相似。使用一个大矩阵直接储存每个节点的表示向量，通过矩阵与向量乘法来实现嵌入过程。缺陷：需要O(V)复杂度的参数，太多结点间参数不共享，每个结点表示的向量都是独特的；无法获取在训练时没出现的结点的表示向量，即transductive；无法应用结点的特征信息。</p><p>浅层编码：Encoder+similarity function+Decoder→学习Z矩阵。</p><p>深层编码：可以与结点相似性功能结合使用。</p><h3 id="4-1-basics-of-deep-learning"><a href="#4-1-basics-of-deep-learning" class="headerlink" title="4.1 basics of deep learning"></a>4.1 basics of deep learning</h3><p>supervised learing: x→f→y</p><p><img src="/2023/03/30/CS224/image-20230405094330602.png" alt="func"></p><p><img src="/2023/03/30/CS224/image-20230405095059110.png" alt="optimize"></p><p>梯度向量：函数增长最快的方向和增长率，每个元素是对应参数在损失函数上的偏微分。<br>方向导数：函数在某个给定方向上的变化率。<br>梯度是函数增长率最快的方向的方向导数。</p><p>迭代：将参数向负梯度方向更新；理想的停止条件是梯度为0，在实践中一般则是用“验证集上的表现不再提升”作为停止条件；学习率是一个需要设置的超参数，控制梯度下降每一步的步长，可以在训练过程中改变。</p><p>minibatch SGD：每一次梯度下降都需要计算所有数据集上的梯度，耗时太久→使用SGD的方法，将数据分成多个minibatch，每次用一个minibatch来计算梯度</p><p>SGD是梯度的无偏估计，但不保证收敛，所以一般需要调整学习率。</p><p>batch size：每个minibatch中的数据点数；iteration：在一个minibatch上做一次训练；epoch：在整个数据集上做一次训练。</p><p>前向传播：compute loss starting from input【求下降的多少】</p><p>梯度反向传播：计算梯度是不断向后的过程【求偏导，求下降的最快方向】</p><p><img src="/2023/03/30/CS224/image-20230405100142457.png" alt="vs"></p><p>引入非线性：提高表达力</p><p>MLP：整合线性和非线性</p><h3 id="4-2Deep-Learning-for-Graphs"><a href="#4-2Deep-Learning-for-Graphs" class="headerlink" title="4.2Deep Learning for Graphs"></a>4.2Deep Learning for Graphs</h3><p>set up：</p><p><img src="/2023/03/30/CS224/image-20230405101219610.png" alt="setting"></p><p>naive approach：input【邻接矩阵+node feature 能表示成grid的形式】需要 O(|V|)的参数，不适用于不同大小的图。对节点顺序敏感（我们需要一个即使改变了节点顺序，结果也不会变的模型）</p><p>图上无法定义固定的locality或滑动窗口，而且图是permutation invariant【order是不固定的】</p><p>Graph Convolutional Networks：通过节点邻居定义其计算图，传播并转换信息，计算出节点表示（可以说是用邻居信息来表示一个节点）</p><p>核心思想：通过聚合邻居来生成节点嵌入→通过神经网络聚合信息【每个结点都能定义】</p><p>！在每一层中结点都有不同的嵌入【节点在每一层都有不同的表示向量，每一层节点嵌入是邻居上一层节点嵌入再加上它自己（相当于添加了自环）的聚合。】 k 跳对应k layers；聚合顺序无关紧要 a set of elements</p><p><img src="/2023/03/30/CS224/image-20230405110059635.png" alt="fomular"></p><p>模型上可以学习的参数【只取决于嵌入的大小，而非图形的大小】：W 邻居聚合权重；B 转换结点本身隐藏向量的权重</p><p>网络中所有的结点都使用相同的更新公式。</p><p>使用matrix表示：H 来自上一层结点的嵌入</p><p><img src="/2023/03/30/CS224/image-20230405144050132.png" alt="matrix"></p><p><img src="/2023/03/30/CS224/image-20230405144207898.png" alt="matrix"></p><p>训练：</p><p>有监督使用标签；无监督使用图结构【即相似度】</p><p>定义邻域聚合函数→定义loss函数→set of datasets→为新结点生成嵌入【inductive】</p><h2 id="5-General-Perspective-on-GNNS"><a href="#5-General-Perspective-on-GNNS" class="headerlink" title="5.General Perspective on GNNS"></a>5.General Perspective on GNNS</h2><h3 id="5-1design-space"><a href="#5-1design-space" class="headerlink" title="5.1design space"></a>5.1design space</h3><p><strong>aggregation</strong></p><p><strong>message</strong></p><p> <strong>GNN layers connectivity</strong>即组合layers的方式</p><p><strong>manipulation</strong>：使原始输入图和应用在GNN中的计算图不完全相同（即对原始输入进行一定处理后，再得到GNN中应用的计算图）</p><p><strong>learning</strong></p><h3 id="5-2-single-GNN-layer"><a href="#5-2-single-GNN-layer" class="headerlink" title="5.2 single GNN layer"></a>5.2 single GNN layer</h3><p><strong>message</strong>：self message【来自上一层结点的嵌入】+neighborhoods‘→转换消息 </p><p><strong>aggregation</strong>：聚合消息【阶不变】eg：求和、平均、最大值</p><p>在这两步上都可以用非线性函数（激活函数）来增加其表现力。</p><p><img src="/2023/03/30/CS224/image-20230405155120889.png" alt="gcn"></p><p><img src="/2023/03/30/CS224/image-20230405155141048.png" alt="graphsage"></p><p><strong>GCN：</strong></p><p>message：对上一层的节点嵌入用本层的权重矩阵进行转换，用节点度数进行归一化<br>agg：sum信息，应用激活函数</p><p><strong>GraphSAGE</strong>在GCN上的延展：1.聚合函数可以是任意的:AGG 2.从结点本身获得消息和邻居信息3.L2归一化</p><p>2 stages→agg：聚合neighborhood信息；将上一层信息与节点本身信息进行聚合</p><p>message：在agg过程中一起实现</p><p><strong>GAT</strong>：【邻居的重要性】</p><p>在GCN和GraphSAGE中，直接基于图结构信息（节点度数）显式定义注意力权重，相当于认为节点的所有邻居都同样重要（注意力权重一样大）。</p><p>GAT中注意力权重：attention mechanism：alpha→用两个节点上一层的节点嵌入计算其注意力系数</p><p>mechanism：alpha的形式：可以使用一个layer 难以收敛【改进：多头注意力 三种功能的注意力再汇总，随机初始化，使用不同的函数，每一个alpha收敛到一个局部最小值】</p><h3 id="5-3-stacking-GNN-layers"><a href="#5-3-stacking-GNN-layers" class="headerlink" title="5.3 stacking  GNN layers"></a>5.3 stacking  GNN layers</h3><p>over smoothing：如果GNN层数太多，不同节点的嵌入向量会收敛到同一个值。随着K layers增加和hops的增加，nodes感受野增加也很快。图信号越来越弱。</p><p><img src="/2023/03/30/CS224/image-20230405163916026.png" alt="visualization"></p><p>【idea：如何解决GNN的 over smoothing】分析解决问题所需的必要感受野；设置GNN层数 L 略大于我们想要的感受野</p><p>增加浅层GNN的表现力：将message和agg过程变成深度神经网络→mlp+pre-layer+post-layer</p><p>增加跳层连接：靠前的GNN层可能能更好地区分节点，最终节点嵌入中增加靠前GNN层的影响力，实现方法是在GNN中直接添加捷径，保存上一层节点的嵌入向量。</p><h2 id="5-Applications-of-Graph-Neural-Networks"><a href="#5-Applications-of-Graph-Neural-Networks" class="headerlink" title="5.Applications of Graph Neural Networks"></a>5.Applications of Graph Neural Networks</h2><p>由原始图定义的计算图 →针对这一假设改进【输入图并非一定是最佳计算图】</p><p>node level：输入图可能缺少功能、属性，或者难以encode；→进行特征增强</p><p>graph structure：过于稀疏，消息传递需要多次，效率低。过于密集，消息传递代价高，或者图过大GPU难以使用。</p><h3 id="5-1-Graph-Augmentation-for-GNNs"><a href="#5-1-Graph-Augmentation-for-GNNs" class="headerlink" title="5.1 Graph Augmentation for GNNs"></a>5.1 Graph Augmentation for GNNs</h3><p><strong>Feature Augmentation：</strong>某些结构缺少或者很难学习</p><p>例如在化学中判断结点是否处于一个循环中。在计算图中看起来是一样的。3和4个结点的循环，因为度数相同（都是2），所以无论环上有多少个节点，GNN都会得到相同的计算图（二叉树），无法分别。解决方法是为循环增加one-hot编码。</p><p>constant node feature&amp; one-hot feature</p><p><img src="/2023/03/30/CS224/image-20230405191802807.png" alt="vs"></p><p>其他常用于数据增强的特征：clustering coefficient，centrality，page rank</p><p><strong>Structure Augmentation</strong></p><p>稀疏图：</p><p>增加虚拟边，邻接矩阵的幂次相加【相当于和朋友的朋友成为了朋友，两跳也相连】</p><p>增加虚拟结点，两结点相距过远，牵线搭桥，使得网络不用太大</p><p>稠密图：</p><p>random sample 折衷，计算效率增加但是会使得信息存在丢失</p><h3 id="5-2-training"><a href="#5-2-training" class="headerlink" title="5.2 training"></a>5.2 <strong>training</strong></h3><p><strong>不同粒度下的prediction head</strong>：节点级别，边级别，图级别</p><p>node-level：使用node embedding做预测 →k classification/regression 将d维嵌入映射到k维输出</p><p>edge-level：使用两个node embedding做预测：可以是像注意力机制那样一个点对另一个点剪→alpha对应边，再使用linear将2d维映射到k维输出；也可以是1 way→点积 k way→多头注意力</p><p>graph-level：每个node聚合 pooling。小图上表现很好→mean方法：结果不受节点数量的影响；sum方法：关心图的大小等信息。在大图上的global pooling方法可能会面临丢失信息的问题，效果不好。解决方案：分层聚合，子集比较。【idea：diffPool 分层、差分池】</p><p>【缺陷：global pooling采用大量信息】</p><p><strong>prediction&amp;labels</strong></p><p>有监督学习supervise learning：直接给出标签（如一个分子图是药的概率）node-level：引文论文属于的学科 edge-level：交易中是否有欺诈行为 graph-level：药物的毒性<br>无监督学习unsupervised learning / self-supervised learning：使用图自身的信号（如链接预测：预测两节点间是否有边）无需外部标签【node-level：节点统计量（如clustering coefficient<a href="https://blog.csdn.net/PolarisRisingWar/article/details/118001121#fn3" target="_blank" rel="noopener">3</a>, PageRank<a href="https://blog.csdn.net/PolarisRisingWar/article/details/118001121#fn4" target="_blank" rel="noopener">4</a> 等）edge-level：链接预测（隐藏两节点间的边，预测此处是否存在链接）graph-level：图统计量（如预测两个图是否同构）】</p><p><strong>loss</strong></p><p>分类 CE</p><p><img src="/2023/03/30/CS224/image-20230405202957261.png" alt="CE"></p><p>回归 MSE</p><p><img src="/2023/03/30/CS224/image-20230405203011210.png" alt="MSE"></p><p><strong>data split</strong>【idea：关于图结构的data split】</p><p>训练集是可以用来调整GNN参数的。验证集可以用来调整训练的策略，超参。</p><p>fixed split：只切分一次数据集，此后一直使用这种切分方式<br>random split：随机切分数据集，应用多次随机切分后计算结果的平均值</p><p>图结构的特殊性，如果直接像普通数据一样切分图数据集，不能保证测试集隔绝于训练集。【测试集里面的数据可能与训练集里面的数据有边相连，在message passing的过程中就会互相影响，导致信息泄露。】</p><p>解决方案：tranductive setting：输入全图在所有split中可见【测试集、验证集、训练集在同一个图上，整个数据集由一张图构成】→仅切分（节点）标签【仅适用于节点/边预测任务】。inductive setting：去掉各split之间的链接，得到多个互相无关的图。【测试集、验证集、训练集分别在不同图上，整个数据集由多个图构成】这样不同split之间的节点就不会互相影响。→因此可以泛化【适用于节点/边/<u>图预测任务</u>】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Course" scheme="https://betterwyl.github.io/tags/Course/"/>
    
  </entry>
  
  <entry>
    <title>Radar</title>
    <link href="https://betterwyl.github.io/2022/12/02/radar/"/>
    <id>https://betterwyl.github.io/2022/12/02/radar/</id>
    <published>2022-12-02T05:45:43.000Z</published>
    <updated>2022-12-02T09:35:44.404Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="2d0dda2b33d6168e9e2f26d4e504bba8f152d2985289768dcd4cc960a6836abf">d415e4f17f1453c208180d28e3376a84d86c5ad9a59d1f28e481428f03fb92ef323f346a5609df48048393ca4cd79690535ad816882fb8bf05f6f3f11f718b095df6bb9abad638c8e6141ec942ff2c0d3d314323b3cf2818b14032c38ca402945fbef8bfa8658e81e6d7b54bf9c2ee5b156ea05050f9310fb294628eca1b64cd3b178a68a9980605473d8184eb93d3d2d4b07c05da1eb1419da00988185f45362e44701ef5640aa62225c21805e0fcbab7f8778cb2b8c1bdada6f04b75e4308b5031137b84ddd5dcf8adec7680bac5820faa89e397b3558827a9be4e4943b4a847c767cbc1d41862867e492f274796c085d45ae6694549b1f1d9835aaebcb623629e4953ec2cbdb2295aa0f8bdbe4471eecd062b005af918022fc788a3691375a5472f62a2555f4eda0c3211f1565f31a1af2fa5af88a855f19494c3035ca6f3092fe4888cd380ea07951d35a1bc2c5954ebb649fbda8670f739f0d05c80cb690945d82e24fc9991d61b04e37edd578a5437a9394d8a7abbcf978ad082fa7007ee595ec91ab3518bd83e255b2357f9b3e410efd47d7579d7e3dc30856f999077107d6572f05f4981eddc660458dc7f642674d60825eab07c3f3ff67d19b2511b9d00aed5e6b17fc4e2f5a11258cd149fac1fc9a17b9d2572967fd83751b804c459e809e7cb5129ce5c9476d014fa61db61be987271472e8cf2f26878ca35ccb6b797972fe7223cc82a9c6aa026b959213bca9a3409f2fe81dfa099592b553e96343880b383e3dc9f8c915674700875e3f5943c0c6e7c82efd9f3dd7f1c48ef2a40fc1318b26e9259f78aea97009a06e42344eb69cbb892c71120d01e1f44456ab803ab4fac497eb73e38223ec72997f5bbf055fae4b8248a9ace7651c8bb0382f166d47d6c1191c729ca1fa37e7eb2e0fcf9f12363226213e6e9170e23e220f6e1ad4d4f50467e4d1ebfdd97b3e96ea16f47156d8e169a4416b77cb01d41078b62d6bd3466b642d64368e72547aed3189ea78b527313058b490a656ac50e27d5f8064c2bb6fcd3e9e17143d18daca231843437d994636ef682515d3cd457d40e9931871370b5ea3a85d5546fb29159e32ffde0343b09c44a9531970e1dd040bd9a5ff258337554f9ae173ee5a847ae8ca88385fecde5430e1495488d9975868cb349fa8327220d5ada2fde90b44eba067048fc579f6cb8e3a680d8ceb24a69f0fd3384ed37a32709b64075a7585763fe485d96b2a2ab65a73f7e8f94b4cd209f7350c219375d5f920a948eaf0cd463cdedca00d4900e778d5c81c7a7d1d2e41c862cc852ad1781310bfbf36dd54017dbc67fc0f2fdba5688003a616a74c43a9577c619fc525117bbbe840723aaa11751157aaf0178757f9f43f9cacc18e67de5edb3b77dc86809cd6ea6e06879cb3635f89420f1b71b2ee2dd999e14c9702ab3ea7805c0bdde4f70f38f9d5556c460019f2988665fcdf501f744a3a2dae4e102eef2c921d7ad0116cf138fd9d38ab4ff4e2ddf89260008337bf49a850775e68994c344bc2ac659ded6cd851cbe30ac5c2d047e0f2205fd89bd2563b1f3ff686878711385409b22320188ab7e73d36d4d11676b8ec6bb0eba00e00088cee84d9a5de3d94a63efbd086342568c6d90fba9ec0c4dda32c46c80832f171106311958c205933e759f3013bc60f23b588bb7b2e24a31d2cb280e76b27e3e908d49bdd92302b9fd766dfa246acc1fae0bf8c7bca669a3ace31e1729bd4cb8c6c584d6fa50ec140a9595e1571541f567ea4a5b618a110bddd477fd2cd6edd17051f40a36cb3b07fe3008c3f384271dbc02712f47af6def71fb018e9bf233e0d6b372b80225bb1267949ba48ff8855833990cbf2f67a88ad3be3cde787d7c8236229d62e76c4af73e3a918298292d0c3337e092122fd1009f740445a2cd0d740b14a87247e5c7f663038a51a160f6bbeef99f6a0953e88d9fd6fc7ddcdc447e27b888ddfc9a48bfba9dbceb920ee13a32e300579ab55c4b86912aeb6233dedf8f16be24c6c7970379790060c123403ba26daddd2266039103dbe57178430728ed936db1ee16a1a2e8b182155738a7cd3211e822565afc515b31a3835e6212e278df0e7f0dc32fdea38684dfaa59ab8edc18395e1af8f7d011ce669ba7207db706b2a17cab8d3c8f33f6263c0e64bf7cb8e540ea30b8cdf4bad9f4e6dfdd744000a137e1db59b9e483c4273f6c04cc6511e8b69cf2923b042a09a70330943fc47ee8e14627602aa9db46c37b1214e3a225a1470833aa78bb03a0ae580beea53edee253d84d45e539ba2aede76d5a3580b31cfcfc16a9f654dc656f03c6f898b5d5a173ae2b5d747f736e81377ee890daa53c3903fe610e2f174dcf13ac9d0d93a19284e972e7e53ad02114b20abe6c742f7270ea645d361dc14fec061aa00acddf64f0ddfa5f82c5c271ebee71e0078f4a09b88f197ee47112c8217c2476c1a0eac0c9843c43d8abdf167ad11f913cb24759bfdadeab80ff54abe6fe92e50d9c14547a65a02da378d9ddc1cdd4b7eac8e7e2de3eafb4ce323c80ff628d1acf67c6a9370f080f1ad95735330e07621def2787d512a4d502a05be3199bb875ec3ab846d16b101491336aefb3ce13d0b64c65e8dbc59bd02cf23bb7733417ae4748f5ca4116ddeddd1941e256c305faaea2d8e94325a951f46df801f07dbb356c9923ace760118effe1665231d982358833139857804c5151c4a8552630b39f88c0b36f2224cd07f16f5b54dea94460a5c7773186dbd0ba9782d7e4f4982a54dc11a9c0d93588e038ed8aeaf812ea88ec1890cb7b9ae0465670e468d86c466e32025c56dffea37319fa89e83597471cec8b2d58216916e5d5e5e0908d495ba960e4f59d26c9ea7ec49b99250b57ae24bf935201a64135d136385a6eeed18e21946520a366ca5859547fafca809bd794f795e2761e5eccb06d9d214250b01bd9ccd1c303d432ddc63f4fe4e604b7846d12ae803e1ee4f5d66220c9a1b9200da1c47047ea15c11a0e485015d76e5ef4fb08fb60b4979b2bd43ac7c91729d7a058b12f6ac1e8e334b66aca1175e9d98198a0addf947451a558d6ad49f25109f4c0c49fe474def9698c967671fc57cccc13ceeee32b8d692d3331532729799ee32fe7a29741234ca31611a9ae986a8604e3a387a114122a973a36a3c6b9b744d4c8232c143c1d7b2c586131221fbd255a22e03c02231bc49d541af670b6aa465e83fd08eb87812c7d710ad7b8190738df1f6b2468fa152ed67195484a8180c17045bde5a66bfd5d896c69440214c03485ca7e1bc5b26bd7a3297d513bc1cb0e23dc2ca285c736bf28a1e7a96ba6d539997655f02659b8f852580206095f243e4b72bb75de3a7adc4b5ad5ec1b459e92bb04194515c9fb42e79f51349724b05684ad9b56da0822c35119663ad1f8e00a66ec3e136f3455c59d535bebcd14a75392c7619e4a3acb7e331e68dfa46e13d32b15d23515ab9b9e2f09f465b913fdb23977dc7702215c375a71a8c29376dd0f83baf71f5a607603c6330369bf5eac34d044faa8c35503c20c58c74280384ec97b9f56808e8e8d21f241411a69a70f1a9a0bfdc591146595d1e87c7723c5c3a755f6497b1f68835080c60341fe02be579256113f3c9bf28214b4636c573b24c8047a0c9ca749590a50eeb931608ddd643ae161a4c8e77f7296de2c40e7e8b4827d75398543a4b641647f7c45395666719eca02a17bf46cc6ca6c596c257cbbe3cb90ca1692635b5ee5c1512bac309b89c6afc772c70d3a3ba1a1a75b374760b352bae80bb6866f174b3abfc564731e7828b424e4fe0f3bba36d9b0a1393af15c098a792a33a675e8ef5e721e0b43ee7aac4071ec942553a65707dd2c2d2ae6ff1ba43f56bc0fc50ee49d84f34991e9a37de4b1128b793746f914e3590f113309ee5e90c43ce72f65d4906531e3c37283fa3f3975bdcbe974fcb0d0715cb1b06fbb4ad22c4edd445572cce94690d1b04ce7b8ed970ac4bad8d3c0672c70ad044cc3ade0bb47f9d4a0d359dd6bc6912ca429a76b4e5de532a2fe13e8ab90d18a22c0f7b8c9222cf9e60b780dc427095529dbe7753ac198d56ea353aa22d88f553d43265b1ddbb3e01687d6e992e43716217cc723d559ff6d3af9176daf1df28dc89d5e4435d36a074977759e7957e76ac48675b413126f7e33d1255bf2adf522dc839ddbd8a7ff0fd1f0085c69c06a33bddbb7c4010cab0bea268d1e4d371935680f3def94b5238bfad8f928ddba8f5082aa9732d3a4a527e7e97d22e3549abbceb91181e404f442dcf918911c64755a3022b6a4a60e1d4d018d9cb6be9a9e96d53fc29bf4b7f7f2f0b9956b1b476eddde264a731c087a7ffadbeae567ca4bb5961a0f07a4b8b174d21aa0153aa160714b9167170d6bac7848838c791b7f58cebc05b25327505d3982d6e527da4944642bae030b93d28936e6acdbe60a3149d2e12b5f2eb87bc4b0f6aad26dc5d6bb64b044422c88f31dc33b2ec3e845e5eb915dc8c0c8ec3498dbf1294a5b9f07af8a018cc43addb135832b116c3ded9a40c3b3c0019f11f21c88528dff5a427b489ffa3d0d407531f0251161efed8cdb99d509c62c414cca4e3bb31f73de551fb3d38cc42247f58424781adfb7659009bf492dd1e62628e2d00cebb86e2ed292c8323813c31726329bab39fd1b02a256ddf1ae3e27fb8a2f3d176ac93eaf2cf4b05381d757c6222f30ed48e9ef83207d2b49e2b85606409b8ce8c44ddfdd5fd90a1349ce275eb1be2756c2233a77ec6f846b6011972bbb93639eff475e092848e27e8ce2e575ea24bdc39c41aa3bfff27a39b6586089ce999c72ae8679815f8c0bdb33271d30bf4f129886453be80d84e5d4cd06b2bef90efcbeb414c211914a6d4f17a47883ea2b4e0d508f1cd6e1c7083a288e792e2c6245751b62694cfea86883a8ac2dfa3c9d4c29610ff7e1697077e5d105111d5de31553e32b25df161e42793883c86dbdfa10959edf371621aac7ff29d7cd84cc32593df4a9c799ff9af5a145fa0315b007fbaacda733106f383cc8b29bed8d839a118cdaaafd375893afe6a12b162bb76aef610af9ff92b1e4500885fd5460a6173a690017a7cc83a209369ff5fca23b4ad447412134b1d613e5a07984019aa5fb8cd60bb447256665598c1b608c825c709c4fb3661c972e7fc0991ebd3c601fb0af7e0325f070a523687bee79053c15d027e400027a53a1a4e953fe8e0752311aa08ee0779d928f19a39f36a65c79c34489b9e0cfab761754774d425c91649b72ef38a02b9e57ba78a114a3f099ac34abbbc70a5200af687caba894e4b0a85854dbbc0309d0b163184a6b3e4fee7b25a8c1ca74109ff796455100cb57eeb6cae1dda9dbee17526a9fb4b03bc2bdf3dc7575a7e84165a379b135767373a597965add8b623fe3b70ba286afebd561f7b3bc3b830906dda51bcec678eab22b5de29078c4fbbc5a326a6821575786369d65dcbe19500e83eedce1ab4e3943f56bbde149be7a15cda5f87be41ae8c4789f8216284c121ac01fc62d260f7f08fe69044a0109910b0de69545049fbdd9f3977e2f415c191ddb858ca0ac6b46c181c399895fe6caed03d8cfe9db6e9667c14714d971edd5bca1cd245a507097ea04a41a96bbe7e32454d3918ef40c44ffe2c1c31995fc0e063d0f56a4005dd53a335ada864678cc1e4d3a8ff88a181c834b815059d90f37f137fdf7b6719dd4ee50819d1219412745bbaeb83b4922d6ee738030a59c1aa78b6724fa9cc2f064bf2091472324a55b29ce4cbdff45babaddf938f2f4d1762a641269a61a434bda7872d9a4560a03401a1ab0085b31ae62d424079fe8c9ef75a36468eda93bf63e7cd58a25f1faecf39a3bb9067dc8c5eba2a88bb248a871db0109e1f0a7a00bd9a8d64ddabef666c5f1f7b9a2e91f64ae22ae0b127aef4f726a3b9d2e6121fd2ee8bc3eef6b86cb40e5469bb81369bf691c38785b809085cd2e5e48aced2e8a2419eef1ada842b4060313141aa827d9a805b094fad6a4529d5c914f5e40cb36871c5a317174d547ab96aecc3ed95a52309eb7a22ee34fe5777538bad2517896fcdd75fd9c9252f6a966af12012eb555a44723f35869edc26e61951b1de159fb4d5bf58c7d63141af1e923b93839446c6bad04a84483ba022965be4d2f8c52d5cf63a3efa3cac4998d0f16bea6156ae45d8f6f5a174bb6275150abf3c803a587d0a42a841a79810197273d13361f7a0fba139f317b7a40f61ef06f5172daa13899c4c2a0d5162ba0c573d738ed601c61c2106a153125102345fd81f1070ee54f9e2215832ef55ff87fc0e524480cd29c242514321be37fd44c108e170b9fc20c022856f0de438ffe5eb1763d13563b9754153fdd06b8445aa4553aab812c11b93eacbcdb095bc6f99092364db4de6e9a5c85adfb51cb152e969227fefbec4a7e675da0afd6e2d8dd67885d65f886ec54287a783135ef7e99af5b312d5731181b226d5c97051fc6a24d34d0df16b5795ff071dc3afc01067be9f241c698eba4b09b6f7c8abd593dc93f759fc60c503f96db21e733b3b3bb2c659e59e10adff719039f36e451364a06f512393c10008401f29df2f7365e4076870a6bda1bcace6bbf93c1538e5a3540731a6e550d9f2fc12dcc25e480e91a8945c82c2e1770892c9032f941b7868cd2f51b1aa2bbb03e7302fde4afc3cf3f1b25fbf20b7977a33c0d2d10dd7dfe7fa169986fe35c1851d6e1641fa37be867f94ded53fedd7f6d85dc857151316e6e6fd70366782f5a88c2b7f50e0fa8d406e1e75ebfc41581bd98443ea7ab892e920f838ccbeaf5e7aee3947cd92c03033632bebc652b96a406ae5294cf0dd70b65fd928c66a20082136616aafc5cbb6521bf5f4d293da7ff2804b1fd235ca2b0c4a2292efd942b7a2d2fc422b24af7b582915423156faa9ec21ed05aa41b59106ed9d807a546fdce8317b8b61405f6b265f6108b2b1756af82d5f5c746d81f4cb3b4cd09402a76e0d098bdb3540f6162ea8b6912ad3b5251c61fab671d6aaeaddd33f009cd63ba55dfb02af9c9c89c67dd30cfde5ce4c4affd3718ca34ded6171593db98d28af17e109f2fec64f33314d208b82a8225f519390e8053b795cde2280a5f7eaf76df70bb543d4e7b3036131e2220ed9dab0f27ae6b0a87648633eae3ee96bb51236a12b38b240e64f0072a8e170cd826bc3df3839ccf04e2222475b0eb48ecd95c7911042e7f1eafbfc386bd1b4c63c63494ec5dc44b551fa2db6b7306cd8a618e7e114b7bb64fb64c42799c520bd6b19271f499846f234997564dab1216a401d55dadc5864dde66431e5542ead0a3eed4849c54bfef8c2b95352d6d84c351cc860eb0a69ce0c6ccd88b7767eeb5d89568a3030771d4cede840f17add08087d385fea53b34ab71f964554aa317629d7852142076d95a5e6045d965954e477139f5cae567af3967d06a85b75bc4dd443516e1e7572455621a3fe2a82276c2904016d8f111bc9355f52781c743eebf700bc4f937244c383e40e99b9da4e49d530ef05c27777f59fefdfabbbde057f066a5b87f0977947f48d01783f955be7f2ae20e903a3e140f7f53ee09e91ec46c049312d89df97b3ef31d62ba22ccbf11e67022e6c23550255f50a85b4579b66abfda2023df2503c05178e3abb9019de5479a5381a4071fa859bd5b281c7d695fdc4df610056509c7be8e69f25bdc067618192ea1d05c15168f081e390a65565cd83ded1a9479f9c68fd6d994c749a014ac362bcc064bf56bd6451ee63ec1bec74e99ca297a23062adafc5570a720b0a757e3b1729fcbcfe747a2593e6d45b0cb77b429ad6b3330848df17b6dd84c5fab8083475a839ed96f9a04577d2d1de818ef1fd8553b67bd969ab4f1777b32342135f6c813e4ec70010758a9b5d9ca39e483f9a88ff6b7b0e4dec901c095c802aee1cfdd35be56a29c540744f4c5537fadf5b3a9e6e444610a2dd41f8ff146f8933d4da0e5e6c9a113b9c8a9a7b07f2dbb404065d4cfbdd27a150535b48196f67d19e652beb74bee12d1d3cb497325926f736c523e9b3823bc104a40653b620a7b060b935365988a8e7ca1c84e6c5969f9e0feae335dea69a8f45c8c3d5d59f6cffedba8c1eda1150d472e033190bed231b4f788c5c6561b590cf847e5206ae419c273098739fc5563ffca9770fdd3cec6be3187b056a43c0f009152a966e362bf83c8daaf10229ff08e08822c0c535a360169aba9dde0b62796e70fdd5ac1797451</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:supervisor name】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="学习记录【Radar】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90Radar%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>coding指南</title>
    <link href="https://betterwyl.github.io/2022/11/27/code/"/>
    <id>https://betterwyl.github.io/2022/11/27/code/</id>
    <published>2022-11-27T03:43:22.000Z</published>
    <updated>2022-12-02T09:33:30.222Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="0544f65466b06b2cf62eb91154fafc6f1065d6964e775b67ba715fae2ffcfcb1">b0761d989dc3722c80d9340751a72af2f12a37b171181f2c4e12fd09bb2d1409a2495a9c2461278e57fca7b51f1569b5166ffc4b8853ee6011345aa620305a3ade2e9a0e733b786d74cde3e132c60f039fe91e63315e0d0bd6e530a5d855ac56408869021be25df7362dd56728dd1f979c5ed3427336d9e2e0354296afe46fc238ed2b405cf82cf264eadb8d60dca529cbb8f6dec4e7487952247b648453f283e7eb0497902a2d7a9bf6d0b79366582decbb1a3f962211ea2086f576c4a9c302aca0572ac2d04776f7bfadffe152333a87964fcc40756322e6dcdbd94a450f6c3e52a2b6b1f2880b1f1626383405b8b008c71c2d927f4ce1dc2a085da061db2d583c67033c89810756dde7b09d82b5e0ff310ac18970a2af15c197e3da29b125ef3d1e5daff4fb9227028c594e42c030fedbee46c17b358754765074a2e067cfa103c40d6f3f79355ca2648b34206dfd4e5aad23a4b18b5d5ba5785c808f12371452ae03214001e2d87ff5c67a69625b5057da52d33962279fea28c48ee2a8f3b88760e5fb15fc5a0f362993286351b70352d5f898cfc0bf576ad44ebcb60e8410890be5b67435519dc5484e497919aa1bba471d0a21a80cd34b9f5bc3eba5e66b70366db48567d9af5a78498cf88707aafb23aef0a57eb550e13b3aee9028bd0c6a1c1b918e596027969fe48c3ebfa20e857f4bae2719d648a32703f3b839a1d9b8601b01ee7601350b81682b90deaa2eb29686247d4e5a7d2dee611b775e2de235a150ff747e927e26517ce65598fe3d84df1cd19f19e21774629afc94e8fe7a8b023d7f3471756f484fcebda71bc5b929f714700ec081efb5a3d1361d0f45cc1bb8867b821510157fe82df432ab9437b81ecbd9b2de8282799fbda172a43eaddf69dd66cf35eac7b504c9a8e534c2043a99939ca1fcfa88d2d9b53b507f5a7aaba397f4e06ece5489e8c60d477df7b0a9e9fb00d8209fe1ec12ffacd728fc1512998cde31054993161d065da81cdb3b8fc9f9a1516087d82400a34ef5dff822737a99eaab343c4401f7301d9d300289bc44fa0fdd1da032c2878f540061a058640da197f6be8184fa1e5ad4f3e780db4477c7df1d4d3d1c22f84b4495ffbd2b7125e51c83f763ae2f8753cadacc10f14c3a0ac6c9761574903e1b286ba60604e4515076a4afd33502f97d275a5a0d4f7aa8f866993f0da91802c7b93ba510added596179a7f73f2fbe03a00b01fba30a88803754b923da6dc684f571a3810a59f8725c07cead749b6c35f5ccbe97787772c76f3c6f227efde31f42b3c6d134e77d6090b8491f4606491cdff37b68dc9898bf79107a9313cee41cb6ea6506b3101666cb947690590eb6781d62a11b3722f0bf09d84b0a33dd822072abf9a3983cc0b3f361f5692c755253d06aab65fcd74042938df64e4f3a0d3b0b1455adb25b93091e1b5a2c1c8c540a2506bb56cec12d334dd613ecac282b92128c749db11c64b8a7daf6feb5e782f3c9b4b990fd9289c0d85e581bb9dc084acad749448451976fb038a46385585405e7327c4845141e4eb6a2ad4361913ce8d4c1ca34aec66395f094e7cebfc24b326443448c8c0f2d4798b13f1eca8f9bc5a9ec67c70ffc6e5ee63fbc53537a6614293a205450b9db05098a0d1426f78f00a0ba50c590386c5ac46b398cf39dbf12e2c23149ecbfc0e1061f7fc1b4d925654bd42c7ed0a654b3e3ddb22f783027bdf8d06e308a0e9fbb6c9d3d6de8d5f1f5921f6ff18cfe3dad57d9d1126dfa2bb8823c19b9c4a903e528bfc511a2047515f8a89094d481fd34507d709b60d3c526b8863f760cadf52db90c570ac6c8e8a0a870b865b2b04cf8ef623b288fdc0d879c4b74ed3f8e75f6010fb49a638f5c7fb861ebe51e65bce135ea5ea8693b56820869d1d2a32af99ca9ec7222c9fdb1a0577f5507f52441b9985b1550bd655699e6716f5986d96a98748404756642c7e3edd81b412021662b0ab32c0da226dc421581410169325b48a54990fcfad16893481f49a4bbf9add874bad1e18a9ff287b57400592fced09d5acf95c6abb0787abd00149a8f86a2eefabb742271d4b5f32564b20637610cfdb46d13f6e4181408d3a663fb87c01bc35fb07a82a60387b0be2da710e7f7b1bf018099e6a17a87ebb8a4de7c94d7cddb44cee9a19ee769e06b39c13a56f47234cc851ace3c1f79e2be9fbed72d5276c391755f02b4f2b1b120fdc20ac25f53be50105006aed9d06cb2db0b22d890eb260e007a555ffcb5d878e3aa238c54f9405b30ce5260ac5f3d9422742086121376a5e8c6a2508c15441557f0fa5796538b88d0c9913eb32b3d31016d9bede705777652ec0aed14fafc911c4d0dc26a2ab589955dc57b8f43527dd42ac68417ce2a34ff19300564ad300469569dffaed9557d77b6d262ce60734dfb6bfe62b3fbcb27f3708cfbeee2800e858e1c6790bac16b3423eba981a623b945d15c155e8ad97982466e87d5002d30c589900cf39749479d4807aefbf8b3309c234179963f1b88ee19a381b2062369f691e9ead5cc7f0b1fd32f0d9bdddf6ff5095901663f455368cd92902e7db5266c478811cf1b0e909c7c6cb395a79be1a3c88bf04c2d9b9756b29838bd6be99688ea9848caaf5bab2ac820b460011cf953c30d8de3cc481e3cb53a3dd5c3d04e6f4693c1c9d30990ff5749da39f310e8aa118746f0107426bb5c85da93b9751f60c5bb9f4a57ee32754f26641ed774c1955851f0f9bd604e039e0f38ee28226496186f2fcd1db55e22f527a99e35b0c6a111eefcb34e0daba46373dde052e46fe661f1c69a53c2735e7b8caa038e67571111f7ee8c0e82484fa06ac45061843d02026c69d188f6a30815458e30418d7fb2242764599d68123d00a3e5e098625a02e3ce755a75529e094e64123eb9534bba1457e92e192a6c237ae46f2c3a79b183a78e64416b4caf8c1106f70067248c8f07f4bdbb816f3b0111e2fe94afb5f93649fb3868a377434d55ff1d8724e7a132284e039bb718b31be628aa0382762c5d7476bc87169c21361afccd878d2e4a6a83d3572ee5bfbb7f52bbaf6ab8fd12871b620efa933c9a44d387f3dd44400e5644e827603f948d9c6213dcb966972bace696775574393782a9b0056792b9a50ea7ab59a06694acb6bf86dd9902ab3cfc305a84bc9bd6036f730c31b47ecf6bc6a1937876235c28c80c76ac7f85724729f2fe8c09a1a15fabda4829fdf7b1f99a8e03c3c28896e352884272c330eaf4cd899ed57654d44b3d39e367c494ec72f1db80eaa93861086aeb7edd1328c38b63de2aa1665052b74bc18b571b0e73130485c3a95da2fcb90f0c34b0392018dff4e8fd44e65e127075201d30bb4391a93fe677104810972f816e0a4bb875b243b507e88916e298d50f4bfeaa08cf8b9f948bfe3fffcee1efcf63290c58bed994710dda583e8d6965090b4b8f1481553794183934d5d363593a3cae6be8fbc753dde2bf608d0be971aee485e8e5b9444bdadd7ce0eac87f2f02738c61b8781fef1d5120b35b4f7f96f3b8a869d446d3399292c4ffaf3a42302711c02f6f271e8981d941950fce584b84a3b531ecb6fa3a332753097fc675f15ab4db205f1dd29239c8878a886082fcd3705e1d6dd2c5135127903191d3c1f3359f68b4aff5958d6a6a8fe92d9149c6133bc3c39c5c4069fd5ec7fcb32ae8749e03867537696c2c0c93ced2bc09b3a2e2dc67c1caae154ab563b5ac1a485c17e50ec6781aeab8a0506d97c239a9c6a13678681c88993917ad450060fd384ad1d3d1551864e980197e79930393a04cc88228d9c5041532a1ea92321ab2d51290609426cb4d2a899448f80937be2dbf3000fd3e79707e9db33315c4feff291752b87cf704810a56f69dd1758530de59e8ff9b16437ce1b0f5218aec2ed27dee1c46f5c0cbf53d7d61a4efbc2fb83cdae6bb9fad39d41058822b0cf2f8b2df414a01ade0cab54b0f9d7366d36ae1c54f6ee706ac5be5005c3b2ab97ddc4f060300741e04b1c306df4c686dfaf1cff0390ed2af46248d6545c7d15585cb7af9698f6f7113521eaaa5fdd6434572019750b0c3df70ede64fd129f12e560fb1cee6fbf2310b2903a1ce0c154370a8560af284ac735913a1759e86780a05b624953516afc6ddf0691d6df9ad01701a5bd140ff5ec2e89d8ed9d35157be950c28e34f40b9fab046f2d96ece08380f9b45333ce7096ba9634e3f48ef312f6106ce89b24c21a76d71ef5df1a4d9c9f53bb494620182b4fb467370fb36fd32bf9a80fc32bee8cb364a5c91acc5aff384395bb77dc4854956c203fe571fda5a42360ffe64b475cc634a8f2112dde8579f9875c7e5605e653934b035613fcbccaf70c7f9099436fbf84c9ecdae160de36c438cd84f521845b48dfbc4aeb720c4154fdc473c93b75155d6d0e5a73ad4a75a6bf1ee4181aa11fac9217d4e0b294484361c3a5bfa8d46fb2278948a08b0862ad2d3225ba363feb51280976529f074d09c9505f0898f6ba7c629f2c4b0e48797d48dca35f98b01348f07defc96af7139f94f5741e39f19b25fd1108a38618949b96d3ad159f0b8364fc54ccffe3dd7b8301b8062452e100b487e53d00f7e7cf48dda4c4b8a2571351f340ecafc5763d364a884291211ac420d4bf311cc2cf670242e8a3928359261073912748d0d4852d10571a59868fd2deca418b553a5fa7b858decc332165608832fac786fd12835c32bbf39b321eaf104a4b215c6e808b8fbe12bb202d7ea1338843a7a902c6f926321cb47dc6a23e7f8857fb43a715742c112aee15d069e49a22a0c12e01544d3c7fa5525230ebefb8f7f8b3b26f1888c4b9090cc480b2a8ce13823a623067a274b9d810ab9c7753a0146561d2fd5ac4085446be00368143dc822f33d832ce596505be41d43f150eedc681cf6d690ffd22cb2937595d086e4274a690df6d00279bebec187280776cce8db06777fd219342215b5d1326ea21256e0c629abd8819a8cf3db14171297192923ce2a69f7b62dfa3cc127c958f2cb8da43bede21d8cd8277abfca509ea994988bfa8b50bc76bdfbfe344578f6191c4e7530ed6333e137292d8109ca1b5faa60c687f66c6079f084d3fae94d62d899a3786c221bb63a8b503e833dc6c1c3f49c80731db0c1a3f48156b217d80699e7408c4f8bbfe707b2feab87d46060c6fde70b92542841a470342776957df40c3e9a4f3d1aff0cacce99f2bbf15b562c792c9bb6fa2b70e0c1a9e6f0c71a315f3a532da2937deb82166b044f2500350d88a5322062787c2d43619b691313fbdb729515cb23d46dec20c7e18696f20df31fb861be7805601564d4e0301d79ebfd85be45de2c0f3a9f86a5deac5faadc660cf57b671f2b35ed24e290cd236098e9c734f72ac2123132d1fa1b34b3c52fa4d7c1b4670d4364b467049598af7f19fd3394d0756544e73840aa76710dc2d53b3d00d64d4fa901274540d5f20dbfc4986abd984d3f9f4c48e8a9ccd12f3fd0cbe5d1ee70b068c682b7469876bb123c7928edff4f589c6b600df3c0df0a55544ec63dbab0d21af14d709f45172c43e56c8f38796d59e09e2b131230fd02ab90d287829ddcace3c12f0111b5f8da6305f45bffc98d10031867fec963eff148369ae9a15485bcd787f26c989f396a7422630abd7c6c229f60cb3e93ea7bc7689254d01c3c11f11048adb1befd71921649ea88339439552e4c22ecefcd527b913010c5a463776b506a84d0d561796926f684a8be52a02193e426ecb93b1d1a2cf069276ffbb77c0e51b127d9540d6d73e9c6bd71b3079cdd580443697c85e53b36615b643cf818b3193da35ba28384eaaadb94e82025fd7dac077746472b18fc686a7f60134bcdbcf88cc13cca4807dc781aad605a4260af629bb5419cbf470ded23eb22d9917919561613889d5167d258068e541b30f8ea3bacdb5ed20a02227a6b6b3d30a2bed4e5d452a4cdf0d2e5efb09203257ba2daa2e32f25fdb2728f6cd76a00e1eb5804371a431185bd13113c9e5cd800ecc7dd342b5c270d98007cd3c9f0627b1e0c249cd5326fde964a5b19d12ebccab8fa48bff7858999f57ca85940d961c892485837bae7ebbd743e9ece4eefc073b0383925c3a4451010c3fa1044be48cf49529ff33010301f1ae77dad67d7398a307272f2da4c14dbe606819b6bf36abbb24a2cf030a7c8f35850f8aea444a99cb9b50cee363e5533fad136bdcef1affabcdb948256f5a05faac050fe2e304ee30ad76c7370f60b7e86ff9de397e6b8025c31b79001c2fd43c9a14138ac1abc3e9c262dfbb095f732d4eebb4c2f0b59d959aa4bd7ff13b63876c2155253c32a7bcd44354f800f400358be8dee392f8ec65c6a07c59ea9a317544208b640a70ce4966a7a073d28414961fa54f685229892fd01f20545b4522d29b3daca75e45669f4c6d59bb18e461ea91e63e1f8b779371526049ab606bcc73af229a9f8ad6e87b9cb06a3b6f3ded67e19e4162c7cfb409176abd7852e4fcbdfafbb77b33f634f9eca95e149f699be770b329b373488d10e9d699670ee6684a0c0803fb60172d92cf5f81b08f46cfcc8baba3d4bd6bb337ff813eb6e8244aae948696c8f56d9b28980c3690c32bd30783acadb7327efa8a6877642aa840ed01ce3a2753fcf1522a2a44a192d88a15a474f2bf4f646a4fba65ef3f2297623fe054812537cc2dca7de92d21be02cd10f488733af63a340fa70c262f0143b541f123a98d1d16fc8566e86d086d9bda24ca69a3c7c6f33bf01c56ed52c082140ac671d953fe8e250becda8dbcc631fe534fd10221c20ff478d479deb8453959cc36da0ede46e1bb8f257cf2b95e5a4e16d67603bded8d6e84d7825182fbd88d711b5f467187f63366a0e9803c0ca2ad5a5b560411cc5497903f0e073c1fc88ae387fbd94715585dfbcd76663c40fd36f66800daefa29125c5315e006c6e990c61aee86575d6120b4cf6a7e9daba06060f78764238f56746b0289d1843ea098365c6696aac39ba70b1837cb173db39b0f6f21abb67b931579671ebeb218c6f8e13d3d0f1b4bcb32429fe0d5fdb148f1bda14324cf045c81940689b11cbc1ad9c1be0afc9448542527307bc49b85ff950add4733d97d6e177cdc0743c9e545a79c5d85e2912d32df3e691b1369c9924d6543931c4bc488780eb74eaa6fdc2c49eb57dc59209361059d07691a4676f262b22254a92dbf86cb42f19d1e750ed209c25599f1523e29a196246281f278003f6b98c001dc29266d284047653d851e50b179cd063ef44e68bebdd28b6a0337b7550ee29a182362b1095261663947b8441b167dce828bee1f5c3bd6861b80bbf1988e7f3fe427ef65f0159a0ace3ea8cd9dc0c7dec8d546316715d73daed0fa6fccb0eac28a2c189d101895877025df0eb7cb33a515ef5cee3da83d0464afddd2de0e9a7985ca6e9ef50f50ae706f296f52033a22e04ffba26c01b72c373efeed6</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:supervisor name】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="code已复现备注" scheme="https://betterwyl.github.io/tags/code%E5%B7%B2%E5%A4%8D%E7%8E%B0%E5%A4%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>Inductive link prediction in knowledge graph——A survey</title>
    <link href="https://betterwyl.github.io/2022/11/20/survey/"/>
    <id>https://betterwyl.github.io/2022/11/20/survey/</id>
    <published>2022-11-20T14:21:39.000Z</published>
    <updated>2022-11-20T15:12:09.683Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="This-is-the-official-repository-of-Inductive-link-prediction-in-knowledge-graph-A-Survey-a-comprehensive-survey-with-regard-to-inductive-link-prediction-the-context-of-knowledge-graph"><a href="#This-is-the-official-repository-of-Inductive-link-prediction-in-knowledge-graph-A-Survey-a-comprehensive-survey-with-regard-to-inductive-link-prediction-the-context-of-knowledge-graph" class="headerlink" title="This is the official repository of Inductive link prediction in knowledge graph: A Survey , a comprehensive survey with regard to inductive link prediction the context of knowledge graph."></a>This is the official repository of Inductive link prediction in knowledge graph: A Survey , a comprehensive survey with regard to inductive link prediction the context of knowledge graph.</h2><h1 id="Please-feel-free-to-CONTACT-us-or-COMMENT-below-the-article"><a href="#Please-feel-free-to-CONTACT-us-or-COMMENT-below-the-article" class="headerlink" title="Please feel free to CONTACT us or COMMENT below the article."></a>Please feel free to CONTACT us or COMMENT below the article.</h1><h2 id="Authors-Hongyu-Sun-Jian-Luo-Pengcheng-Li-Yue-Shen-Yulin-Wang"><a href="#Authors-Hongyu-Sun-Jian-Luo-Pengcheng-Li-Yue-Shen-Yulin-Wang" class="headerlink" title="Authors: Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang"></a>Authors: Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang</h2><ul><li>H. Sun, J. Luo, Y. Shen and Y. Wang are with the Department of Electronic Engineering and Information Science, University of Science and Technology of China (USTC), Hefei, China.<br>E-mail: <a href="mailto:sunhongyu@mail.ustc.edu.cn">sunhongyu@mail.ustc.edu.cn</a>; <a href="mailto:jianluo@mail.ustc.edu.cn">jianluo@mail.ustc.edu.cn</a>; <a href="mailto:yueshen@mail.ustc.edu.cn">yueshen@mail.ustc.edu.cn</a>; <a href="mailto:yulinwang@mail.ustc.edu.cn">yulinwang@mail.ustc.edu.cn</a>.</li><li>P. Li is with the Institute of Advanced Techonology, USTC, Hefei, China.<br>E-mail: <a href="mailto:pechola@mail.ustc.edu.cn">pechola@mail.ustc.edu.cn</a>.</li></ul><h2 id="If-you-find-this-repository-useful-in-your-research-please-consider-citing-our-work"><a href="#If-you-find-this-repository-useful-in-your-research-please-consider-citing-our-work" class="headerlink" title="If you find this repository useful in your research, please consider citing our work:"></a>If you find this repository useful in your research, please consider citing our work:</h2><p>@inproceedings{2022_excellent_survey,<br>title={Inductive link prediction in knowledge graph: A survey},<br>booktitle = {ustc_greatest_survey},<br>volume = {1958},<br>pages = {666},<br>year = {2022},<br>publisher={ustc_greatest_team},<br>author = {Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang},<br>}</p><div class="row">    <embed src="mydocument.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Workshop</title>
    <link href="https://betterwyl.github.io/2022/11/12/vnietest/"/>
    <id>https://betterwyl.github.io/2022/11/12/vnietest/</id>
    <published>2022-11-12T05:45:43.000Z</published>
    <updated>2022-12-02T09:33:08.245Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="057d4399f8a546af0cc717afb7a958d1695b784a7d92e50334f6c1e56b055aa0">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:supervisor name】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="Workshop （小群讨论分享版）" scheme="https://betterwyl.github.io/tags/Workshop-%EF%BC%88%E5%B0%8F%E7%BE%A4%E8%AE%A8%E8%AE%BA%E5%88%86%E4%BA%AB%E7%89%88%EF%BC%89/"/>
    
  </entry>
  
  <entry>
    <title>3D检测</title>
    <link href="https://betterwyl.github.io/2022/11/11/3D%E6%A3%80%E6%B5%8B/"/>
    <id>https://betterwyl.github.io/2022/11/11/3D%E6%A3%80%E6%B5%8B/</id>
    <published>2022-11-11T14:20:15.000Z</published>
    <updated>2022-11-20T14:55:23.987Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>形状学习策略与不利天气、遮挡和截断等导致的点云质量恶化相结合</p><h2 id="Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】"><a href="#Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】" class="headerlink" title="Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】"></a>Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】</h2><p>数据集：KITTI<br>分析形状缺失的情况<br>外部遮挡；信号缺失【这边视为能观察到的，只是丢了】，表现为空相素【我的理解是过于稀疏】；自遮挡，远侧部分被近侧遮挡【每个物体都不可避免地会发生自遮挡导致的形状错失】；<br><strong>创新点：</strong><br>BtcDet是第一个针对受遮挡影响的对象形状的3D对象检测器<br>Complete object shapes= observed objects shapes ∪ the occluded object shapes<br>根据估计的占用概率P（OS）进行目标检测<br>形状占用网络<br>assemble the approximated complete shapes<br>假设：<br>大多数前景对象类似于数量有限的形状原型；【处理：利用source point来补全target】<br>H(A,B) →源B是否覆盖目标A的大部分区域 最终A形状=源B中得分最高的Top3+目标A原始的点云。源对象来自训练集的其他帧。<br>车辆和骑自行车者，大致对称【处理：借助标记的边界框，镜像补充它们】<br>识别observed objects shapes ∪ the occluded object shapes【是在球面坐标系中完成】<br>使用均匀间隔的球形网格对点云进行体素化，以便可以通过任何LiDAR点后面的球形体素精确地形成遮挡区域。【就是这个S可能会存在很多voxel中】非空体素若有包含形状S则为1，会形成一个occupancy 的概率。<br>用了一个占有率网络<br>Occlusion-Aware Proposal Refinement【plus】<br>将占有率投影到RPN的各层feature中→feature拼接→ROI pooling→优化的bbox</p><p>PIXOR ing</p><p>————————————————2022-6-26更新——————————————————<br>MOT论文</p><h2 id="MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries"><a href="#MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries" class="headerlink" title="MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries"></a>MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries</h2><p>代码：<a href="https://github.com/a1600012888/MUTR3D" target="_blank" rel="noopener">https://github.com/a1600012888/MUTR3D</a>  nuScenes数据集<br>端到端的多相机3D跟踪框架【解决问题：多相机进行3D跟踪时，会出现检测精度降低、复杂场景中的遮挡和模糊、边界对象丢失】<br>创新点：模拟一个对象的整个轨迹的3D状态，关联【空间和外观相似性】对象到3D轨迹中。<br>Metric：评估当前3D跟踪器中的运动模型：平均跟踪速度误差(ATVE)和跟踪速度误差(TVE)。可以测量被跟踪物体的估计运动的误差</p><p>自回归的方式逐帧更新自身→解码器头从每帧中的每条轨迹查询中预测一个候选对象，并且在来自同一轨迹查询的不同帧中解码的预测被直接关联→<br>损失：新查询和旧查询<br>新真值目标作为查询的回归目标，在新生查询的候选目标之间执行匹配。旧查询：先前帧的活跃查询。、跟踪当前帧中之前出现的目标，它在第一次成功检测到真值目标后被分配。<br>查询是有生命周期的，在代码中设置阈值。【查询更新：使用来自历史帧的特性来更新跟踪查询。】</p><p>两个指标：TVE是在MOTA最高的召回时的平均速度误差【可作为当前3D 跟踪器中运动模型的质量评价】</p><h2 id="Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving"><a href="#Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving" class="headerlink" title="Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving"></a>Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving</h2><p>数据集：nuScenes 3D<br>创新点：3D单目Detection和3D MOT一体；异构线索整合【编码外观 几何特征】<br>信息跨帧传播、估计相似度以生成三维轨迹、整合世界坐标系中的几何相对关系以估计速度、属性和框平滑度优化。<br>1单目检测方法：KM3D+其他检测头平行的Re-ID头<br>2异构线索整合：对外观、几何和运动信息的兼容表示进行了编码<br>3时空信息流：？？？？<br>空间信息流：3D探测器的主中心头提取图像中的中心点，+外观特征和几何特征， MLP层连接以生成其输入。<br>时间信息流模块→多头交叉注意力？<br>Loss：<br>单目3D检测损失LMono3D、跟踪损失Ltracking和时间一致性损失LCons</p><h2 id="MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection"><a href="#MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection"></a>MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</h2><p>创新点：去除中心限制，基于深度信息引导的三维物体检测方法<br><a href="https://github.com/ZrrSkywalker/MonoDETR" target="_blank" rel="noopener">https://github.com/ZrrSkywalker/MonoDETR</a><br>数据集：KITTI3D</p><p>特征提取：<br>视觉：多尺度特征图 下采样8 16 32<br>深度：下采样16。两个 3×3 卷积。对象的同一 2D 框内的像素分配有对象的相同深度标签。对于同时在多个框内的像素，选择离相机最近的物体的深度标签。<br>视觉编码器3块，深度编码器1块。<br>每个编码器块由一个自注意力层和一个前馈神经网络 (FFN) 组成</p><p>前景深度图方法：参考Categorical depth distribution network for monocular 3d object detection. 深度离散化为 k + 1 个 bin，其中第一个 bin 表示前景深度，最后一个表示背景。线性递增离散化（LID）【更远物体的深度估计固有地会产生更大的误差。[dmin, dmax]】</p><p>Depth-guided Decoder【不需要引入额外的深度估计的监督，3D GT boxes的 Z 值→深度值。估计深度属于某一范围得概率，然后根据其属于某一范围得索引返回连续的深度值。】<br>检测头：<br>在深度引导变换器之后，深度感知对象→基于 MLP 的头中以进行 3D 属性估计。在推理过程中，输出3D 边界框。分别计算每个查询的损失，将无序查询与真实对象标签匹配。<br>损失：<br>六个属性损失：两组。L2D 和 L3D。<br>第一组外观：对象类别、2D 大小和投影的 3D 中心。<br>第二组3D 空间属性：由深度、3D 大小和方向组成。【。】【在训练开始时，网络通常预测的 3D 属性不如 2D 属性准确，L3D 的值不稳定会干扰匹配过程。只使用 L2D 作为匹配每个查询标签对的匹配成本。】</p><p>————————————————2022-3-23更新——————————————————</p><h2 id="SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation"><a href="#SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation" class="headerlink" title="SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation"></a>SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</h2><p>核心内容：单个关键点估计与回归3 d 变量相结合，预测每个被检测物体的3d包围盒<br>在图像平面上估计投影的三维点。并行地增加一个三维参数回归分支。</p><p>损失函数： 3 d 盒子的8个角点【包含一个分类和回归分支。】分离了每个参数在三维包围盒编码阶段和回归损失函数中的贡献</p><p>当前方法：会较依赖于（rcnn 、rpn）+一个生成伪点云深度<br>缺点是：多阶段具有复杂性、引入噪声<br>改进方向有：几何推理、数据合成、3d-2d后处理</p><p>骨干网：DLA-34| 特点：实现深层聚合<br>采用这个网络的改进： bn替换为gn<br>3D检测网络：<br>关键点分支:物体由关键点表示【投影三维中心，图像中的8个点左乘内参矩阵】<br>回归分支:待回归的参数全部编码成差的方式去学习</p><p>损失函数<br>分类损失cls_loss：采用带惩罚因子的Focal Loss</p><p>———————————————2021-11-14更新————————————————————</p><h2 id="GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving"><a href="#GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving" class="headerlink" title="GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving"></a>GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</h2><p>作者：香港中文大学；商汤科技 王晓刚团队  CVPR2019<br>数据集：kitti<br>解决的问题是：<br>1仅使用二维边界框进行特征提取时，由于信息缺失，会出现表示模糊的问题。<br>【一个二维框有多种三维盒子表示。矛盾在于：相同特征输入又要分类器给出不同置信度；残差损失难估计】2 改进回归残差的loss 残差分类<br>创新点：<br>    在2D检测基础上，高效地获得3D长方体的方法【包含了大小方向的粗信息，该长方体即 “guidance”】<br>    提取3D盒子可见表面的潜在的3D信息，解决仅使用二维盒子的特征时的特征模糊问题。<br>    改进方法：考虑质量感知损失的离散分类方法比直接回归方法具有更好的精度。<br>流程：<br>image→CNN检测器→2D box &amp; orientation【2D框检测和方向预测】→3D guidance→guidance投影【2D框和可见表面提取特征】→3Dsubnet→refined 3D box【细化】<br>2D+O<br>3D box尺寸估计：汽车的尺寸具有低方差和单峰的特性。可以人为设定尺寸，初始化检测出3D box尺寸。<br>物体3D盒的顶部中心在2D平面上有一个稳定的投影，非常接近2D边界盒的顶部中点，而3D底部中心有一个类似的稳定投影，位于2D边界盒的上方和附近。<br>3d box 在相机坐标系下的粗计算：</p><p>高h估算已知，归一化后已知。因此d深度已知。<br>【d相当于一个比例系数现在，可以算出真实的三维空间坐标 3dbox 中心=0.5×（Cb+Ct）】</p><p>【这些以后可以得到一个粗位置。】<br>表面特征提取：<br>对α的角度进行讨论。前后左右会改变。上面必看到。<br>对可见的三个面做仿射变换，在指定特征层提取三个面特征加上2d box提取特征融合，最终特征回归真实的3d box。<br>残差回归改进为分类公式，细化3d box。【将残差值划分区间，计算标准差，以标准差为刻度，作为一个区间划分标志。】区间问题看成多重的二元分类问题。【意思是：如果有一个2Dbox 他不能和gt匹配→证明说他在区间概率为0。置信度低，那就是一个背景】<br>移位特征部分：【每个残差区间用最相关的投影特征？？？】<br>质量感知损失<br>分类预测的置信度能反映对应类的目标框的质量，目标框越准确得分越高。<br> 重合高的得分高</p><p>实验部分：<br>该方法和DEEP3DBOX方法【学习记录V4】相比，后者的AP高。该方法在2D检测上性能没有调到最好，核心工作是在3D部分。和Mono3D【特征很多很复杂的那篇。陈晓智√学习记录V4】、3DOP【双目的】、DeepMANTA比较。<br>分析：<br>该方法不擅长处理图像边界上的对象(通常带有遮挡或截断)。1利用了表面特征中潜在的三维结构信息，消除了仅使用二维边界框所带来的表示模糊。2残差回归问题重新表述为分类。利用质量意识损失增强模型的识别能力。4没有任何额外的数据或标签进行训练。</p><h2 id="Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image"><a href="#Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image" class="headerlink" title="Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image"></a>Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</h2><p>作者：一些外国人 研究方向是目标检测的           CVPR2017<br>创新点：<br>    定位车辆部件，特征点→可以预测隐藏部分的位置，23D之间的匹配，恢复3D车辆信息<br>    由粗到细的多任务卷积神经网络。借鉴RPN 网络产生2Dbox 。output：六个任务共享特征向量：region proposal , detection , 2D box regression , part location , part visibility , 3D template prediction.<br>    对图中的3D框的车辆的其他信息打标签</p><p>3D形状和模板数据集<br>数据集很庞大：103个汽车模型，每一个汽车模型都有36个特征部件的三维坐标，还有一个1*3数组代表长宽高信息。<br>真实的输入图像的每一个车辆建模 【里面有超多信息的】<br>在2D 和3D中的表示【框和部件】。V是可见性【是否遮挡啥的】<br>核心方法：Deep MANTA网络部分</p><p>从粗糙到精细化的前向传播：【均基于第一个卷积层的feature maps输入】使用三个网络【第一个网络就是RPN；第二三个网络？有点不懂】→结果更精确【原因：1克服大的物体大小变化，提供更高的准确度；2保持高分辨率，用于检测难以检测的车辆；】<br>多任务预测：第三个网络：每一个bounding box，同时输出其对应的：S，V，T【怎么能一下子输出这么多东西，奇怪？？？】 NMS去除冗余边框。<br>网络推断部分：利用一个就是前面的Deep MANTA网络的输出+3D部件数据+3D模板数据【方法：从103中遍历一个最相似的+23D匹配】<br>损失部分：<br>LOSS：【需要最小化五个损失函数】三个子网络<br>net1：LRPN<br>net2 | net3：+L检测损失函数 【分类损失（是车还是背景）+边框回归损失】<br>net2 | net3：+L特征部件定位<br>net3：+Lvis【车辆部件可见性】<br>net3：+L 模板相似度<br>【可以看出net3 Loss多：越往后越精细的意思吧】<br>半自动打标签：需要一个3D CAD的车辆模型数据集。作者手动标记了每一个车辆模型相应的N = 36个特征部件点的位置，并连线得到车辆模型的特征轮廓。对于每一个真实车辆的3D边框，通过算法计算出跟车辆模型的3D边框相似性。<br>选中最高的进行映射【该车辆模型及相应的特征部件点和轮廓的位置都可以映射到原图像真实车辆的位置去。】<br>实验部分：<br>数据集：KITTI<br>CNN：GoogLenet和VGG16。【预先用ImageNet数据集训练得到初始参数。VGG16效果较好】<br>NET1：在RPN网络中作者使用了7种长宽比，10种倍数作为参数。【该特征图上每一个点都可以产生70个anchor box。】<br>对比：【指标：AOS和AP】<br>第一个是不使用特征改进，且和RPN一样使用最后一层低分辨率的特征图的模型（类似于原先的RPN）<br>第二个是不使用特征改进，但是使用第一次卷积后的特征图的模型<br>第三个是Deep MANTA<br>3D定位正确性：ALP 通过阈值判定法<br>3D模板预测，特征部件定位，可见性评估：也是设定一些阈值。</p><h2 id="3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020"><a href="#3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020" class="headerlink" title="3DSSD: Point-based 3D Single Stage Object Detector 2020"></a>3DSSD: Point-based 3D Single Stage Object Detector 2020</h2><p>文章主要内容：点云数据进行单阶段3D检测的模型，速度快。<br><a href="https://github.com/dvlab-research/3DSSD" target="_blank" rel="noopener">https://github.com/dvlab-research/3DSSD</a><br>这篇文章的代码写得太好了，很清楚。<br>前人研究：点云数据投影到图像中、体素表达 缺点是会丢失信息。<br>现在研究：直接用点云数据<br>分析了这个PointRCNN的问题【3D Object Proposal Generation and Detection from Point Cloud<br>这篇文章还没看，但很重要的样子。】第一阶段 获取proposal (SA&amp;FP)前景点 第二阶段 refinement。【要解决耗时长的问题，特别是第一阶段，去除 FP层和第二阶段】<br>F-FPS方法和D-FPS方法<br>SA层的下采样步骤中用到了D-FPS方法【这个方法结果基本覆盖了整个场景，缺点就是前景点少了。→F-FPS前景点多+范围大。有利于回归】 SA中会进行特征聚合。【点和点周围的聚合，背景过少，不利于分类】。<br>→两个方法融合</p><p>CG layer【是对SA模块的变形：只用F-FPS中的提取的点作为初始中心点，初始中心点移动到它们对应的实例中→候选点。】预测头之前加入额外层来提取特征。三个步骤：中心点选择、周围点提取和语义特征生成。点集D-FPS 和F-FPS 的点将它们的归一化位置和语义特征group起来作为输入→mlp 层提取特征。<br>Anchor-Free回归：【2d中的centernet 根据关键点】<br>每个候选点到对应实例的距离、实例大小以及角度<br>Anchor-based方法缺点：要生成的anchor太多啦。要设计很多大小和方向的框【2d中的SSD、frcnn都是】<br>centerness对齐策略：1、点是不是在目标里。2、点到六个面距离，一个公式。 →1×2<br>【点云的点都在目标表面，他们的中心标记将非常的小且相似，很难从其他点中得出好的预测】<br>损失函数：分类、回归和shift损失。</p><p>基于模板匹配 ：打分</p><h2 id="3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection"><a href="#3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection" class="headerlink" title="3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection"></a>3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection</h2><p>文章主要内容：输入Stereo图像对作为来估计深度，将图像平面中的像素坐标重新投影回3D空间来计算点云。该方法优势：召回率高，能给出准确率高的对象框。<br>提出一个生成候选的能量最小化函数。能量最小化函数由 对象大小、地面、深度信息特征【点云密度、到地面的距离、有无遮挡、free space可行驶的区域？】<br>3Dbbox中圈出高密度的点云区域。限制：点云不能垂直延伸到bbox外，且这个bbox附近点云高度要矮一些。</p><p>KITTI图像包含许多小对象、严重遮挡、高饱和区域和阴影。不适用之前的目标检测网络。<br>能量函数各个分量含义：<br>pcd：box内体素是不是有点云。【相当于一个比例，这边很奇怪 难道不是box内点云比例越大越好吗？也许可以这么理解，尽可能把他们框起来，如果box全部都有说明box小了】<br>fs：可行驶空间不能有box 如果一个体素是在box内 那么肯定不会在可行驶区域内【最小化盒子内部的可行驶区域】<br>Height Prior：box内点云高度越接近这个对象的平均高度。要这个体素内有点云才参与计算。<br>Height Contrast：表示包围box附近的点云的高度应低于box内点云的高度。【定义附近的概念：+0.6m】<br>根据这个能量最小化生成2000个框 +nms+贪婪算法 得到最终box<br>地面估计：RANSAC【迭代的方式从一组包含离群的被观测数据中估算出数学模型的参数】将平面拟合到估计的地面像素来估计地面。<br>通过SVM来学习权重【这边有点看不明白】，使用IOU作为损失函数。<br>目标检测和方向估计网络<br>基于Fast R-CNN在最后一个卷积层之后添加一个上下文分支和一个方向回归损失来扩展这个基本网络，以共同学习对象的位置和方向。<br>平滑L1损失进行方向回归。</p><h2 id="Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving"><a href="#Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="Stereo R-CNN based 3D Object Detection for Autonomous Driving"></a>Stereo R-CNN based 3D Object Detection for Autonomous Driving</h2><p>文章主要内容：检测关联左右图像中的对象，基于关键点【mask r cnn】和box约束的3Dbox估计+密集3D盒子法(让他更精确)。<br>基于Faster R-CNN的工作。在本文中通过评估多比例特征地图上的锚来修改金字塔特征的原始RPN【Similar with FPN 意思是这边采用多个feature map】。<br>问题：RPN和FPN对比。<br>之前知道Faster R-CNN 是由Fast R-CNN+RPN构成的。一张图通过RPN获得一堆proposal，【proposal都是在原图上画个框，映射到一个feature map，池化变成统一尺度，之后再做分类和回归。】<br>在FPN中，对多个feature map分别做分类和回归，获得到一堆proposal【feature map这次有多张图】 </p><p>双目的RPN过程<br>左右特征图concat起来。目标是左右的并集，和anchor IOU大于0.7 IOU小于0.3定义正负标签。<br>FPN在Faster R-CNN中FPN产生的每一个尺度的feature map都要送进RPN做一次proposal的提取。<br>六个【我觉得论文这边写错了 这样好奇怪哩 应该是两个uv坐标offset + wh回看之前的是一个点坐标+wh 一共是四个 】<br>左右候选框都是由同一个 anchor 生成，共享类别置信度得分，它们就可以一一对应起来。我们在左右 RoI上分别使用NMS，选取最高的2000个候选框用于训练。测试时，选取300个候选框。</p><p>在RPN之后→RoiAlign的操作→ 获取FPN的左和右featuremap →concat相应的特征→fc层得到对象类别、立体边界框、维度和角度【使用θ表示车辆相对于摄像机框架的方向，使用β表示物体相对于摄像机中心的方位 sin/cos值】<br>keypoint的检测。Mask R-CNN的结构进行关键点的预测。4个3D keypoint，即车辆底部的3D corner point，同时将这4个点投影到图像，得到4个keypoint 起到一定约束作用。<br>3D框恢复。是从2D的框和关键点来恢复的。7个点 left左上右下 right左右 +keypoint：推导<br>Dense 3D Box Alignment：最小化左右视图refine？【这个地方有点不理解，先跳过】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【3D】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%903D%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>range-view</title>
    <link href="https://betterwyl.github.io/2022/07/29/range%20view/"/>
    <id>https://betterwyl.github.io/2022/07/29/range%20view/</id>
    <published>2022-07-29T10:17:47.000Z</published>
    <updated>2022-11-20T14:59:43.036Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>源于看了一篇基于rangeview的mot论文 觉得range的方法可操作性强且耕耘不多。</p><p>————————————————2022-10-2更新——————————————————</p><h2 id="Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation"><a href="#Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation" class="headerlink" title="Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation"></a>Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation</h2><p>创新点：<br>RV-PV-BEV；<br>dilated convolution 2d【residual block】；<br>two-stage RCNN【解决bev视图没有高度的问题】<br>Input：range image 【编码距离、坐标、强度→输入距离图像5 × h × w】<br>处理：Kitti3D相机前视图中的目标【90°场景 5 × 48 × 512】；waymo：64×2650<br>扩张残余块（DRB）<br>问题产生：尺度变化感知不清【使用range image送入到2Dcnn】。<br>将扩张卷积插入正常残差块：【代码中有三个不同的扩张率。最后用1×1融合】</p><p>RV-PV-BEV<br>问题是：Range image会重叠，因此特征提取要在BEV中进行。适应不同尺度。<br>流程是：Range image 上记录特征点对应投影到BEV平面，相同的点使用平均池化。<br>【如果在开始时投影，BEV将作为主要特征提取器。】</p><p>3D RoI Pooling<br>问题是：range or bev都无法明确地学习3D边界框高度方向上的特征。<br>解决方案是：使用相对位置进行高度上的编码。<br>【固定数量、不同的栅格包含目标的不同部分。由栅格的空间关系，信息可通过相对位置编码。】此处有个将三维全部展平为一维然后连接的处理。<br>损失函数：</p><p>Kitti数据集处理</p><p>Waymo：</p><p>固定栅格：12×12×12  128 proposals with a 1:1 ratio<br>优势： [30，50]m和[50，75]m  运行速度为22 fps<br>我的想法：range view的方法照道理快一些【pvrcnn 12fps】，猜测是没有分割出前景点会不会在BEV部分计算量大【关注：前景点分割方法→影响速度】。相对于rangedet其结构还是比较复杂 。</p><h2 id="RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection"><a href="#RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection" class="headerlink" title="RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection"></a>RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection</h2><p>ICCV2021 from 中科院自动化所&amp;图森<br>代码：<a href="https://github.com/TuSimple/RangeDet" target="_blank" rel="noopener">https://github.com/TuSimple/RangeDet</a><br>和BEV视图的区别，信息稠密的更有效利用【体现在NMS的改进】，近大远小会有尺寸变化【体现在MKC和改进型FPN】<br>创新点：<br>1Meta-Kernel Convolution<br>2 Range Conditioned Pyramid Assignment<br>3 Weighted NMS<br>Meta-Kernel Convolution【解决使用卷积会把密集信息忽略的问题，比如在一个rangeimage中，两个点靠近，而他们实际的距离可能很远，那就丢掉了这部分信息】</p><p>Range Conditioned Pyramid Assignment<br>【使用ResNet中的BasicBlock将其卷积核替换了一下此处有点不理解，结合blog】Meta-Kernel Convolution：将卷积核的权重变得可调整。使用中心点与邻域点的差值，放大了检测了属于点和点之间的特征差异。】<br>依据：距离范围的远近：近距离label 局部特征图；远距离的label分配全局的特征图。【思想FPN金字塔】<br>Weighted NMS<br>每个pixel都会预测一个box，一个truth可以被很多pixel预测。那不能全部删掉，而是采用较高score的框进行加权。<br>作者使<br>我的思考：该论文具体问题具体分析提供改进的方法。挺巧妙的。【3D→2D的这样逆过程可以使用一些2D的方法，从而来实现一些2D中已经实现任务，在分割、跟踪上？】</p><h2 id="【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021"><a href="#【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021" class="headerlink" title="【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021"></a>【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021</h2><p>代码：<a href="https://github.com/PRBonn/LiDAR-MOS" target="_blank" rel="noopener">https://github.com/PRBonn/LiDAR-MOS</a><br>数据集：SemanticKITTI<br>创新点：将rangeview用于mos任务；使用了时域上的信息，即残差；<br>Input：<br>3D LiDAR 扫描生成的range image+残差图像【当前帧和先前帧之间的距离的残差→d 是关于r的】<br>r将第k帧的点云旋转至当前帧l第i个像素上的距离值<br>d 归一化表示</p><p>Output：当前帧中的一个标签范围 【红的表示移动物体】<br>流程：3D点云序列的投影图+残差图像<br>1投影公式<br>2使用到SLAM中获得过去时间序列的雷达信息→残差计算【T代表着每个序列的相对变换 传感器得到？有点不理解来源 数据集里的吗】</p><p>结合SLAM读数【其实这边有点不懂，SLAM知识缺失】和残差图像→现有的分割网络通过利用残差图像中的时间信息来区分运动物体和背景上的像素。二进制表示<br>如何将上述两步信息融合 【就是整套需要变换和重新投影流程】<br>1之前的扫描序列转化为当前的2重新投影到当前范围视图3计算距离</p><p>CNN结构【现成的】<br>使用三个网络比对RangeNet++ MINet SalsaNext<br>指标IOU：移动物体<br>实验部分：<br>在slam上添加噪声测试其稳定性<br>【自己生产自己比较，当前较少该方向的成果，作者结合了之前方法重新做实验比较】<br>semantic segmentation  SalsaNext【这个不太理解，加了一堆东西】<br>scene flow 对平移向量设置阈值判断是否移动<br>我的思考：该论文引入与之前帧的残差作为一个特征，若物体移动较慢很可能识别为静物。能不能设置区间或者其他方法放大时间上的信息，【改进点：针对速度小的物体分割效果提升】。一个问题：相对本车静止，他实际上也是运动的。【回答：在实际场景 趋势显露】<br>LiDAR 的 MOS 的实现并不多 ？实验比较的依据可靠性</p><h2 id="Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet"><a href="#Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet" class="headerlink" title="Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet"></a>Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet</h2><p>代码：<a href="https://github.com/haomo-ai/motionseg3d" target="_blank" rel="noopener">https://github.com/haomo-ai/motionseg3d</a><br>数据集： KITTI+自己标注<br>创新点：<br>1双分支结构，【两部分变成并联结构，使用SalsaNext，在此基础上add】。<br>2解决range-view信息没有有效利用问题：加了Meta-Kernel Module<br>通过中心点的相对坐标计算 3×3 邻域的权重，然后使用 1×1Conv 聚合邻域特征来更新中心特征。<br>这么做的目的是：细化结果，并减少对象边界周围出现的伪影。<br>其他tricks：加注意力机制；减少resblock<br>由一个用于编码外观特征的距离图像分支和一个用于编码时间运动的残差图像分支<br>网路LOSS：每个类别频率交叉熵和 【The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-<br>union measure in neural networks,” in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2018】pixel和point都可以用的<br>我的思考：顾名思义，双分支。这篇文章对LMNet的改进可以理解为串联改并联了。同时，他用了一个没见过的损失函数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【3D】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%903D%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>YOLO系列</title>
    <link href="https://betterwyl.github.io/2021/12/25/yolo%E7%B3%BB%E5%88%97/"/>
    <id>https://betterwyl.github.io/2021/12/25/yolo%E7%B3%BB%E5%88%97/</id>
    <published>2021-12-25T14:56:18.000Z</published>
    <updated>2022-11-20T14:54:33.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>———————————————————2021-12-25更新————————————————</p><h2 id="YOLOv3-An-Incremental-Improvement-技术报告"><a href="#YOLOv3-An-Incremental-Improvement-技术报告" class="headerlink" title="YOLOv3: An Incremental Improvement 技术报告"></a>YOLOv3: An Incremental Improvement 技术报告</h2><p>    backbone Darknet-19→Darknet-53 精度速度权衡<br>【该全卷积网络是没有全连接层的 也是其可以兼容任意尺度图像的原因→FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数】<br>兼容任意尺度图像→输入要是32倍数<br>怎么变成416？强行缩放、加灰边</p><p>小目标性能提高<br>多尺度融合 ：不同尺度anchor 每个尺度3个anchor<br>255是85（coco80类+xywhc+边框置信度）×3（anchor）<br>【下采样32倍：13×13×255每一个代表原图32。】<br>【FPN特征金字塔】<br>concat：拼接 【上采样2倍完26×26+原先下采样26×26的拼接。厚度是不一样的。】<br>损失函数<br>正样本：与gt 的IOU最大的那个anchor 所在尺度的grid cell去负责【与之前区别 不看中心点落在哪个gridcell里面】<br>注意的是：如果高于某个阈值的话那就不是正样本而是不参与，小于某个阈值的话那就是负样本。<br>正样本的坐标【遍历所有cell 还有anchor】<br>+正样本的置信度和类别（标签为1）使用二元交叉熵损失函数BCE<br>+负样本置信度（标签为0）<br>测试：1、计算预测框信息2、设置阈值，去掉得分低的3、多分类nms重叠大的消除</p><h2 id="YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><p>网络结构：<br>backbone Darknet-53→CSP Darknet-53<br>生成特征图后再合在一起作为下一个网络输入【厚度越来越大了】<br>特征提取网络改进 SPP和PANet<br>Bag of freebies<br>数据增强：1像素级别：处理光度失真【调整图像的亮度、对比度、色调、饱和度和噪声】几何失真【添加了随机缩放、裁剪、翻转和旋转】2假设遮挡：随机融合或者使用零像素值3style transfer GAN 风格迁移<br>不同类别之间存在数据不平衡：单阶段Focal loss双阶段难例挖掘<br>本论文使用：<br>Mosaic混合了四张训练图片<br>SAT：【不太理解。？网上：它在前后两阶段上进行操作。在第一阶段，神经网络代替原始的图片而非网络的权重。用这种方式，神经网络自己进行对抗训练，代替原始的图片去创建图片中此处没有期望物体的描述。在第二阶段，神经网络使用常规的方法进行训练，在修改之后的图片上进检测物体。】<br>网络正则化的方法：Dropout、Dropblock<br>CIOU：将覆盖区域，中心点距离和纵横比考虑在内。<br>Bag of specials 需要增加推理的过程<br>增大感受野 SPP【本文使用】 ASPP RFB<br>注意力机制 SE SAM【将空间注意改成点注意】<br>特征集成方法ASFF BiFPN PANET【本文使用 是一种拼接 不是相加!!】<br>激活函数【代码可以直接调用库】MISH ReLU leaky-ReLU parameter-ReLU ReLU6 SELU<br>后处理方法：soft NMS【物体的遮挡】、DIoU NMS【添加中心距离的信息】<br>通过均值和方差对网络激活进行归一化【本论文使用CmBN】<br>理解：这篇文章讨论和验证了目标检测中每个部分的多种策略，对这些策略进行选择和部分改进，然后组合在一起，基于yolov3去改动，比较各模块选择什么方法能让目标检测性能最优化。有注重对各模块在单GPU上的性能比较。</p><h2 id="Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）"><a href="#Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）" class="headerlink" title="Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）"></a>Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）</h2><p>abstract关键词：类别不平衡<br>自动选择这些困难的样例可以使训练更加有效和高效<br>【意思是之前的region proposals生成正负比例不平衡。假如说10个里面9个负样本，那直接判别负样本正确率非常高。】<br>VOC2007和2012中的mAP分别为：78.9%，76.3%<br>一个概念：启发式超参数搜索 【使用循环神经网络来生成参数→在训练过程中不变的数 当然会增加时间复杂度】<br>原因：<br>Fast R-CNN允许更新整个卷积网络 SPP net、MR-CNN固定住卷积网络、也没使用SVM。<br>train部分：<br>在Fast R-CNN SPP net  MR-CNN把RoI与真实框的交叉比(IOU)大于等于0.5即判定为目标RoI。本文去掉小于等于的设置背景的。<br>Fast R-CNN在一个 mini-batch中，它们之间的比例是1：3。本文中是去掉了这个比例。<br>没有采用设定背景和目标样本数的比例方式处理数据的类别不平衡【如果哪个类别不平衡，那么这个类别的损失就会比较大，这样被采样的可能性也比较大】<br>方法：<br>    之前使用的方法：<br>只选出那些 hard negative 的样本进行训练→Hard Negative Mining Method<br>SVM + Hard Negative Mining Method<br>基于SVM及检测器训练时需要分类器对样本进行分类，把其中错误分类的样本放入负样本集合再继续训练分类器直到模型收敛。<br>困难样本挖掘。【会比都是简单的效果好。】<br>缺陷：端到端难以操作；需要迭代训练的时候又另外腾出时间来生成这种hard negative，每迭代几次就固定模型一次，速度慢。<br>一般用svm才会用这个方法。但是fastrcnn和fasterrcnn都没使用。→想另外的方法<br>    本文：<br>SGD回传 对样本进行一个重新选择【选择困难样本或者对困难样本赋予更高的权重】<br>SGD是以mini-batch为单位来更新模型的：对于每个mini-batch，先从数据集中取N张，然后每张图片采样B/N个RoIs<br>重合率比较大的ROI之间的损失也比较相似→解决办法：使用了 NMS(非最大值抑制) 算法：把损失按高到低排序→选择最高的损失→计算其他 ROI与这个 ROI的 IoU→移除 IoU 大于一定阈值的 ROI，然后反复上述流程直到选择了 B/N 个 ROIs。<br>    提出一个是修改loss层：【缺点：将没选择的ROI的loss设置为0。但是这种做法并不高效，因为即便很多ROI的loss都是0，也就是不需要更新梯度，但是这样仍需要给每个ROI都分配存储空间，并且每个ROI都需要后向传播】</p><p>    两个相同的 ROI网络，一个只可读【前向传递的时候分配空间】，另一个可读可修改【同时为前向和后向分配空间】。<br>经过ROI plooling层生成feature map，然后进入只读的ROI network得到所有ROI的loss；然后是hard ROI sampler结构根据损失排序选出hard example，并把这些hard example作为第二个ROI network的输入。</p><h2 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）"></a>You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）</h2><p>abstract关键词：<br>视作回归问题。<br>一个单一的神经网络预测bbox和类概率。<br>由于整个检测pipeline是一个单一的网络，可以直接对检测性能进行端到端的优化。<br>快！每秒45帧。实时 是之前其他实时物体检测系统mAP的两倍以上<br>避免背景错误，产生false positives。<br>【对比之前的：是通过region proposal产生大量的可能包含待检测物体的bounding box，再用分类器去判断每个 bounding box里是否包含有物体，以及物体所属类别的概率。分开处理较难优化】<br>核心思想：<br>视为回归问题。利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别<br>流程：<br>input：resize图像到448 * 448 →网络→NMS<br>Unified Detection<br>栅格 各管各的<br>image→S*S的栅格 每个栅格负责检测中心落在该栅格中的物体<br>每一个栅格预测B个bounding boxes&amp;置信度得分<br>x y w h IOU+C【conditional class probability在一个栅格包含一个Object的前提下，它属于某个类的概率】<br>conditional class probability信息是针对每个栅格的。<br>confidence信息是针对每个bbox的。<br>【上述两者相乘：包含bounding box中预测的class的 probability信息，也反映了bounding box是否含有Object和bounding box坐标的准确度】<br>一个图 ：S×S×(B×5+C)<br>网络设计：<br>24个卷积层和2个全连接层<br>【卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。】</p><p>YOLO借鉴GoogLeNet分类网络结构。没使用inception module。使用 1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>训练：<br>Pretrain网络：上述网络中的前20 个卷积层+average-pooling layer+全连接层【ImageNet 1000-class的分类任务数据集】<br>Pretrain的结果的前20层卷积层应用到检测中，+剩下的4个卷积层及2个全连接。<br>损失函数：<br>如果一些栅格中没有物体，那么就会将这些栅格中的bounding box的confidence 置为0，相比于较少的有物体的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。</p><p>坐标预测：xywh<br>含有物体的bbox IOU预测 confidence<br>不含有物体的bbox IOU预测 confidence<br>类别预测：Class 有没有中心落在网格中 有就预测概率的意思。</p><p>看起来可以直接用7×7×30理解。30前20个代表的是预测的种类，后10代表两个预测框及其置信度(5x2) 7*7<br>每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box predictor来负责预测【当前哪一个predictor预测的bounding box与ground truth box的IOU最大，这个 predictor就负责物体检测→每个predictor可以专门的负责特定的物体检测→训练后预测效果更好】</p><p>局限性：<br>YOLO对小物体的检测效果不好【小物体，因为一个栅格只能预测2个物体。而且小物体IOU影响较大】<br>YOLO容易产生物体的定位误差</p><h2 id="YOLO9000-Better-Faster-Stronger-CVPR-2017"><a href="#YOLO9000-Better-Faster-Stronger-CVPR-2017" class="headerlink" title="YOLO9000: Better, Faster, Stronger (CVPR 2017)"></a>YOLO9000: Better, Faster, Stronger (CVPR 2017)</h2><p>abstract关键词：<br>保持原有速度，精度上提升<br>一种目标分类与检测的联合训练方法：YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测<br>改进措施：<br>    Batch Normalization：批处理规范化<br>每一个卷积层后添加batch normalization，通过这一方法，mAP获得了2%的提升。batch normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。<br>    High Resolution Classifier：<br>在ImageNet上对448×448上的分类网络进行了10次微调。我们在检测时对产生的网络进行微调。模型在检测数据集上finetune之前已经适用高分辨率输入。这种高分辨率的分类网络mAP增加了近4%。<br>    Convolutional With Anchor Boxes：<br>YOLOv1采用的是：全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体。【YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差】<br>YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。【RPN预测的是边界框相对于先验框的偏移】YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。每个位置的各个anchor box【注重形状】都单独预测一套分类概率值。【影响：mAP有稍微下降】</p><p>input： 416×416<br>特征图大小：13×13【维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。】<br>最初的YOLO输入尺寸为448×448，加入anchor boxes后，输入尺寸为416×416。模型只包含卷积层和pooling 层，因此可以随时改变输入尺寸。<br>作者在训练时，每隔几轮便改变模型输入尺寸，以使模型对不同尺寸图像具有鲁棒性。每个10batches，模型随机选择一种新的输入图像尺寸（320,352,…608，32的倍数，因为模型下采样因子为32），改变模型输入尺寸，继续训练。<br>Dimension Clusters<br>维度聚类【k-means聚类方法】。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标。5种大小的box维度来进行定位预测，这与手动精选的box维度不同。结果中扁长的框较少，而瘦高的框更多【就和尺度没关系了】125=5×25<br>Direct location prediction<br>直接目标框预测【不是预测偏移了 每个anchor 检测周围】</p><p>细粒度特征拥有较细粒度特征的层变形【？】<br>多尺度训练：训练Yolo v2时不固定image size，而是每训练10个epochs随机选取【32倍数】<br>网络：New Network: Darknet-19（特征提取器）<br>19个卷积层和5个maxpooling层 3×3 卷积层 【其中使用1×1卷积层】 2×2maxpooling<br>改进：方法wordtree</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN系列 （实习摸鱼中）</title>
    <link href="https://betterwyl.github.io/2021/11/17/R-CNN%E7%B3%BB%E5%88%97/"/>
    <id>https://betterwyl.github.io/2021/11/17/R-CNN%E7%B3%BB%E5%88%97/</id>
    <published>2021-11-17T03:56:22.000Z</published>
    <updated>2022-11-27T04:05:41.776Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来锐捷实习了，我每天的任务是写两个脚本，但奈何我。。。<br>又不想虚度光阴，认真看看好了。<br>这里伙食确实不错。</p><h1 id="R-CNN-系列"><a href="#R-CNN-系列" class="headerlink" title="R-CNN 系列"></a>R-CNN 系列</h1><h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014" class="headerlink" title="R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)"></a>R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)</h2><p><strong>背景</strong>：首次用深度学习CNN的方式进行目标检测的尝试。之前都是使用传统提取特征，比较好的是SIFT和HOG。【看代码复习】<br><strong>创新点</strong>：<br>使用候选区域，与滑动窗口方法相比，CNN处理的图像窗口减少了两个数量级。<br>CNN提取特征：RCNN使用的CNN网络是AlexNet。<br>有监督pre-training+domain-specific finetuning处理label少的情况<br>使用bounding box regression进行修正【？具体】<br>目标检测：<br><strong>生成候选区域</strong>：<br>选择性搜索【一张图像生成约2K个候选区域 （Selective Search）】目的是为了改善传统提取特征方法中机从左到右、从上到下枚举式的低效。<br>Selective Search：过分割，将图像分成小区域；合并可能性最高的相邻两个区域<br><a href="http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a><br>【相关论文 颜色、纹理相近；尺度要均匀：不能大鱼吃小鱼；形状】；输出所有候选区域。</p><p>基于上述方法搜出的候选框是矩形的，而且是大小各不相同。为了要得到固定尺寸的图片输入到CNN中，进行缩放。作者比较了两种方法：1 各向异性缩放：不管比例 直接缩到227*227 ；2 各项同性缩放：将边界拓展成正方形 然后不在框里面的直接用框外的颜色均值代替填充；用固定背景颜色填充<br>本文：采用各向异性缩放、padding=16的精度最高</p><p>如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5就把这个候选框标注成物体类别，否则就是背景。<br><strong>特征提取</strong>：<br>对每个候选区域，使用深度卷积网络提取特征 （CNN）<br>比较：VGG和AlexNet。前者精度高但是计算量是后者的7倍。<br>物体检测的一个难点：物体标签训练数据少，若直接采用随机初始化CNN参数的方法，训练数据量是不够——采用的是有监督的预训练。<br>AlexNet原本是做图像分类任务，为做目标检测任务，替换掉AlexNet的最后一层的全连接层（4096 * 1000）。<br>1pre-train：采用了迁移学习的思想： ImageNet训练的CNN，先进行网络图片分类训练。<em>该数据库有大量的标注数据，共包含了1000种类别物体，预训练阶段CNN模型的output 1000个神经元。<br>网络优化：采用随机梯度下降法，学习速率大小为0.001<br>2fine-tune：在小型目标数据集（PASAC VOC）对上面得到的model进行改动。将模型的最后一层修改类别数。（20+1background）<br>RCNN的结构实际是5个卷积层、2个全连接层。<br>input: 2000</em> 227 * 227 * 3<br>output:  2000 * 4096 * 1<br>从每个候选区域中提取4096维特征向量。特征是通过前向传播通过五个卷积层和两个全连接层减去平均的224X224 RGB图像来计算的<br><strong>线性SVM分类器</strong>：<br>训练过的对应类别的SVM给特征向量中的每个类进行打分，每个类别对应一个二分类SVM。output: 2000*N（N目标的类别）作者测试了IOU阈值各种方案数值，通过训练发现，IOU阈值为0.3效果最好。IoU&gt;0.3的region proposal的特征向量作为正例，其余作为负例。<br>减少bbox：非极大值抑制法<br>测试时：2000×4096维特征与N个SVM组成的权值矩阵4096×N相乘，每一列即每一类进行非极大值抑制剔除重叠建议框</p><p>【CNN做特征提取（提取fc7层数据），再把提取的特征用于训练svm分类器原因：CNN容易过拟合，需要大量训练数据，因此CNN训练数据做了比较宽松的标注，一个bounding box正样本可能只包含物体的一部分，用于训练CNN。SVM适用于少样本训练，对于训练样本数据的IOU要求比较严格，只有当bounding box把整个物体都包含进去了，才把它标注为物体类别】<br><strong>Bounding Box 回归</strong>：<br>就是得到的候选框可能与ground truth相差比较大。解决：利用回归的方法重新预测了一个新的矩形框，借此来进一步修正bounding box的大小和位置<br>边界框回归是利用平移变换和尺度变换来实现映射。<br>使用相对坐标差：【比例值是恒定不变的；对坐标偏移量除以宽高即做尺度归一化：尺寸较大的目标框的坐标偏移量较大，尺寸较小的目标框的坐标偏移量较小】<br>IoU大于0.6时，边界框回归可视为线性变换：【log(1+x)/x  x趋近0 整个趋近于1】<br>AlexNet第5个池化层得到的特征即将送入全连接层的输入特征的线型函数。<br><strong>存在问题</strong>：R-CNN需要两次进行跑CNN model，第一次得到classification的结果，第二次才能得到(nms+b-box regression)bounding-box<br>三个模块（CNN特征提取、SVM分类和边框修正）是分别训练的，并且在训练的时候，对于存储空间的消耗大。检测速度慢，47s/per image。</p><h2 id="Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明"><a href="#Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明" class="headerlink" title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明"></a>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明</h2><p><strong>核心贡献</strong>：在R-CNN的基础上提出了空间金字塔变换层，速度、精度提升。<br>【CNN网络后面接的FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数，故需要固定长度的输入限制网络的输入大小<br>CNN网络会有大量的重复计算，造成的计算冗余】<br>R-CNN：输入需要对候选区域做填充到固定大小【对候选区域做填充缩放操作，可能会让几何失真、有冗余信息，这都会造成识别精度损失】;每个候选区域都要塞到CNN内提取特征向量【一张图片有2000个候选区域，也就是一张图片需要经过2000次CNN的前向传播，这2000重复计算过程会有大量的计算冗余，耗费大量的时间。】<br><img src="https://upload-images.jianshu.io/upload_images/3940902-0db3d0e4fa819bf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/444/format/webp" alt="比较"><br>SPP-net：针对候选框的重复计算部分，候选区域到全图的特征映射之间的对应关系。【直接获取到候选区域的特征向量，不需要重复使用CNN提取特征】<br>使用空间金字塔变换层将接收任意大小的图像输入，输出固定长度的输出向量。<br>【相当于从中间截断了，那前面的CNN可以接受不同尺寸图像了。都是为了保证全连接层的一个固定】<br>空间金字塔变换层<br>以不同的大小的bin块来提取特征的过程。【成列向量与下一层全链接层相连。这样就消除了输入尺度不一致的影响。】<br>不同大小侯选区域在feature map上的映射塞给SPP层<br>SPP layer分成1x1(塔底)，2x2(塔中)，4x4（塔顶）三张子图，对每个子图的每个区域作max pooling。输出都是(16+4+1) 每个块提取出一个特征21维特征向量。然后×256。【其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出神经元】</p><h2 id="Fast-R-CNN-CVPR-2015"><a href="#Fast-R-CNN-CVPR-2015" class="headerlink" title="Fast R-CNN(CVPR 2015)"></a>Fast R-CNN(CVPR 2015)</h2><p><strong>创新点</strong>：<br>SSP→RoI池化层：避免对每个候选区域提取特征，避免大量重复计算。<br>将分类与定位两大任务融入一个网络中来，获得了比R-CNN快的训练测试速度。边框回归直接加入到CNN网络中训练，损失部分采用多任务损失：<br><strong>方法</strong>：<br>利用选择性搜索获取图像中的推荐区域→将原始图片利用VGG16网络进行提取特征，之后把图像尺寸、推荐区域位置信息和特区得到的特征图送入RoI池化层，进而获取每个推荐区域对应的特征图。<br>input：待处理的整张图像；候选区域<br>网络分成两个并行分支，一个对推荐区域进行分类，一个对推荐区域的位置信息做预测。<br><img src="https://img-blog.csdn.net/20180527160553808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dlbnRlbHlhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="FastRCNN"><br><strong>RoI池化层</strong>:<br>【对SSP改进部分，SSP有不同尺度的特征图。这边简化了一下下采样到统一尺度】RoI只采用单一尺度进行池化→VGG16 后产生一个7×7×512维度的特征向量作为全连接层的输入【每个RoI区域的卷积特征分成4×4个bin，然后对每个bin内采用max pooling，这样就得到一共16维的特征向量。】<br>RoI pooling解决了SPP无法进行权值更新的问题。【？解答：SPP是将所有的特征图上的RoI保存下来，然后选择进行的网络微调，不会更新SSP layer之前的层，就相当于和前面的断开了，前面的特征图都不共享。Fast就是从输入到选择RoI都是同一批图，这样就能效率高的反向传播了。】<br>两个好处：将图像中的RoI区域定位到卷积特征中的对应位置；将这个对应后的卷积特征区域通过池化操作固定到特定长度的特征，然后将该特征送入全连接层<br>采用SVD对全连接层分解:<br>一张图像约产生2000个RoI，近一半的前向传递时间都花在计算全连接层上。SVD对全连接层进行变换来提高运算速度。截断SVD可以减少30%以上的检测时间，mAP只下降很小(0.3个百分点)。<br>一个大的矩阵可以近似分解为三个小矩阵的乘积，分解后的矩阵的元素数目远小于原始矩阵的元素数目，从而达到减少计算量的目的。→对全连接层的权值矩阵进行SVD分解。<br><strong>多任务的损失</strong><br>Fast R-CNN直接使用Softmax替代SVM分类，利用Softmax Loss 和Smooth L1 Loss对分类概率和边框回归联合训练<br>【Fast R-CNN网络主要有2个网络分支，一个网络分支负责输出推荐区域的分类概率，另一个网络分支负责输出每个推荐区域位置信息偏移量。】</p><h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><p><strong>RCNN FastRCNN总结对比</strong><br>二者的ROI区域生成：单独模块 选择性搜索<br>RCNN把分类和坐标分成两个网络，fastRCNN加上特征提取融合在一起了（Deepnet）。<br><strong>创新点</strong>：<br>使用RPN生成检测框，提高检测框生成速度。区域生成网络(RPN)+Fast RCNN<br>产生建议窗口的CNN和目标检测的CNN共享<br>核心方法：<br>使用卷积层提取特征图：<br>conv x13 relux13 poolingx4<br>conv层：kernel_size=3，pad=1，stride=1<br>对卷积进行填充：【相当于保持尺度不变 假设输入MxN 进来后(M+2)x(N+2) 那么3x3输出后还是MxN】<br>pooling层：kernel_size=2，pad=0，stride=2<br>【假设输入MxN 进来后(M+2)x(N+2) 那么2x2步长2输出后(M/2)x(N/2)→四次pool就变成了1/16 特征图上面密集的点对应到原始图像上面有16个像素的间隔】 最后conv5输出通道数有256（针对ZF ：VGG16是512-d,ZF是256-d）<br>【800/16 x 600/16=50 x 38 特征一共是50 x 38 x 256】</p><p><strong>RPN网络生成检测框</strong>：【相当于目标定位，二分类】<br>input: (M/16)x(N/16) 先经过一次3x3卷积 output: 50 x 38 x 256<br>分成两条线：<br>softmax判断正负样例<br>bb回归修正→proposals精准化<br>每一个点都负责原图中对应位置的9种尺寸框的检测→50 x 38 x 9 个anchor<br>【anchors多尺度方法：9个尺度，三种形状→长宽比1:1 1:2 2:1】<br>【可理解为在原图尺度上，设置了许多候选Anchor。通过CNN判断标记有目标的positive anchor和没目标的negative anchor】<br><strong>正负样本怎么划分</strong>：<br>1    对每个标定的ground truth区域，与其重叠比例最大的anchor记为正样本。【一个gt对应一个正样本】<br>2    剩余的anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本【每个ground truth可能会对应多个正样本anchor。但每个正样本anchor 只可能对应一个grand truth 一对多关系】。<br>如果其与任意一个标定的重叠比例都小于0.3，记为负样本。<br>3    剩余的anchor、跨越图像边界的anchor丢弃<br>计算anchor box与ground truth之间的偏移量：ground truth box与预测的anchor box之间的差异<br><strong>损失</strong>：rpn_loss_cls【softmax】、rpn_loss_bbox【smooth L1】、rpn_cls_prob【用于下一层的nms非最大值抑制操作】</p><p>p表示anchor i预测为物体的概率<br>p×正样本=1负样本=0 【回归只有在正样本时候才会被使用！】<br>t表示正样本anchor到预测区域的4个平移缩放参数<br>t×正样本anchor到Ground Truth的4个平移缩放参数</p><p>生成anchors →softmax分类器提取positvie anchors →bbox回归positive anchors →Proposal Layer生成proposals</p><p><strong>Roi Pooling</strong>：【共享信息】<br>input：<br>特征图和proposals 提取proposal feature maps→后续全连接层判定目标类别<br>候选框的特征图水平和垂直分为7份，对每一份进行最大池化处理。49维送入全连接层。【即使大小不一样的候选区，输出大小都一样，实现了固定长度的输出】<br>分类：<br>利用已经获得的proposal feature maps，通过全连接层与softmax计算每个proposal具体属于的类别，输出cls_prob概率向量；再次利用bounding box regression获得每个proposal的位置偏移量bbox_预测，用于回归更加精确的目标检测框。</p><p><strong>训练部分</strong>：<br>1    使用ImageNet模型初始化，独立训练一个RPN网络。<br>2    使用上面RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络。【两个网络每一层的参数完全不共享】<br>3    使用上面的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的学习率设置为0，仅仅更新RPN特有的那些网络层，重新训练。两个网络已经共享了所有公共的卷积层<br>4    仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个network，继续训练，微调 Fast-RCNN特有的网络层实现网络内部预测proposal并实现检测的功能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>Anchor-free系列</title>
    <link href="https://betterwyl.github.io/2021/10/26/anchorfree/"/>
    <id>https://betterwyl.github.io/2021/10/26/anchorfree/</id>
    <published>2021-10-26T01:35:22.000Z</published>
    <updated>2022-11-20T14:58:55.156Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="CornerNet-2个角点：左上角和右下角角点"><a href="#CornerNet-2个角点：左上角和右下角角点" class="headerlink" title="CornerNet 2个角点：左上角和右下角角点"></a>CornerNet 2个角点：左上角和右下角角点</h2><p>input：image经过一个ConvNet→生成feature map<br>hourglass network: 捕获图片在多个尺度下的特征。1、降采样操作缩小输入的大小；2、上采样恢复到输入图像大小<br>常用：使用多个pipeline分别单独处理不同尺度下的信息，网络的后面部分再组合这些特征。<br>上面分支module负责预测左上角corner，下面分支module负责预测右下角corner<br>需要group操作。<br>heatmaps预测：Corners点概率；<br>中心点不一定局限在某一个位置上 而是一个区域→中心点落在半径r范围内设置一个iou→圆圈内的点的数值是以圆心往外呈二维的高斯分布；【对不同负样本点的损失函数采取不同权重值的原因：白色虚线是一个预测框，预测框的两个角点和ground truth并不重合，但是该预测框大概框住了目标，因此是有用的预测框，所以要有一定权重的损失返回】<br>loss：带有惩罚因子的Focal loss 降低接近真值损失 进一步解决样本不均衡<br>embeddings预测：解决的是配对问题：一个目标的两个角点，二者的embedding vector之间的距离应该很小。每个点所属的目标中心点；采用L1范数，距离大于0.5或者两个点来自不同类别的目标的都不能构成一对。<br>offsets：表示在取整计算时丢失的精度信息，输入图像到特征图之间会有尺寸缩小。anchor-based方法是算和anchor偏移。针对小尺寸目标的回归。这是针对角点的。</p><p>corner pooling：怎么知道这个点就是角点？特征点肯定是最大的。红色的部分。找到值最大的网格然后确保该网格左边的网格全部都能变成最大值，也就是把水平方向最明显的特征向左延续；竖直方向最明显的特征向上延续；这样当两幅heatmap相加时→两者最明显特征的路线相重叠，这样加出来的值肯定也是最大的，因此就能推测出左上角关键点的位置。</p><p>测试：<br>在得到预测角点后，会对这些角点做NMS操作，选择前100个左上角角点和100个右下角角点。测试图像采用0值填充方式得到指定大小作为网络的输入，而不是采用resize，另外同时测试图像的水平翻转图并融合二者的结果。最后通过soft-nms操作去除冗余框，只保留前100个预测框</p><h2 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a>ExtremeNet</h2><p>检测目标的4个极值点（即最上点、最下点、最左点、最右点）和一个中心点<br>需要group操作。针对CornerNet预测的角点经常落在目标外部，没有足够的目标特征改进。<br>backbone：Hourglass Network<br>heatmap：*5 4个极值点+1个中心点。<br>根据顶、底、左、右四个点集，从四个点集中各抽取一个得到四个极值，计算几何中心坐标，找到该中心坐标在中心点heatmap中的得分，如果高于阈值，那么这四个极值点组成的bounding box返回一个最终得分→五个点得分的平均值。<br>Ghost box抑制：解决比如要找2的中心，有可能找的是更大的这个框而不是2的box。如果box，其里面所有的包围框的得分超过了其本身得分的3倍→修正为原来的1/2</p><p>Edge aggregation：解决极值点不唯一问题。对于左边右边极值点在竖直方向聚合；顶部底部极值点在水平方向聚合。【沿着聚合方向，将第一个单调下降区间内的点的score按一定权重累加到原极值点上，并在达到局部最小值的时候停止聚合】</p><h2 id="Objects-as-Points【Centernet】"><a href="#Objects-as-Points【Centernet】" class="headerlink" title="Objects as Points【Centernet】"></a>Objects as Points【Centernet】</h2><p>只预测中心点。通过检测物体的中心点以及中心点对应的w,h来实现检测。不需要group操作<br>input：512<br>backbone: DLA 沙漏型<br>hourglass network 姿态检测<br>output：fm128 降了4倍 → 三个head<br>三个head：<br>1、    heatmap【功能是预测中心点】：128<em>128</em>class；同CornerNet<br>关于高斯圆的半径确定，iou overlap 情况讨论：三种情况的半径，预测的全覆盖；gt全覆盖；交错 overlap=0.7作为临界值，取最小值作为高斯核的半径R。<br>2、128<em>128</em>2；【对应location宽、高】使用L1<br>3、28<em>128</em>2；【中心点：细化调整 offset  x y】引入偏置的损失值，降4倍后取证会带来误差。<br>o是预测的偏移值数量 R表示Heatmap的缩放因子 p~是缩放后取证的坐标</p><p>为什么没有给每个类别预测宽高？考虑物体中心点不会重合<br>3D目标检测：<br>1、depth不好直接回归； 在特征点估计网络上添加了一个深度计算通道，L1 loss。参考文章：Depth map prediction from a single image using a multi-scale deep network.<br>2、l w h L1 loss<br>3、方向也很难回归；用两个bins来呈现方向→方向用8个标量值来编码的形式，每个bin有4个值。对于一个bin，两个值用作softmax分类，其余两个值回归到在每个bin中的角度。参考文章：3d bounding box estimation using deep learning and geometry.√</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>backbone挖坑中</title>
    <link href="https://betterwyl.github.io/2021/10/10/backbone%E8%80%95%E8%80%98%E7%89%88/"/>
    <id>https://betterwyl.github.io/2021/10/10/backbone%E8%80%95%E8%80%98%E7%89%88/</id>
    <published>2021-10-10T03:23:22.000Z</published>
    <updated>2022-11-20T14:53:23.380Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>系统学一下backbone吧 </p><h1 id="ResNet系列"><a href="#ResNet系列" class="headerlink" title="ResNet系列"></a>ResNet系列</h1><p>深度网络讨论：网络深度增加时<br>1、出现了退化问题（Degradation problem）：网络准确度出现饱和，甚至出现下降。【排除过拟合，因为训练的误差也很高】<br>2、存在着梯度消失或者爆炸的问题，这使得深度学习模型很难训练。【梯度消失：权值更新的时候，越前面的更新慢，越后面的更新相对正常。那么前面的层相当于只做简单因那个蛇，深层网络学习等价于后面几层的学习了。梯度爆炸：一般出现在深层网络和权值初始化值太大的情况下。在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【图像分类】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>车道线论文阅读 （施工多年版本）</title>
    <link href="https://betterwyl.github.io/2021/09/23/%E8%BD%A6%E9%81%93%E7%BA%BF%E8%AE%BA%E6%96%87%E6%96%BD%E5%B7%A5%E4%B8%AD/"/>
    <id>https://betterwyl.github.io/2021/09/23/%E8%BD%A6%E9%81%93%E7%BA%BF%E8%AE%BA%E6%96%87%E6%96%BD%E5%B7%A5%E4%B8%AD/</id>
    <published>2021-09-23T02:21:22.000Z</published>
    <updated>2022-11-20T14:54:53.833Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>英东师兄给发了一大堆车道线论文。</p><p>基于语义分割的方法</p><h2 id="SCNN"><a href="#SCNN" class="headerlink" title="SCNN"></a>SCNN</h2><p>将传统的卷积层接层的连接形式的转为feature map中片连片卷积的形式。<br>创新点：适用于检测长距离连续形状的目标或大型目标，有着极强的空间关系但是外观线索较差的目标，例如交通线，电线杆和墙。<br>空间信息能在卷积层上传播。【feature map的行或列也看成layer，也使用卷积加非线性激活，从而实现空间上的深度神经网络。】<br>数据集：CULane Dataset 提出<br><a href="https://xingangpan.github.io/projects/CULane.html" target="_blank" rel="noopener">https://xingangpan.github.io/projects/CULane.html</a><br>没有包括一些车道线模糊，条件恶劣的情况，而这些情况人类可以推断出来，且这具有很高的实用价值。<br>每张图使用3条注释。<br>*传统的关于空间关系的建模方法是基于概率图模型的，例如马尔科夫随机场(MRF)或条件随机场(CRF)。【要去了解一下】<br>D、U、R、L是四个信息传递模块。D、U沿着H方向做了从上到下和从下到上的信息传递；R、L沿着W方向做了从左到右和从右到左的信息传递。每一个模块的卷积函数都共享同一个卷积核。</p><p>MRF/CRF中每个像素点会直接接收其他所有像素点的信息(大卷积核实现)，这其中有许多冗余计算；而SCNN在信息传递顺序传递，由此简化了信息传递的结构加快了模型的运算效率。</p><p>output：加入一个分支网络。【这个分支网络能够直接区分不同车道标记，这样鲁棒性更好。共有4中类型的车道线。输出的概率图经过这个分支网络预测车道标记是否存在。】</p><h2 id="SAD【开源-语义分割-知识蒸馏】"><a href="#SAD【开源-语义分割-知识蒸馏】" class="headerlink" title="SAD【开源/语义分割+知识蒸馏】"></a>SAD【开源/语义分割+知识蒸馏】</h2><p>提出基于知识蒸馏的车道线检测模型SAD，允许车道检测网络在不需要额外标签和外部监督的情况下加强自身的表征学习。增强CNN的特征表达能力。SAD只参与模型训练，因此不会增加推断时的计算复杂度。<br>*知识蒸馏补充（模型压缩）：<br>Net-T完整地学习Ground Truth →由Net-S同时学习Net-T的Logit和Ground Truth，最终Net-S作为应用模型，而Net-T并不进行部署上线。<br>Logit：模型输出的对于各个类别的概率预测值<br>Ground Truth：真值<br>hard target 包含的信息量（信息熵）很低【类似于one-hot，有一个特别突出】，soft target包含的信息量大，拥有不同类之间关系的信息。<br>    更改损失函数→Net-S两部分的知识——<br>Loss-soft：Net-S的输出和Net-T的分布差异。带有T的目标。<br>Loss-hard：GroundTruth的分布差异，可理解为就是正常的分类，有明确的答案。<br>论文中说：第一个目标函数的权重要大一些<br> q是Net-S的输出，p是Net-T的输出，c是Ground Truth</p><p>q和p要用softmax-T公式：<br> →<br>【T=1就是softmax。T参数是一个温度超参数，按照softmax的分布来看，随着T参数的增大，这个软目标的分布更加均匀（平缓）。】</p><p>知识蒸馏图解<br>1 训练大模型：先用hard target，也就是正常的label训练大模型。<br>eg. T-Net：对softmax（T=1）的输出与原始label求loss2<br>2 计算soft target：利用训练好的T-Net大模型来计算soft target（也就是大模型“软化后”再经过softmax的output），再加一个额外的S-Net的soft target。<br>eg.对Teacher的softmax（T=20）输出＋softmax（T=20）的输出=loss1。<br>3 训练小模型，通过lambda来调节两个loss functions的比重。<br>loss = loss1+loss2<br>4 预测：将训练好的小模型按常规方式使用。<br>　　　　　　　<br>创新点：<br>首次使用网络自身的注意力地图作为蒸馏目标。<br>第一次尝试使用网络自身的注意力地图作为蒸馏目标。<br>语义分割任务→通过层间的信息流动，使网络可以在深层保留场景的上下文信息。该算法的这一特性刚好匹配车道线检测任务的特点，即车道线长且窄的形态</p><h2 id="CurveLane-NAS【弯道车道线检测】"><a href="#CurveLane-NAS【弯道车道线检测】" class="headerlink" title="CurveLane-NAS【弯道车道线检测】"></a>CurveLane-NAS【弯道车道线检测】</h2><p>Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending华为诺亚方舟实验室<br>AutoML链→<a href="https://github.com/huawei-noah/vega【新东西的使用？】" target="_blank" rel="noopener">https://github.com/huawei-noah/vega【新东西的使用？】</a><br>从网络搜索的角度，针对长条形车道线，捕捉车道线全局的连贯性特征和局部的弯曲特征。<br>Elastic Backbone Search Module，有效提取车道线的语义特征和隐藏特征；<br>Feature Fusion Search Module，充分融合不同尺度的全局和局部特征；<br>Adaptive Point Blending Search Module，一种多尺度后处理策略，根据不同尺度的预测结果，得到一个更为准确的车道线检测结果。</p><p>数据集：CurveLanes华为发布的CurveLanes数据集中90%以上都是曲线车道，约包含13万5000张图像（共15万张图像）数据集有100k训练图片，仅标注了车道线形状，未标注车道线类型。<br>*NAS补充：（网络结构搜索）简单来讲就是定义网络结构参数的自动调优</p><p>基于分类方法</p><h2 id="Ultra-Fast-Structure-aware-Deep-Lane-Detection-【开源】"><a href="#Ultra-Fast-Structure-aware-Deep-Lane-Detection-【开源】" class="headerlink" title="Ultra Fast Structure-aware Deep Lane Detection 【开源】"></a>Ultra Fast Structure-aware Deep Lane Detection 【开源】</h2><p>这篇文章从分类的角度来构思车道线检测这个问题。（行分类）<br>提出了基于row anchor的网络【最后面增加1个空cell，这个cell表示该row anchor中没有车道线】，让网络在不同的行中选择属于车道线的列，减少了传统语义分割pixel level prediction的复杂度，同时使用global feature来增加网络的感受野， 提升在有车辆遮挡关系下的网络推理能力。<br>*补充感受野：<br>卷积神经网络每一层输出的feature map上的像素点在输入图片上映射的区域大小。【特征图上一个点→代表原图一个区域】<br>感受野大小的计算：<br> 从前往后算 （lk,lk-1感受野大小，fk卷积核大小，s步长）<br> 从后往前算 （先计算最深层在前一层上的感受野，然后逐渐传递到第一层）<br>【计算感受野大小时，忽略了图像边缘的影响，即不考虑padding的大小】<br>eg.典型的VGG net问题—— 2个3x3的卷积核替代5x5的卷积核。<br>    创新点：<br>    提出了一种新的车道检测公式，旨在以极快的速度解决无视觉障碍问题。【与深度分割方法相比，不是分割每个像素，而是选择车道的位置。】<br>    基于提出的公式，我们提出了一种结构损失，以明确利用车道的先验信息。<br>    特点：速度快<br>具体来说，我们的公式是使用全局特征在图像的预定义行中选择车道位置，而不是基于局部感受野分割车道的每个像素，这显著降低了计算成本。<br>【借助于全局特征→具有整个图像的感受野。与基于有限感受野的分割相比，可以学习和利用来自不同位置的视觉线索和信息。】<br>它将问题转化为对图像中的特定行进行分类，每一个类别代表车道线所在的一个位置。图像行上的格网点位置。<br>    操作：<br>对原图进行一定降采样操作后【降采样到800x288】→一个Feature-map<br>    每一个channel代表一条特定的车道线；<br>    每一行对应原图中某几行组成的一个Row anchor；<br>    Row中的每一个col，对应了原图中某几列的位置，另外有一个额外的col，代表了无车道线，即背景类。<br>核心：<br>    车道检测新公式<br>input：H×W图片<br>最大车道数假设为C。<br>基于全局特征预测每行锚点上所有位置的概率分布。因此，可以根据概率分布选择正确的位置。<br> 【每一行都有一个坐标。i车道索引，对应j列。】<br>监督使用的是交叉熵的形式。P是预测T真实。</p><p>跑得快的原因：<br>对于文章的方法其是在特征图上进行的。特征图尺寸h×w进行的行切分之后是1<em>w的特征，进行预测之后得到 (w+1)维度的预测结果。c*h</em>(w+1)<br>分割:H<em>W</em>(C+1)<br>视觉不可见场景下的预测:<br>使用全连接的方式从而拥有了全局的视野，从而增加了对于这样场景的适应性。【有点不理解。如何通过这种方式，利用来自其他位置的信息，文中说公式具有整个图像的感受野，比分割方法大得多。从学习的角度来看，还可以使用基于我们公式的结构损失来学习车道形状和方向等先验信息】<br>另一个显著的好处是，这种公式以基于行的方式建立车道位置模型，这使我们有机会明确地建立不同行之间的关系。该方法可以弥补由低级像素化建模和高级车道长线结构造成的原有语义鸿沟。【问题如何明确地建立不同行之间的关系。】<br> 车道线的结构Loss:</p><p> 相邻行锚中的车道点应该彼此接近<br>【比如说第二行和第一行同一条车道线，那个小块他们接近。】<br> 车道形状——相邻行之间的朝向性应该是一致的【这边有点不解】<br>第j行，第i个车道的位置，选择预测概率最大的作为预测值。但不可导，换个形式。→使用二阶差分的形式。</p><p>特征聚合：<br>最后一项是分割的损失【有点不理解，文中说使用交叉熵作为辅助分割损失】<br>【复现指南：<a href="https://blog.csdn.net/weixin_46716951/article/details/112650165?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-6.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-6.no_search_link】" target="_blank" rel="noopener">https://blog.csdn.net/weixin_46716951/article/details/112650165?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-6.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-6.no_search_link】</a></p><p>基于锚点的方法</p><h2 id="Line-CNN代码不是公开的"><a href="#Line-CNN代码不是公开的" class="headerlink" title="Line-CNN代码不是公开的"></a>Line-CNN代码不是公开的</h2><h2 id="LaneATT-基于anchor实现，且应用了注意力机制"><a href="#LaneATT-基于anchor实现，且应用了注意力机制" class="headerlink" title="LaneATT 基于anchor实现，且应用了注意力机制"></a>LaneATT 基于anchor实现，且应用了注意力机制</h2><p>Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection<br><a href="https://github.com/lucastabelini/LaneATT" target="_blank" rel="noopener">https://github.com/lucastabelini/LaneATT</a><br>LaneATT是一种基于锚的单级模型，类似YOLOv3或SSD。提出了一种新的基于锚点的聚合全局信息的注意机制。<br>*注意力机制补充<br>三种注意力域，空间域(spatial domain)，通道域(channel domain)，混合域(mixed domain)<br>通俗解释：attention机制可以它认为是一种资源分配的机制，可以理解为对于原本平均分配的资源根据attention对象的重要程度重新分配资源，重要的单位就多分一点，不重要或者不好的单位就少分一点，在深度神经网络的结构设计中，关注权重。<br>很多种类别：<br><a href="https://zhuanlan.zhihu.com/p/146130215" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/146130215</a></p><p>    空间域(spatial domain)<br>一般认为，卷积神经网络中的池化层（pooling layer）直接用一些max 法或者average 法的方法，将图片信息压缩，减少运算量提升准确率。【改进：直接将信息合并会导致关键信息无法识别出来，所以提出了一个叫空间转换器（spatial transformer）的模块。】<br>因为训练出的spatial transformer能够找出图片信息中需要被关注的区域，同时transformer又能够具有旋转、缩放变换的功能，这样图片局部的重要信息能够通过变换而被框盒提取出来。<br>    通道域(channel domain)<br>通道域的注意力机制原理【可以从基本的信号变换的角度去理解。信号系统分析里面，任何一个信号其实都可以写成正弦波的线性组合，经过时频变换后，时域上连续的正弦波信号就可以用一个频率信号数值代替了。】<br>eg.在卷积神经网络中每一张图片初始会由（R，G，B）三通道表示出来，之后经过不同的卷积核之后，每一个通道又会生成新的信号。<br>图片特征的每个通道使用64核卷积，就会产生64个新通道的矩阵（H,W,64），每个信号都可以被分解成核函数上的分量，产生的新的64个通道对于关键信息的贡献不定→给每个通道上的信号都增加一个权重，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高，即关注的权重（注意力）。<br>方法    优缺点<br>空间域    将每个通道中的图片特征同等处理。这种做法会将空间域变换方法局限在原始图片特征提取阶段<br>通道域    对一个通道内的信息直接全局平均池化，而忽略每一个通道内的局部信息。</p><p>    混合【没太看明白？？？】<br>里面有一种残差注意力学习(residual attention learning)<br><a href="https://mp.weixin.qq.com/s/KKlmYOduXWqR74W03Kl-9A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/KKlmYOduXWqR74W03Kl-9A</a></p><p> 核心：<br>input:一幅图像→使用Resnet作为特征提取【骨干网】，生成一个特征映射→汇集起来提取每个锚的特征+与一组由注意力模块产生的全局特征相结合，通过结合局部和全局特征，这在遮挡或没有可见车道标记的情况下可以更容易地使用来自其他车道的信息。<br>主干从输入图像生成特征映射。每个锚被投影到特征图上。这个投影用于汇集与注意力模块中创建的另一组特征相连接的特征。最后，使用得到的特性集【两层，一层用于分类，另一层用于回归】组合特征传递给全连接层→预测最终的输出车道。<br>anchor用原点和方向角表示。【车道线起点的意思，出现在一张图片的左右下】<br>output: 输出车道边界线<br>对于每个anchor，网络最终输出3类信息：<br>    K+1概率（K条车道线类型和一个类别的背景）；<br>    offset：水平偏移x0，x1…（预测与锚线之间的水平距离）；<br>    车道线长度l<br> NMS：</p><p>xa、xb是两条车道。当两者距离小于阈值时，该anchor被当作正样本，当两者距离大于阈值时，该anchor被当作负样本。</p><p> Loss：</p><p>*补充Facal loss：【理解为用一个函数去度量难分类和易分类样本对总的损失的贡献。】<br>负样本数量太大，占总的loss的大部分，而且多是容易分类的，因此使得模型的优化方向并不是我们所希望的那样。这个函数可以通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。<br><a href="https://blog.csdn.net/u014380165/article/details/77019084?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link" target="_blank" rel="noopener">https://blog.csdn.net/u014380165/article/details/77019084?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link</a><br>*补充 此处回归使用smooth L1</p><p>L1范数损失函数，也被称为最小绝对值误差。L1损失的缺点就是有折点，不光滑，导致不稳定。loss对于离群点更加鲁棒，相比于L2损失函数，其对离群点、异常值不敏感，可控制梯度的量级使训练时不容易飞。<br>L2范数损失函数，也被称为最小平方误差（LSE）。受离群点影响较大，即离群点可能放多点权重。【警惕梯度爆炸现象】<br>【启发：可以更改loss来测试性能。】<br><a href="https://blog.csdn.net/l641208111/article/details/114286443?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link" target="_blank" rel="noopener">https://blog.csdn.net/l641208111/article/details/114286443?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link</a><br>【参考复现<a href="https://blog.csdn.net/weixin_50996258/article/details/114194722】" target="_blank" rel="noopener">https://blog.csdn.net/weixin_50996258/article/details/114194722】</a></p><p>##3D-LaneNet: End-to-End 3D Multiple Lane Detection<br>数据集：tuSimple<br>方法：<br>网络内逆投影映射IPM + 基于anchor的车道线表示<br>一个基于CNN的端到端的3D车道线检测网络3D-LaneNet，可以直接检测多个车道，并估计车道曲率<br><em>补充：逆投影变换IPM：<br>动态逆变换主要用于自动驾驶中，安装在车辆上的相机在采集车道线图片时，原本平行的车道线将呈现出一定角度，将图片中具有一定角度的车道线恢复平行的过程，就称为动态逆投影变换。</em>此概念涉及到相机标定知识。</p><p>存在的解决方案：加载离线生成的预映射车道和基于感知的实时车道检测。<br>基于相机的车道检测，一般假设地面是平坦的，如果说投影到三维的坐标系中，违反假设会导致估计不准确。<br>创新性：<br>定义了3D车道线检测任务的度量标准，同时也第一个提出了3D检测任务的解决办法；<br>一种新的双路径【是指信息在两条通路中被处理，正视图和俯视图】架构，部署内部网络特征映射IPM投影？？？？<br>一种新的基于anchor的车道输出表示，使直接的、端到端训练网络，用于3D和基于图像的车道检测。【将问题归结为一个物体检测问题，其中每个车道线都是一个物体，并且其3D曲线模型的估计就像对象的边界框一样。】<br>一种随机生成具有车道拓扑变化(车道数、汇集、分叉)和三维形状的合成样本的方法。<br>模型：<br>输入：一张图（正视图） →基于双特征图image view and top view（对特征图进行一些透射投影变换以生成虚拟的鸟瞰视图）；<br>假设：相机的内参矩阵κ已知（焦距，光心）；车辆相对于路面的侧倾角为0；不知道高度和俯仰角，因为随着车辆的动力学运动，他们是会改变的。<br>输出：道路的纵向切片【避免了聚类、奇异值？？】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【车道线】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E8%BD%A6%E9%81%93%E7%BA%BF%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>小方法</title>
    <link href="https://betterwyl.github.io/2021/09/10/DL%E8%B4%B4%E5%A3%AB/"/>
    <id>https://betterwyl.github.io/2021/09/10/DL%E8%B4%B4%E5%A3%AB/</id>
    <published>2021-09-10T08:33:22.000Z</published>
    <updated>2022-11-20T14:53:38.374Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Norm方法学习"><a href="#Norm方法学习" class="headerlink" title="Norm方法学习"></a>Norm方法学习</h1><p>Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。<br>Batch Normalization的目的：使整个训练样本集feature map满足均值为0，方差为1的分布规律。【在图像输入的时候进行预处理会使得前面部分的网络接收到满足某一分布的特征矩阵】<br>理论上：计算出整个训练集的fm进行标准化处理，但是这样工作量很大。→针对batch进行处理，可以知道batch越大效果越好。<br>在训练过程中要去不断的计算每个batch的均值和方差，并使用移动平均(moving average)的方法记录统计的均值和方差，在训练完后我们可以近似认为所统计的均值和方差就等于整个训练集的均值和方差。然后在我们验证以及预测过程中，就使用统计得到的均值和方差进行标准化处理。<br>计算的feature map每个维度（channel）的均值和方差。<br><a href="https://img-blog.csdnimg.cn/20200221215813522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200221215813522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70</a></p><p><a href="https://img-blog.csdnimg.cn/20200226145423805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200226145423805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70</a></p><p>在视觉领域，其实最常用的还是BN，但BN也有缺点，通常需要比较大的Batch Size。<br>Group Normalization<br>batch size的大小对GN并没有影响，所以当batch size设置较小时，可以采用GN<br>和batch_size无关，我们直接看对于一个样本的情况。假设某层输出得到x，沿channel方向均分成num_groups份，也是对每一份求均值和方差。</p><h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><p>pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。<br>pooling目的是为了保持某种不变性（旋转、平移、伸缩等）<br>常用的有mean-pooling，max-pooling，Stochastic-pooling：<br>mean-pooling，正向传播对邻域内特征点只求平均，优缺点：能很好的保留背景，但容易使得图片变模糊。反向传播特征值根据领域大小被平均，然后传给每个索引位置<br>max-pooling，正向传播对邻域内特征点取最大并记住最大值的索引位置，以方便反向传播能更好的保留纹理信息。反向传播：将特征值填充到正向传播中，值最大的索引位置，其他位置补0。<br>Stochastic-pooling：只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大。接着按照概率值来随机选择，一般情况概率大的，容易被选择到，比如选择到了概率值为0.3的时候，那么（1，2，3，4）池化之后的值为3。使用stochastic pooling时(即test过程)，其推理过程也很简单，对矩阵区域求加权平均即可，比如上面图中，池化输出值为：1×0.1+2×0.2+3×0.3+4×0.4=3。在反向传播求导时，只需保留前向传播已经记录被选中节点的位置的值，其它值都为0,</p><p> global average pooling（全局平均池化）：全局平均池化一般是用来替换全连接层。在分类网络中，全连接层几乎成了标配，在最后几层，feature maps会被reshape成向量，接着对这个向量做乘法，最终降低其维度，然后输入到softmax层中得到对应的每个类别的得分，过多的全连接层，不仅会使得网络参数变多，也会产生过拟合现象，针对过拟合现象，全连接层一般会搭配dropout操作。而全局平均池化则直接把整幅feature maps（它的个数等于类别个数）进行平均池化，然后输入到softmax层中得到对应的每个类别的得分。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【深度学习tips】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0tips%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>传统方法</title>
    <link href="https://betterwyl.github.io/2021/08/17/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95/"/>
    <id>https://betterwyl.github.io/2021/08/17/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95/</id>
    <published>2021-08-17T03:56:22.000Z</published>
    <updated>2022-11-20T14:55:16.065Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h1><h2 id="Viola-Jones-（2001）"><a href="#Viola-Jones-（2001）" class="headerlink" title="Viola-Jones （2001）"></a>Viola-Jones （2001）</h2><p>input→候选框、特征提取、分类器→NMS→output<br><strong>创新点</strong>：<br>1、采用积分图像（integral image）技术，加速对Haar-like输入特征的计算。<br>2、采用 AdaBoost 算法进行特征选择。<br>3、采用检测级联技术提高准确率，允许图像的背景区域被很快丢弃,从而将更多的计算放在可能是目标的区域上，减少了计算开销。<br><strong>Haar特征</strong>：在一定程度上反应了图像灰度的局部变化，这种特征捕捉图像的边缘、变化等信息。<br>人脸的五官有各自的亮度信息，例如眼睛比周围区域的颜色要深，鼻梁比两侧颜色要浅。Haar-like特征对于这些“块特征”（眼睛，嘴，发际线）具有比较好的效果，但对树枝或主要靠外形（如咖啡杯）的物体不适用。<br>计算：卷积定理：一个W×H 的图像与 m×n 的filter 做卷积，新生成的图像大小为(W−m+1)×(H−n+1), 新图像的每一个像素其实就是原图一个m×n 的local patch与 m×n 的filter 的乘积和。新图像有多少个像素，就对应着原图多少个m×n 的矩形<br><strong>积分图特征</strong>：就是像素左上角的累加。<br><strong>AdaBoost分类器</strong>：针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。【集成学习】同时进行特征选择与分类器训练。<br>例子：<a href="https://zhuanlan.zhihu.com/p/27126737" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27126737</a><br><strong>级联检测</strong>：AdaBoost训练出来的强分类器一般具有较小的误识率，但检测率并不很高，一般情况下，高检测率会导致高误识率，这是强分类阈值的划分导致的，要提高强分类器的检测率就要降低阈值，要降低强分类器的误识率就要提高阈值，这是个矛盾的事情。而级联检测通过增加分类器个数可以在提高强分类器检测率的同时降低误识率。</p><p><strong>存在问题</strong>：<br>Haar-like特征是一种相对简单的特征，其稳定性较低<br>弱分类器采用简单的决策树，容易过拟合。因此，该算法对于解决正面的 人脸效果好，对于人脸的遮挡，姿态，表情等特殊且复杂的情况，处理效果不理想<br>基于VJ-cascade的分类器设计，进入下一个分类器后，之前的信息都丢弃了，分类器评价一个样本不会基于样本在之前步骤中的表现，导致分类器的鲁棒性差。</p><h2 id="HOG-SVM-（2005CVPR-行人检测）"><a href="#HOG-SVM-（2005CVPR-行人检测）" class="headerlink" title="HOG+SVM （2005CVPR 行人检测）"></a>HOG+SVM （2005CVPR 行人检测）</h2><p><strong>HOG特征</strong>：<br>1、灰度化（如果是灰度图，只计算目标像素的梯度，如果是彩色图，那么就要计算各个RGB分量上的梯度）+伽马变化<br>2、梯度计算 sobel 121<br>3、直方图 例子：input 64×128 8x8个像素作为一个cell→这样把图像分割成了8x16个cell<br>横轴：[0, 180]度以20度为一个bin，平均分成9份；<br>纵轴：按照像素梯度的方向找对应的bin，然后把该像素梯度对应的幅值按照比例放入到相应的bin中。<br>Block混叠空间块的归一化：选用了2x2的Block，即4个9×1的直方图组合成一个36×1的向量构成一个直方图。归一化，然后间隔一个Cell依次向后循环，直到扫描完整张图。<br>构建HOG特征描述子：若2×2Block，对于例子的图，水平7个，竖直15个，那么共有105个，一组36个向量，总共有有3780维向量。<br>优缺点：<br>核心思想是所检测的局部物体外形能够被梯度或边缘方向的分布所描述，HOG 能较好地捕捉局部形状信息，对几何和光学变化都有很好的不变性；HOG 是在密集采样的图像块中求取的，在计算得到的 HOG 特征向量中隐含了该块与检测窗口之间的空间位置关系；<br>很难处理遮挡问题，人体姿势动作幅度过大或物体方向改变也不易检测（这个问题后来在DPM中采用可变形部件模型的方法得到了改善）； 跟SIFT相比，HOG 没有选取主方向，也没有旋转梯度方向直方图，因而本身不具有旋转不变性，其旋转不变性是通过采用不同旋转方向的训练样本来实现的； 跟SIFT相比，HOG 本身不具有尺度不变性，其尺度不变性是通过缩放检测窗口图像的大小来实现的；由于梯度的性质，HOG 对噪点相当敏感，在实际应用中，在 Block 和 Cell 划分之后，对于得到各个像区域中，有时候还会做一次高斯平滑去除噪点。<br><strong>SVM分类器</strong>：<a href="https://zhuanlan.zhihu.com/p/31886934" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31886934</a><br>特征空间上的间隔最大的线性分类器；对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。<br><strong>NMS</strong>：筛选出一定区域内属于同一种类得分最大的框。<br>非极大抑制的执行过程如下所示：<br>1、对所有图片进行循环。<br>2、找出该图片中得分大于门限函数的框。在进行重合框筛选前就进行得分的筛选可以大幅度减少框的数量。<br>3、判断第2步中获得的框的种类与得分。取出预测结果中框的位置与之进行堆叠。此时最后一维度里面的内容由5+num_classes变成了4+1+2，四个参数代表框的位置，一个参数代表预测框是否包含物体，两个参数分别代表种类的置信度与种类。<br>4、对种类进行循环，非极大抑制的作用是筛选出一定区域内属于同一种类得分最大的框，对种类进行循环可以帮助我们对每一个类分别进行非极大抑制。<br>5、根据得分对该种类进行从大到小排序。<br>6、每次取出得分最大的框，计算其与其它所有预测框的重合程度，重合程度过大的则剔除。<br><strong>Soft-NMS</strong>：</p><h2 id="DPM-（2008-物体检测基于HOG）"><a href="#DPM-（2008-物体检测基于HOG）" class="headerlink" title="DPM （2008 物体检测基于HOG）"></a>DPM （2008 物体检测基于HOG）</h2><p>DPM改进后取消了原HOG中的块(Block)，只保留了单元（Cell）：但归一化时，是直接将当前单元与其周围的4个单元（Cell）所组成的一个区域归一化，所以效果和原HOG特征非常类似。<br>采用了有符号梯度和无符号梯度相结合的策略：计算梯度方向时可以计算有符号（0-360°）18维或无符号（0-180°）9维梯度方向，一共27维。<br>之前使用PCA来降维；DPM近似处理：对27维直方图求和，4个单元每个单元27维向量拼接，一共有31维特征。<br>计算响应图，代表能量分布【梯度方向越亮的方向可以解释为行人具有此方向梯度的可能性越大】→Latent SVM训练→检测识别</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="https://betterwyl.github.io/2020/05/19/test/"/>
    <id>https://betterwyl.github.io/2020/05/19/test/</id>
    <published>2020-05-19T03:56:37.000Z</published>
    <updated>2022-11-12T07:39:12.210Z</updated>
    
    <content type="html"><![CDATA[<p>test<br>1111</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;test&lt;br&gt;1111&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="番茄炒鸡蛋【话很多】" scheme="https://betterwyl.github.io/tags/%E7%95%AA%E8%8C%84%E7%82%92%E9%B8%A1%E8%9B%8B%E3%80%90%E8%AF%9D%E5%BE%88%E5%A4%9A%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>奇思妙想更新中</title>
    <link href="https://betterwyl.github.io/2020/03/17/Forever/"/>
    <id>https://betterwyl.github.io/2020/03/17/Forever/</id>
    <published>2020-03-17T03:56:22.000Z</published>
    <updated>2022-10-30T04:47:39.190Z</updated>
    
    <content type="html"><![CDATA[<p>——————————————————————————</p><p>“可是小时候也是同一个我，<br>用一个下午的时间看蚂蚁搬家，等石头开花，<br>小时候不期待结果，小时候哭笑都不打折。”</p><p>——————————————————————————</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;——————————————————————————&lt;/p&gt;
&lt;p&gt;“可是小时候也是同一个我，&lt;br&gt;用一个下午的时间看蚂蚁搬家，等石头开花，&lt;br&gt;小时候不期待结果，小时候哭笑都不打折。”&lt;/p&gt;
&lt;p&gt;——————————————————————————&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="番茄炒鸡蛋【话很多】" scheme="https://betterwyl.github.io/tags/%E7%95%AA%E8%8C%84%E7%82%92%E9%B8%A1%E8%9B%8B%E3%80%90%E8%AF%9D%E5%BE%88%E5%A4%9A%E3%80%91/"/>
    
  </entry>
  
</feed>
