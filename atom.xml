<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yulin Wang</title>
  
  <subtitle>Be patient, brave and honesty</subtitle>
  <link href="https://betterwyl.github.io/atom.xml" rel="self"/>
  
  <link href="https://betterwyl.github.io/"/>
  <updated>2024-08-26T03:01:14.470Z</updated>
  <id>https://betterwyl.github.io/</id>
  
  <author>
    <name>wyl</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>视频防抖算法研究</title>
    <link href="https://betterwyl.github.io/2024/06/28/paper3/"/>
    <id>https://betterwyl.github.io/2024/06/28/paper3/</id>
    <published>2024-06-28T10:17:47.000Z</published>
    <updated>2024-08-26T03:01:14.470Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="基于opencv的视频防抖demo"><a href="#基于opencv的视频防抖demo" class="headerlink" title="基于opencv的视频防抖demo"></a>基于opencv的视频防抖demo</h3><p>稳像可视化：基于opencv的视频防抖demo</p><iframe src="//player.bilibili.com/player.html?isOutside=true&aid=112700277393016&bvid=BV1uShTerEXD&cid=500001599980044&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h3 id="时间同步："><a href="#时间同步：" class="headerlink" title="时间同步："></a>时间同步：</h3><p>方案一：效率优先</p><p>同步点可视化</p><img src="/2024/06/28/paper3/fast.png" alt="nihe_curve.png" style="zoom:50%;"><p>误差拟合曲线：</p><img src="/2024/06/28/paper3/nihe_curve.png" alt="nihe_curve.png" style="zoom:50%;"><p>方案二：精度优先</p><p>同步点可视化：</p><img src="/2024/06/28/paper3/better.png" alt="nihe_curve.png" style="zoom:50%;"><p>误差拟合曲线：</p><img src="/2024/06/28/paper3/nihe_curve1.png" alt="nihe_curve.png" style="zoom:50%;">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="【project】" scheme="https://betterwyl.github.io/tags/%E3%80%90project%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>基于 4D 毫米波雷达点云的 3D 目标检测</title>
    <link href="https://betterwyl.github.io/2024/05/29/paper1/"/>
    <id>https://betterwyl.github.io/2024/05/29/paper1/</id>
    <published>2024-05-29T10:17:47.000Z</published>
    <updated>2024-08-03T03:17:33.567Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Motivation</strong></p><img src="/2024/05/29/paper1/image-20240803105946707.png" alt="image-20240803105946707" style="zoom:50%;"><p><strong>研究内容1：基于关联感知的4D毫米波雷达点云的3D目标检测算法</strong></p><p><img src="/2024/05/29/paper1/image-20240803110507607.png" alt="image-20240803110507607"></p><ol><li>鉴于现有工作在捕捉雷达点云物理语义关联上的局限， 我们提出可学习的局部关联表示， 增强点云之间的信息流； 引入节点-关系融合感知机制， 解决现有方案对局部变换敏感的问题。</li><li>本算法填补了 graph-based 方案在 4D 成像雷达目标检测上的空白， 相比基准模型在 4D 雷达数据集上提升了 21%mAP，比肩雷达点云和摄像头融合算法。</li></ol><p><u>基于图神经网络的毫米波雷达点云的 3D 目标检测方法（ 202311490730.X）</u></p><p>对关联感知机制的探索：</p><img src="/2024/05/29/paper1/image-20240603112117316.png" alt="image-20240603112117316" style="zoom: 33%;"><p><strong>研究内容2：基于对比学习的全稀疏4D毫米波雷达点云的3D目标检测算法</strong></p><ol><li><p>引入帧内和帧间的对比学习，以捕捉毫米波雷达点云样本之间的全局语义关联。</p></li><li><p>为提高处理速度和减少稠密计算资源在稀疏数据上的浪费，设计自适应特征传播的全稀疏网络，实现直接由体素到物体的直接检测。</p></li></ol><p><img src="/2024/05/29/paper1/image-20240803110630666.png" alt="image-20240803110630666"></p><p>结果：</p><img src="/2024/05/29/paper1/image-20240603112035905.png" alt="image-20240603112035905" style="zoom: 50%;">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="【project】" scheme="https://betterwyl.github.io/tags/%E3%80%90project%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>基于原始毫米波雷达数据的目标识别和点云生成</title>
    <link href="https://betterwyl.github.io/2024/05/29/paper2/"/>
    <id>https://betterwyl.github.io/2024/05/29/paper2/</id>
    <published>2024-05-29T10:17:47.000Z</published>
    <updated>2024-08-29T05:31:20.449Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本项目基于<strong>FMCW毫米波雷达ADC信号</strong>提出一种<strong>双分支的目标识别算法</strong>，可实现高精度目标检测和分类。算法能够有效提取雷达信号中的目标特征参数（距离、速度和角度），并通过时频分析和深度学习算法实现了对复杂场景下目标的鲁棒识别，<strong>在Precision上达到92.3%</strong>，相比基准模型<strong>在Recall上提升了38%</strong>。</p><p>预处理过程：</p><p>接收信号与本地参考信号进行混频产生中频后通过ADC进行数字化，再采用如下3D-FFT算法进行处理，分别估计距离、多普勒速度和角度的频谱。</p><p><img src="/2024/05/29/paper2/image-20240701215258230.png" alt="image-20240701215258230"></p><p>算法流程图</p><p><img src="/2024/05/29/paper2/image-20240701220003512.png" alt="image-20240701220003512"></p><p>RA heatmap可视化：</p><p><img src="/2024/05/29/paper2/image-20240603114233373.png" alt="image-20240603114233373"></p><p>生成点云图一览</p><p><img src="/2024/05/29/paper2/pc.png" alt="image-20240803111003283"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="【project】" scheme="https://betterwyl.github.io/tags/%E3%80%90project%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>RAGNN</title>
    <link href="https://betterwyl.github.io/2023/10/15/RAGNN/"/>
    <id>https://betterwyl.github.io/2023/10/15/RAGNN/</id>
    <published>2023-10-15T06:45:43.000Z</published>
    <updated>2023-11-12T02:57:52.956Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RAGNN"><a href="#RAGNN" class="headerlink" title="RAGNN"></a>RAGNN</h1><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1bmCPbHOnLmQBnzzkFGAxJQ </span><br><span class="line">提取码：school</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="paper code" scheme="https://betterwyl.github.io/tags/paper-code/"/>
    
  </entry>
  
  <entry>
    <title>Graph Transformer</title>
    <link href="https://betterwyl.github.io/2023/08/27/Graph-Transformer-learning/"/>
    <id>https://betterwyl.github.io/2023/08/27/Graph-Transformer-learning/</id>
    <published>2023-08-27T06:45:43.000Z</published>
    <updated>2023-09-12T11:34:52.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Graph-Transformer"><a href="#Graph-Transformer" class="headerlink" title="Graph Transformer"></a>Graph Transformer</h1><h2 id="Positional-and-structural-encodings"><a href="#Positional-and-structural-encodings" class="headerlink" title="Positional and structural encodings"></a>Positional and structural encodings</h2><p><strong>位置编码（PE）</strong>意在提供给定节点在空间中的位置的信息。如果两个节点在图或子图中的位置相邻，那么其位置编码应该相近。通常做法是计算一对节点之间的距离，或特征向量。</p><p>Local PE-node features-Within a cluster: 1、随机游走矩阵非对角元求和  2、节点到聚类中心距离</p><p>Global PE-node features -Within a graph  ：1、Eigenvectors of the Adjacency, Laplacian or distance matrices  2、与graph中心的聚类。3、连通成分的独特定义 4、SignNet  </p><p>Relative PE-edge features  ：1、成对距离：最短路径、核、随机游走、贪心等 或者使用全局or局部PE 。2、PEGlayer：位置编码→用于连接预测任务</p><p><strong>结构编码（SE）</strong>旨在提供一个图结构或子图结构的嵌入，以增强GNN的表达能力和泛化能力。如果两个节点拥有相同的子图结构，亦或两个图相似，那么它们的结构编码也应该相似。</p><p>Local SE-node features-m radius 【划定半径范围、相当于子图】：1、度2、随机游走矩阵对角元3、预定义一些结构</p><p>Global SE-graph features  【两张图相似则globalSE也相似】</p><p>Relative SE-edge features  【和Local SE相关的边的嵌入】</p><p><strong>1-Weisfeiler-Leman test</strong></p><p>Weisfeiler-Lehman算法在大多数图上会得到一个独一无二的特征集合，这意味着图上的每一个节点都有着独一无二的角色定位（例外在于网格，链式结构等等）。因此，对于大多数非规则的图结构，得到的特征可以作为图是否同构的判别依据——WL Test（例如两个图是否是同质的，取决于节点的排列方式）。</p><h2 id="Rethinking-Graph-Transformers-with-Spectral-Attention"><a href="#Rethinking-Graph-Transformers-with-Spectral-Attention" class="headerlink" title="Rethinking Graph Transformers with Spectral Attention"></a>Rethinking Graph Transformers with Spectral Attention</h2><p>可学习的位置编码</p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><h3 id="Multi-Headed-Attention-MHA"><a href="#Multi-Headed-Attention-MHA" class="headerlink" title="Multi-Headed Attention (MHA)"></a>Multi-Headed Attention (MHA)</h3><p>1、 Prepare for multi-head attention：每个head 的vector表示做线性变换。</p><p>forward：</p><p>input： <code>[seq_len, batch_size, d_model]</code> or <code>[batch_size, d_model]</code></p><p>在最后一个维使用线性变换，分为多个heads</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.linear &#x3D; nn.Linear(d_model, heads * d_k, bias&#x3D;bias)</span><br><span class="line">head_shape &#x3D; x.shape[:-1] #最后一个维为：d_model</span><br><span class="line">x &#x3D; self.linear(x)</span><br><span class="line">x &#x3D; x.view(*head_shape, self.heads, self.d_k)</span><br></pre></td></tr></table></figure><p>2、MultiHeadAttention</p><p>每一个q、k、v都输入到preparationmultiheadattention的module中取准备映射数据。</p><p>计算分数：query和key</p><p>为每个head计算mask：[seq_len_q, seq_len_k, batch_size]</p><p>forward：input：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input：[query: torch.Tensor,key: torch.Tensor,value: torch.Tensor,mask: Optional[torch.Tensor] &#x3D; None]</span><br><span class="line">query、key和value的原始数据形式 [seq_len, batch_size, d_model]→[seq_len, batch_size, heads, d_k]</span><br><span class="line">计算query和key相乘的得→[seq_len, seq_len, batch_size, heads] .</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>My Work</title>
    <link href="https://betterwyl.github.io/2023/06/14/radarwork/"/>
    <id>https://betterwyl.github.io/2023/06/14/radarwork/</id>
    <published>2023-06-14T05:45:43.000Z</published>
    <updated>2023-07-25T10:06:44.958Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="8dd4ea66b1038f0b968b7b40dca0e9c0f3f1817920513db7bb3ed686c6a805c7">d415e4f17f1453c208180d28e3376a84d86c5ad9a59d1f28e481428f03fb92ef323f346a5609df48048393ca4cd79690535ad816882fb8bf05f6f3f11f718b095df6bb9abad638c8e6141ec942ff2c0d3d314323b3cf2818b14032c38ca402945fbef8bfa8658e81e6d7b54bf9c2ee5b156ea05050f9310fb294628eca1b64cd3b178a68a9980605473d8184eb93d3d2d4b07c05da1eb1419da00988185f45362e44701ef5640aa62225c21805e0fcbab7f8778cb2b8c1bdada6f04b75e4308b5031137b84ddd5dcf8adec7680bac582d38659f08d60e3d19d8f07103bdbaa7d62d6424c5bd23214605153e945116acf767dd01417c430400c2f837470ccfea127bf68bbded79571ea77748e47b3b0d5</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:please wechat me】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>My note on Visualization</title>
    <link href="https://betterwyl.github.io/2023/06/03/visualization/"/>
    <id>https://betterwyl.github.io/2023/06/03/visualization/</id>
    <published>2023-06-03T07:45:43.000Z</published>
    <updated>2023-06-04T10:31:20.007Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="83e52850d11fabfc25f538d217442ff6056b195e3c30e26a3af47bdd29f42bb3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:please wechat me】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>My note on OpenPCDet&amp;MMDet3D</title>
    <link href="https://betterwyl.github.io/2023/04/30/openpcdet/"/>
    <id>https://betterwyl.github.io/2023/04/30/openpcdet/</id>
    <published>2023-04-30T07:45:43.000Z</published>
    <updated>2023-07-25T10:07:53.028Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="OpenPCDet"><a href="#OpenPCDet" class="headerlink" title="OpenPCDet"></a>OpenPCDet</h1><p>在PCDet中搭建3D目标检测框架只需要写config文件将所需模块定义清楚，然后PCDet将自动根据模块间的拓扑顺序组合为3D目标检测框架，来进行训练和测试</p><p><img src="/2023/04/30/openpcdet/image-20230426220506879.png" alt="STRUCTURE"></p><p>数据处理流程</p><h3 id="Dataset-Preparation-以KITTI数据集为例："><a href="#Dataset-Preparation-以KITTI数据集为例：" class="headerlink" title="Dataset Preparation  以KITTI数据集为例："></a>Dataset Preparation  以KITTI数据集为例：</h3><p>更改数据集配置文件： tools/cfgs/dataset_configs/kitti_dataset.yaml</p><p>生成the data infos：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml</span><br></pre></td></tr></table></figure><h3 id="Pretrained-Models"><a href="#Pretrained-Models" class="headerlink" title="Pretrained Models"></a>Pretrained Models</h3><h3 id="Training-amp-Testing"><a href="#Training-amp-Testing" class="headerlink" title="Training &amp; Testing"></a>Training &amp; Testing</h3><p>使用预训练模型测试和使用多GPUs测试</p><ul><li><p>要测试特定训练设置的所有已保存检查点并在 Tensorboard 上绘制性能曲线，请添加参数：<code>--eval_all</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125; --eval_all</span><br></pre></td></tr></table></figure></li><li><p>要使用多个 GPU 进行测试，要执行以下操作：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sh scripts&#x2F;dist_test.sh $&#123;NUM_GPUS&#125; \</span><br><span class="line">    --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125;</span><br><span class="line"></span><br><span class="line"># or</span><br><span class="line"></span><br><span class="line">sh scripts&#x2F;slurm_test_mgpu.sh $&#123;PARTITION&#125; $&#123;NUM_GPUS&#125; \</span><br><span class="line">    --cfg_file $&#123;CONFIG_FILE&#125; --batch_size $&#123;BATCH_SIZE&#125;</span><br></pre></td></tr></table></figure><p>使用多GPUs训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sh scripts&#x2F;dist_train.sh $&#123;NUM_GPUS&#125; --cfg_file $&#123;CONFIG_FILE&#125;</span><br><span class="line"></span><br><span class="line"># or </span><br><span class="line"></span><br><span class="line">sh scripts&#x2F;slurm_train.sh $&#123;PARTITION&#125; $&#123;JOB_NAME&#125; $&#123;NUM_GPUS&#125; --cfg_file $&#123;CONFIG_FILE&#125;</span><br></pre></td></tr></table></figure><h3 id="以VOD数据集为例子PP-Radar复现"><a href="#以VOD数据集为例子PP-Radar复现" class="headerlink" title="以VOD数据集为例子PP-Radar复现"></a>以VOD数据集为例子PP-Radar复现</h3><p>按照openPCDet框架中数据集中Kitti的格式，生成XXXXinfo.pkl 文件</p><p>需要更改两个cfg文件：一个是数据集的cfg文件，一个是模型的cfg文件【采用的是PointPillar Config】。</p><p>在PillarVFE需要进行更改：原因是因为在点云中没有RCS和Doppler特征。</p><p>在tensorboard进行查看：</p><p><img src="/2023/04/30/openpcdet/image-20230430150428353.png" alt="可视化"></p><p>结果：</p><p><img src="/2023/04/30/openpcdet/image-20230430150323635.png" alt="result"></p><p>需要注意的是evaluate方法的修改~</p><h1 id="MMDet3D"><a href="#MMDet3D" class="headerlink" title="MMDet3D"></a>MMDet3D</h1><h2 id="work-flow"><a href="#work-flow" class="headerlink" title="work flow"></a>work flow</h2><p><img src="/2023/04/30/openpcdet/v2-7ecc8e5e19c59a3e6682c5e3cdc34918_r.jpg" alt="workflow"></p><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p> stream: batch →  backbone → neck → head [cls + reg]→  gt bbox encoder→ loss</p><h3 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h3><p>路径：mmdetection3d/mmdet3d/models/backbones</p><p>对backbone进行扩展，可以继承上述网络，然后通过注册器机制注册使用。通过 MMCV 中的注册器机制，你可以通过 dict 形式的配置来实例化任何已经注册的类</p><h3 id="LEARN-ABOUT-CONFIGS"><a href="#LEARN-ABOUT-CONFIGS" class="headerlink" title="LEARN  ABOUT  CONFIGS"></a>LEARN  ABOUT  CONFIGS</h3><p>路径：mmdetection3d/configs/_base_</p><h3 id="把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集-dataset-，模型-model-，训练策略-schedule：Optimization-config-和运行时的默认设置-default-runtime：Hook-config"><a href="#把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集-dataset-，模型-model-，训练策略-schedule：Optimization-config-和运行时的默认设置-default-runtime：Hook-config" class="headerlink" title="把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集 (dataset)，模型 (model)，训练策略 (schedule：Optimization config) 和运行时的默认设置 (default runtime：Hook config)"></a>把这个路径下每个文件夹的内容选取组件进行组合，一共有4个组件：数据集 (dataset)，模型 (model)，训练策略 (schedule：Optimization config) 和运行时的默认设置 (default runtime：Hook config)</h3><p>configs文件的命名风格：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;model&#125;_[model setting]_&#123;backbone&#125;_&#123;neck&#125;_[norm setting]_[misc]_[gpu x batch_per_gpu]_&#123;schedule&#125;_&#123;dataset&#125;</span><br><span class="line">*&#123;xxx&#125; 是被要求填写的字段而 [yyy] 是可选的</span><br><span class="line">    &#123;model&#125;：模型种类，例如 hv_pointpillars (Hard Voxelization PointPillars)、VoteNet 等。</span><br><span class="line">    [model setting]：某些模型的特殊设定。</span><br><span class="line">    &#123;backbone&#125;： 主干网络种类例如 regnet-400mf、regnet-1.6gf 等。</span><br><span class="line">    &#123;neck&#125;：模型颈部的种类包括 fpn、secfpn 等。</span><br><span class="line">    [norm_setting]：如无特殊声明，默认使用 bn (Batch Normalization)，其他类型可以有 gn (Group Normalization)、sbn (Synchronized Batch Normalization) 等。 gn-head&#x2F;gn-neck 表示 GN 仅应用于网络的头部或颈部，而 gn-all 表示 GN 用于整个模型，例如主干网络、颈部和头部。</span><br><span class="line">    [misc]：模型中各式各样的设置&#x2F;插件，例如 strong-aug 意味着在训练过程中使用更强的数据增广策略。</span><br><span class="line">    [batch_per_gpu x gpu]：每个 GPU 的样本数和 GPU 数量，默认使用 4x8。</span><br><span class="line">    &#123;schedule&#125;：训练方案，选项是 1x、2x、20e 等。 1x 和 2x 分别代表训练 12 和 24 轮。 20e 在级联模型中使用，表示训练 20 轮。 对于 1x&#x2F;2x，初始学习率在第 8&#x2F;16 和第 11&#x2F;22 轮衰减 10 倍；对于 20e，初始学习率在第 16 和第 19 轮衰减 10 倍。</span><br><span class="line">    &#123;dataset&#125;：数据集，例如 nus-3d、kitti-3d、lyft-3d、scannet-3d、sunrgbd-3d 等。 当某一数据集存在多种设定时，我们也标记下所使用的类别数量，例如 kitti-3d-3class 和 kitti-3d-car 分别意味着在 KITTI 的所有三类上和单独车这一类上进行训练。</span><br></pre></td></tr></table></figure><h3 id="DATASET-PREPARATION"><a href="#DATASET-PREPARATION" class="headerlink" title="DATASET PREPARATION"></a>DATASET PREPARATION</h3><p>离线转换的方法将其转换为 KITTI数据集的格式，因此只需要在转换后修改配置文件中的数据标注文件的路径和标注数据所包含类别；对于那些与现有数据格式相似的新数据集，如 Lyft 数据集和 nuScenes 数据集，我们建议直接调用数据转换器和现有的数据集类别信息，在这个过程中，可以考虑通过继承的方式来减少实施数据转换的负担。</p><p>当现有数据集与新数据集存在差异时，可以通过定义一个从现有数据集类继承而来的新数据集类来处理具体的差异；最后，用户需要进一步修改配置文件来调用新的数据集。【waymo例子】</p><p>路径：mmdetection3d/mmdet3d/datasets</p><p>自定义数据集：将标注信息重新组织成一个 pickle 文件格式的字典列表 标注框的标注信息会被存储在 <code>annotation.pkl</code> 文件中 在 <code>mmdet3d/datasets/my_dataset.py</code> 中创建一个新的数据集类来进行数据的加载。</p><p>统合数据集或者修改数据集的分布，并应用到模型的训练中。 </p><ul><li><code>RepeatDataset</code>：简单地重复整个数据集</li><li><code>ClassBalancedDataset</code>：以类别平衡的方式重复数据集</li><li><code>ConcatDataset</code>：拼接多个数据集</li></ul><h2 id="CUSTOMIZE-DATA-PIPELINES"><a href="#CUSTOMIZE-DATA-PIPELINES" class="headerlink" title="CUSTOMIZE DATA PIPELINES"></a>CUSTOMIZE DATA PIPELINES</h2><p><img src="/2023/04/30/openpcdet/image-20230426213830604-16828380822496.png" alt></p><p>数据加载、预处理、格式化、测试时的数据增强</p><h2 id="CUSTOMIZE-Model"><a href="#CUSTOMIZE-Model" class="headerlink" title="CUSTOMIZE Model"></a>CUSTOMIZE Model</h2><p>通常把模型的各个组成成分分成6种类型：</p><ul><li>编码器（encoder）：包括 voxel layer、voxel encoder 和 middle encoder 等进入 backbone 前所使用的基于 voxel 的方法，如 HardVFE 和 PointPillarsScatter。</li><li>骨干网络（backbone）：通常采用 FCN 网络来提取特征图，如 ResNet 和 SECOND。</li><li>颈部网络（neck）：位于 backbones 和 heads 之间的组成模块，如 FPN 和 SECONDFPN。</li><li>检测头（head）：用于特定任务的组成模块，如检测框的预测和掩码的预测。</li><li>RoI 提取器（RoI extractor）：用于从特征图中提取 RoI 特征的组成模块，如 H3DRoIHead 和 PartAggregationROIHead。</li><li>损失函数（loss）：heads 中用于计算损失函数的组成模块，如 FocalLoss、L1Loss 和 GHMLoss。</li></ul><p>针对每个组成成分进行添加和修改。</p><h2 id="INFERENCE"><a href="#INFERENCE" class="headerlink" title="INFERENCE"></a>INFERENCE</h2><ul><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id2" target="_blank" rel="noopener">自定义优化器设置</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id9" target="_blank" rel="noopener">自定义训练规程</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id10" target="_blank" rel="noopener">自定义工作流</a></li><li><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/tutorials/customize_runtime.html#id11" target="_blank" rel="noopener">自定义钩子</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>coding指南 2</title>
    <link href="https://betterwyl.github.io/2023/04/02/debug%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/"/>
    <id>https://betterwyl.github.io/2023/04/02/debug%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/</id>
    <published>2023-04-02T06:45:43.000Z</published>
    <updated>2023-04-02T14:46:19.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Point-GNN"><a href="#Point-GNN" class="headerlink" title="Point-GNN"></a>Point-GNN</h1><p>1.os.path.join 路径要写完全了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATASET_DIR=<span class="string">"/dataset/KITTI"</span></span><br><span class="line">DATASET_SPLIT_FILE = os.path.join(DATASET_DIR,<span class="string">'3DOP_splits/'</span>+train_config[<span class="string">'train_dataset'</span>])</span><br><span class="line">print(DATASET_SPLIT_FILE)</span><br><span class="line">/dataset/KITTI/<span class="number">3</span>DOP_splits/train_car.txt</span><br></pre></td></tr></table></figure><p>2.数据增强tips: * *  kwargs和 * args :</p><p>可以设置选择data augment的模式：如下就是选择random_rotation_all模式对应的操作。</p><ul><li><p>args传的是【非键值对】可变数量的参数*<em>列表 *</em></p><ul><li><ul><li>kwargs 传递的就是具体方法涉及的参数信息<strong>不定长度</strong>【键值对】。</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">aug_method = aug_method_map[aug_config[<span class="string">'method_name'</span>]]</span><br><span class="line">cam_rgb_points, labels = aug_method(</span><br><span class="line">    cam_rgb_points, labels, **aug_config[<span class="string">'method_kwargs'</span>])</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> 测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"data_aug_configs"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"method_kwargs"</span>: &#123;</span><br><span class="line">                <span class="string">"expend_factor"</span>: [</span><br><span class="line">                    <span class="number">1.0</span>,</span><br><span class="line">                    <span class="number">1.0</span>,</span><br><span class="line">                    <span class="number">1.0</span></span><br><span class="line">                ],</span><br><span class="line">                <span class="string">"method_name"</span>: <span class="string">"normal"</span>,</span><br><span class="line">                <span class="string">"yaw_std"</span>: <span class="number">0.39269908169872414</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"method_name"</span>: <span class="string">"random_rotation_all"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        ...]</span><br><span class="line">aug_method_map = &#123;</span><br><span class="line">    <span class="string">'random_jitter'</span>: random_jitter,</span><br><span class="line">    <span class="string">'random_box_rotation'</span>: random_box_rotation,</span><br><span class="line">    <span class="string">'random_box_shift'</span>: random_box_shift,</span><br><span class="line">    <span class="string">'random_transition'</span>: random_transition,</span><br><span class="line">    <span class="string">'remove_background'</span>: remove_background,</span><br><span class="line">    <span class="string">'random_rotation_all'</span>: random_rotation_all,</span><br><span class="line">    <span class="string">'random_flip_all'</span>: random_flip_all,...&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_rotation_all</span><span class="params">(cam_rgb_points, labels, method_name=<span class="string">'normal'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    yaw_std=<span class="number">0.3</span>, expend_factor=<span class="params">(<span class="number">1.0</span>, <span class="number">1.1</span>, <span class="number">1.1</span>)</span>)</span>:</span></span><br><span class="line">    xyz = cam_rgb_points.xyz</span><br><span class="line">    <span class="keyword">if</span> method_name == <span class="string">'normal'</span>:</span><br><span class="line">        delta_yaw = np.random.normal(scale=yaw_std)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> method_name == <span class="string">'uniform'</span>:</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>3.Points = namedtuple(‘Points’, [‘xyz’, ‘attr’])</p><p>velo points[x,y,z,flection] velopoints2camera  points xyz in camera with rgb[x,y,z,flection,RGB]</p><p>4.建图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph_generate_fn= get_graph_generate_fn(config[<span class="string">'graph_gen_method'</span>])</span><br><span class="line"></span><br><span class="line">(vertex_coord_list, keypoint_indices_list, edges_list) = \</span><br><span class="line">        graph_generate_fn(cam_rgb_points.xyz, **config[<span class="string">'graph_gen_kwargs'</span>])</span><br></pre></td></tr></table></figure><p>gen_multi_level_local_graph_v3 </p><p>voxel采样：multi_layer_downsampling </p><p>采样后 得到vertex+keypoint_indices 相当于node</p><p>create edges 重要参数 graph_level</p><ul><li><input checked disabled type="checkbox"> 自己采样测试 ：结果1024→816；1000→802</li></ul><p>遍历采样后的每一个点，找邻居值【使用kneighbors】</p><p>5.数据变化过程：以car为例 input：3260samples</p><p>分成两个batch [m,n]【猜测是并行加载数据的操作，看不懂为啥这边要分两个】</p><p>batch_list += [fetch_data(dataset, m或n, train_config, config)] </p><p>input_v, vertex_coord_list, keypoint_indices_list, edges_list, \</p><p>​      cls_labels, encoded_boxes, valid_boxes = batch_data(batch_list)</p><p>cam_rgb_points：[x,y,z,flection,RGB] </p><p>input_v = cam_rgb_points.attr.flection</p><p>假设graph level=2</p><p>points_xyz = vertex_coord_list[0]  </p><p>keypoint_indices_list=特征点的index</p><ul><li><input checked disabled type="checkbox"> 结果分析：</li></ul><p>共有37255个point 【含有xyz坐标：vertex_coord_list[0]  】下采样得到3091个点【含有xyz坐标：points_xyz = vertex_coord_list[1]】</p><p>下采样的点在原始point中的index：keypoint_indices_list: 3,6,19…</p><p>vertex_coord_list [ 0 ]  [ 3 ]==→ vertex_coord_list[ 1 ] [ 0 ]</p><p>【torch.equal(vertex_coord_list[1],vertex_coord_list[2]) =True 为什么是一样的】</p><p>valid_boxes：flag ：bool 表明采样后的 points 在一个 3D box中 比如sum(valid_boxes)=145，有145个点在框中，当然是越大越好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="code已复现备注" scheme="https://betterwyl.github.io/tags/code%E5%B7%B2%E5%A4%8D%E7%8E%B0%E5%A4%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>Motivation</title>
    <link href="https://betterwyl.github.io/2023/04/02/motivation%20pose%20estimation/"/>
    <id>https://betterwyl.github.io/2023/04/02/motivation%20pose%20estimation/</id>
    <published>2023-04-02T06:45:43.000Z</published>
    <updated>2023-04-02T14:16:15.172Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="3D-Pose-estimation-based-graph"><a href="#3D-Pose-estimation-based-graph" class="headerlink" title="3D Pose-estimation based graph"></a>3D Pose-estimation based graph</h1><p>[noticeable]backbone: </p><p>2D-HPE hourglass (NOTICE: which applicated in RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization )</p><p>每个Hourglass module的结构都包含一个bottom-up过程和一个top-down过程，前者通过卷积和pooling将图片从高分辨率降到低分辨率，后者通过upsampling将图片从低分辨率回复到高分辨率。</p><p>基于Hourglass 的改进：Hourglass+Associative Embedding 在多人姿态估计中，先检测身体部位，然后把他们分组给不同的个人。→检测和分组同时进行（Associative Embedding ）</p><p>coordinate/heatmap</p><p>3D-HPE CNN+GCN</p><h2 id="Graph-based-Related-work"><a href="#Graph-based-Related-work" class="headerlink" title="Graph based Related work"></a>Graph based Related work</h2><h2 id="Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks-ICCV2019"><a href="#Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks-ICCV2019" class="headerlink" title="Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks (ICCV2019)"></a>Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks (ICCV2019)</h2><p>code：<a href="https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks" target="_blank" rel="noopener">https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks</a></p><p>Spatial-temporal Graph Construction：一个序列T帧，一帧有M个body joints。Vertices=MT</p><p>local-global：多尺度特征 GCN-based Local-to-global Prediction 【这里很奇怪 代码里没有用到nonlocal3Dblock】</p><p>use different kernels for different neighboring nodes。</p><h2 id="Graph-Stacked-Hourglass-Network-CVPR-2021"><a href="#Graph-Stacked-Hourglass-Network-CVPR-2021" class="headerlink" title="Graph Stacked Hourglass Network (CVPR 2021)"></a>Graph Stacked Hourglass Network (CVPR 2021)</h2><p>问题：  图卷积（只能在一个单一尺度上对特征进行处理，难以提取表征空间的局部和全局空间信息，限制了模型的表征能力，没有利用模型的深度特点）。</p><p>在HourGlassNet 上改进。【HG可看作是conv-deconv或者encoder-decoder的结构】</p><p>downsampling  【 pooling 】 and upsampling 【unpooling 】</p><p>Graph design：residual connections   补充Graph U-Nets</p><p>引入PreAggr 。堆叠了四个 前面multi-scale【spatial aspect of the graph】，后面multi-level【SE block+semantic information】。【保证输入输出通道大小都为64】</p><h2 id="A-Graph-Attention-Spatio-temporal-Convolutional-Network-for-3D-Human-Pose-Estimation-in-Video-ICRA2021"><a href="#A-Graph-Attention-Spatio-temporal-Convolutional-Network-for-3D-Human-Pose-Estimation-in-Video-ICRA2021" class="headerlink" title="A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video(ICRA2021)"></a>A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video(ICRA2021)</h2><p>关键点：图注意时空卷积网络，spectral-based  </p><p>使用方法：图注意力和Semantic graph convolutional networks for 3d human pose regression的工作</p><p>Temporal 改变：1D卷积–2D卷积 基于《3d human pose<br>estimation in video with temporal convolutions and semi-supervised<br>training》方法</p><p>Spatio方法：</p><p><strong>Local Attention Graph</strong> ：每一个node 都有C 个通道特征，经过L个layer。即CL</p><p> two novel convolution kernels【基于身体结构对称性假设】。[noticeable]：one is symmetric kernel </p><p>！each of these two convolution kernels are applied to two distinct GCNs  </p><p><strong>Global Attention Graph</strong>  ：针对于没有直接相连的joint，存在一种sub-segments  关系。【推：没有直接相连的node ，非局部关系→！这么说是可以解决遮挡问题的】</p><h2 id="Modulated-Graph-Convolutional-Network-for-3D-Human-Pose-Estimation-ICCV-2021"><a href="#Modulated-Graph-Convolutional-Network-for-3D-Human-Pose-Estimation-ICCV-2021" class="headerlink" title="Modulated Graph Convolutional Network for 3D Human Pose Estimation (ICCV 2021)"></a>Modulated Graph Convolutional Network for 3D Human Pose Estimation (ICCV 2021)</h2><p>调整GCN中的图结构：使用共享权重而不增加模型参数。</p><h2 id="Semantic-Graph-Convolutional-Networks-for-3D-Human-Pose-Regression-CVPR-2019"><a href="#Semantic-Graph-Convolutional-Networks-for-3D-Human-Pose-Regression-CVPR-2019" class="headerlink" title="Semantic Graph Convolutional Networks for 3D Human Pose Regression (CVPR 2019)"></a>Semantic Graph Convolutional Networks for 3D Human Pose Regression (CVPR 2019)</h2><p>node的<strong>局部和全局关系</strong></p><p>…更新中</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Workshop" scheme="https://betterwyl.github.io/tags/Workshop/"/>
    
  </entry>
  
  <entry>
    <title>CS224W</title>
    <link href="https://betterwyl.github.io/2023/03/30/CS224/"/>
    <id>https://betterwyl.github.io/2023/03/30/CS224/</id>
    <published>2023-03-30T12:45:43.000Z</published>
    <updated>2023-04-05T13:37:10.142Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><h3 id="1-1intro"><a href="#1-1intro" class="headerlink" title="1.1intro"></a>1.1intro</h3><p>图的表示学习：大致来说就是将原始的结点（或链接、或图）表示为向量（嵌入<strong>embedding</strong>），图中相似的结点会被embed得靠近（指同一实体，在结点空间上相似，在向量空间上就也应当相似）。</p><h3 id="1-2Applications-of-Graph-ML"><a href="#1-2Applications-of-Graph-ML" class="headerlink" title="1.2Applications of Graph ML"></a>1.2Applications of Graph ML</h3><p>node level：</p><p>protein folding：给定氨基酸序列计算预测蛋白质的3D结构。模拟蛋白质位置，预测3Dshape。</p><p>edge level：</p><p>recommender systems ：pinsage图像+图结构，更好的推荐 ；任务目标是使相似结点嵌入之间的距离比不相似结点嵌入之间的距离更小。</p><p>药物组合副作用：预测引擎，蛋白质相互作用网络，预测缺失的边缘。</p><p>背景：很多人需要同时吃多种药来治疗多种病症。任务：输入一对药物，预测其有害副作用。</p><p>subgraph level：</p><p>Google map: traffic prediction测一段路程的长度、耗时等：将路段建模成图，在每个子图上建立预测模型</p><p>graph level：</p><p>用GNN的图分类任务来从一系列备选图（分子被表示为图，结点是原子，边是化学键）中预测最有可能是抗生素的分子；</p><p>预测材料变形：用GNN来预测粒子的下一步活动（组成一个新位置、新图</p><h3 id="1-3design-choice："><a href="#1-3design-choice：" class="headerlink" title="1.3design choice："></a>1.3design choice：</h3><p>Bipartite Graph→Folded/Projected Bipartite Graphs</p><p>Representing Graphs：Edge list【难表示】；Adjacency list【对图的分析和操作更方便】</p><p>结点和边的属性：Weight (e.g., frequency of communication)；Ranking (best friend, second best friend…)；Type (friend, relative, co-worker)；Sign: Friend vs. Foe, Trust vs. Distrust；Properties depending on the structure of the rest of the graph: Number of common friends</p><p>Self-edges (self-loops)自环 / Multigraph</p><p><img src="/2023/03/30/CS224/image-20230405203011209.png" alt="Self-edges (self-loops) / Multigraph"></p><p>multigraph有时也可被视作是weighted graph，就是说将多边的地方视作一条边的权重（在邻接矩阵上可看出效果是一样的）。但有时也可能就是想要分别处理每一条边，这些边上可能有不同的property和attribute</p><p>Connectivity</p><p><img src="/2023/03/30/CS224/image-20230403192951728.png" alt="Connectivity"></p><p>connected：任意两个结点都有路径相通 strongly connected components：互相可以访问</p><p>disconnected：由2至多个connected components构成。<br>最大的子连接图：giant component<br>isolated node<br>这种图的邻接矩阵可以写成block-diagonal的形式，数字只在connected components之中出现。【聚类中的块对角个数=类别数】</p><h2 id="2-Feature-Engineering-for-ML-in-Graphs"><a href="#2-Feature-Engineering-for-ML-in-Graphs" class="headerlink" title="2.Feature Engineering for ML in Graphs"></a>2.Feature Engineering for ML in Graphs</h2><p>传统pipelines【2 steps】：设计并获取所有训练数据上结点/边/图的特征→分类器/模型→新结点作出预测</p><p>Features: d-dimensional vectors<br>Objects: Nodes, edges, sets of nodes, entire graphs<br>Objective function: What task are we aiming to solve?</p><h3 id="2-1Node-Level-Feature"><a href="#2-1Node-Level-Feature" class="headerlink" title="2.1Node Level Feature"></a>2.1Node Level Feature</h3><p>1.Importance-based features考虑了结点的重要性；</p><p>2.Structure-based features捕获节点附近的拓扑属性</p><p><strong>eigenvector centrality</strong>：如果结点邻居重要，那么结点本身也重要。与link的数量无关，与link的重要性有关。</p><p><img src="/2023/03/30/CS224/image-20230403200310561.png" alt="eigenvector centrality"></p><p><strong>betweenness centrality</strong>：桥梁！如果一个结点处在很多结点对的最短路径上，那么这个结点是重要的。</p><p><img src="/2023/03/30/CS224/image-20230403200532146.png" alt="betweenness centrality"></p><p><strong>closeness centrality</strong>：交通枢纽！到其他结点的路径长度最小。越居中越短。</p><p><img src="/2023/03/30/CS224/image-20230403200844882.png" alt="closeness centrality"></p><p><strong>clustering coefficient</strong>聚类系数：邻居的联系程度</p><p>第1个例子：6 / 6<br>第2个例子：3 / 6<br>第3个例子：0 / 6 </p><p><img src="/2023/03/30/CS224/image-20230403204703668.png" alt="eg"></p><p><strong>三角形/三元组</strong>：结点自我网络。共友也会互相认识。</p><p>三角形可以拓展到某些预定义的子图pre-specified subgraph</p><p><strong>graphlets</strong></p><p>2-5个节点的graphlets，可以得到一个长度为73个坐标coordinate的向量GDV，描述该点的局部拓扑结构topology of node’s neighborhood，可以捕获距离为4 hops的互联性interconnectivities。<br>相比节点度数或clustering coefficient，GDV能够描述两个节点之间更详细的节点局部拓扑结构相似性local topological similarity。</p><p><img src="/2023/03/30/CS224/image-20230403213451852.png" alt="graphlets"></p><h3 id="2-2Link-Level-Feature"><a href="#2-2Link-Level-Feature" class="headerlink" title="2.2Link Level Feature"></a>2.2Link Level Feature</h3><p>link level prediction：根据网络现有的link预测新的link</p><p>eg基于相似性：算两点间的相似性得分（如用共同邻居衡量相似性），然后将点对进行排序，得分最高的n组点对就是预测结果，与真实值作比</p><p>测试阶段：评估无edge的node pairs→rank k→prediction</p><p>链接预测任务的两种类型：</p><p>随机缺失边【第一种假设可以以蛋白质之间的交互作用举例，缺失的是研究者还没有发现的交互作用。新链接的发现会受到已发现链接的影响。在网络中有些部分被研究得更彻底，有些部分就几乎没有什么了解，不同部分的发现难度不同】；随时间推演边【以社交网络举例，随着时间流转，人们认识更多朋友。】</p><p><strong>distance-based feature</strong></p><p><img src="/2023/03/30/CS224/image-20230403215243135.png" alt="distance-based feature"></p><p><strong>local neighborhood overlap</strong></p><p><img src="/2023/03/30/CS224/image-20230403215255078.png" alt="local neighborhood overlap"></p><p>common neighbors： 度数高将表现结果更高。</p><p>Adamic-Adar index：有点像信息熵，度数低的共友比名人共友价值大。</p><p>如果没有共友的情况，则值为0。【缺点：两个点未来仍有可能被连接起来。相距两跳的结点、没有邻居的结点？解决方案global neighborhood overlap】</p><p><strong>global neighborhood overlap</strong></p><p>获得一对结点的得分。</p><p>Katz index：计算node pairs之间所有长度路径的数量 → 对邻接矩阵求幂</p><p>邻接矩阵：代表u和v之间长度为1的路径的数量</p><p>分解路径：将长度为1转化，通过桥接方法。对起始节点和相邻结点求和，到目标结点。【归纳法】</p><p><img src="/2023/03/30/CS224/image-20230403220039044.png" alt="Katz compute"></p><p>观察其对角线，其实就是邻居的数量。</p><p><img src="/2023/03/30/CS224/image-20230403220152827.png" alt="katz"></p><p>比较长的距离以比较小的权重。</p><p>2.3Graph Level Feature </p><p><strong>核方法</strong>：</p><p><img src="/2023/03/30/CS224/image-20230403223205107.png" alt="KERNEL"></p><p>eg. BoW for graph：将图表示成一个向量，每个元素代表对应something出现的次数（这个something可以是node, degree, graphlet, color）node degrees</p><p><img src="/2023/03/30/CS224/image-20230404113506849.png" alt="BOW"></p><p>eg. Graphlet kernel【侧重于node 和 edge，即结构 】直接点积两个图的graphlet count vector得到相似性。对于图尺寸相差较大的情况需进行归一化。计算代价高，k-size对应n^k</p><p><img src="/2023/03/30/CS224/image-20230404113417281.png" alt="Graphlet kernel"></p><p>eg. Weisfeiler-Lehman Kernel ：建立特征描述子，邻域结构叠代结点度。</p><p>color refinement：</p><p><img src="/2023/03/30/CS224/image-20230404114000690.png" alt="hash"></p><p>把邻居颜色聚集起来，使用hash，重新标记颜色。【优化是线性的】</p><p>进行K次迭代，整个迭代过程中颜色出现的次数作为Weisfeiler-Lehman graph feature。上述向量点积计算相似性，得到WL kernel。</p><h2 id="3-Node-Embeddings"><a href="#3-Node-Embeddings" class="headerlink" title="3.Node Embeddings"></a>3.Node Embeddings</h2><h3 id="3-1encoder-decoder"><a href="#3-1encoder-decoder" class="headerlink" title="3.1encoder-decoder"></a>3.1encoder-decoder</h3><p>graph representation learning：学习到图数据用于机器学习的、与下游任务无关的特征，我们希望这个向量能够抓住数据的结构信息。</p><p>将图数据紧密地嵌入到嵌入空间中→自动对结构信息进行编码用于下游任务。</p><p>eg. node embedding：deep walk</p><p><strong>Encoder and Decoder</strong>：对结点进行编码，保证其在嵌入空间的相似性。在二维的相似性【例如距离】，可反应图的结构。</p><p>Encoder：将节点映射为embedding<br>定义一个衡量节点相似度的函数（如衡量在原网络中的节点相似度）<br>Decoder DEC：将embedding对映射为相似度得分。</p><p><img src="/2023/03/30/CS224/image-20230404122425624.png" alt="encoder+decoder"></p><p>ENC(v)=Z · v </p><p><img src="/2023/03/30/CS224/image-20230404123051920.png" alt="embedding"></p><p>→ Z(d,v)每个节点都有一个列来表示该结点的嵌入。</p><p>→v是一个其他元素都为0，对应节点位置的元素为1的向量。</p><p>【缺陷：参数过多，难scale up到大型图；Z的大小取决于结点的数量】</p><p>DEC：节点相似的不同定义：边；邻居；相似的structural roles</p><p>random walks：无监督/自监督→使用结点embedding时，并没有使用结点的label和功能，目标只学习了网络的相似性。</p><h3 id="3-2Random-Walk-Approaches-for-Node-Embeddings"><a href="#3-2Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="3.2Random Walk Approaches for Node Embeddings"></a>3.2Random Walk Approaches for Node Embeddings</h3><p>goal：每个结点的嵌入向量z</p><p>随机游走的相似性：使用概率表示，从结点u开始随机走动得到v的概率</p><p><img src="/2023/03/30/CS224/image-20230404141113888.png" alt="random walk"></p><p>random walk：从某一结点开始，每一步按照概率选一个邻居，停止后得到随机游走序列。→点u和v在一次随机游走中出现的概率【相似的网络邻居，彼此之间很近，之间可能有多条路径】</p><p><img src="/2023/03/30/CS224/image-20230404144913123.png" alt="finally"></p><p>复杂度O(V^2)。改进：softmax归一化的分母。将抽出k个结点作为负样本代替所有结点作为负样本。【k值大相当于加入了更多数据，方差会变小，偏差比较大。】</p><p><img src="/2023/03/30/CS224/image-20230404145538706.png" alt="negative sampling"></p><p>random walk策略</p><p>eg.deep walk：一阶随机游走</p><p>eg.node2vec</p><p>灵活的网络邻居【local(BFS) and global(DFS)】→丰富的结点嵌入【biased walk】：引入二阶随机数</p><p>超参数：p：return parameter【记住游走的来源】；q：DFS和BFS的ratio</p><p>例如保持相同的为走到S2，向前迈进为S3/S4。p小则返回，q小则探索。</p><p><img src="/2023/03/30/CS224/image-20230404153209529.png" alt="visualization"></p><p>node2vec在节点分类任务上表现更好，不同的方法在不同数据的链接预测任务上表现不同。</p><h3 id="3-3-Embedding-Entire-Graphs"><a href="#3-3-Embedding-Entire-Graphs" class="headerlink" title="3.3 Embedding Entire Graphs"></a>3.3 Embedding Entire Graphs</h3><p>聚合/加权平均结点的嵌入；</p><p>introduce a virtual node【虚拟结点将连接到网络中所有其他结点】→嵌入子图/整个图→得到全图的信息；</p><p>anonymous walk embeddings：以节点第一次出现的序号（是第几个出现的节点）作为索引。拜访了不同的结点，但用相同的顺序获得相同的anonymous表示。</p><h2 id="4-GNN"><a href="#4-GNN" class="headerlink" title="4.GNN"></a>4.GNN</h2><p>结点嵌入任务目的：在于将节点映射到d维向量，使得在图中相似的节点在向量域中也相似。使用一个大矩阵直接储存每个节点的表示向量，通过矩阵与向量乘法来实现嵌入过程。缺陷：需要O(V)复杂度的参数，太多结点间参数不共享，每个结点表示的向量都是独特的；无法获取在训练时没出现的结点的表示向量，即transductive；无法应用结点的特征信息。</p><p>浅层编码：Encoder+similarity function+Decoder→学习Z矩阵。</p><p>深层编码：可以与结点相似性功能结合使用。</p><h3 id="4-1-basics-of-deep-learning"><a href="#4-1-basics-of-deep-learning" class="headerlink" title="4.1 basics of deep learning"></a>4.1 basics of deep learning</h3><p>supervised learing: x→f→y</p><p><img src="/2023/03/30/CS224/image-20230405094330602.png" alt="func"></p><p><img src="/2023/03/30/CS224/image-20230405095059110.png" alt="optimize"></p><p>梯度向量：函数增长最快的方向和增长率，每个元素是对应参数在损失函数上的偏微分。<br>方向导数：函数在某个给定方向上的变化率。<br>梯度是函数增长率最快的方向的方向导数。</p><p>迭代：将参数向负梯度方向更新；理想的停止条件是梯度为0，在实践中一般则是用“验证集上的表现不再提升”作为停止条件；学习率是一个需要设置的超参数，控制梯度下降每一步的步长，可以在训练过程中改变。</p><p>minibatch SGD：每一次梯度下降都需要计算所有数据集上的梯度，耗时太久→使用SGD的方法，将数据分成多个minibatch，每次用一个minibatch来计算梯度</p><p>SGD是梯度的无偏估计，但不保证收敛，所以一般需要调整学习率。</p><p>batch size：每个minibatch中的数据点数；iteration：在一个minibatch上做一次训练；epoch：在整个数据集上做一次训练。</p><p>前向传播：compute loss starting from input【求下降的多少】</p><p>梯度反向传播：计算梯度是不断向后的过程【求偏导，求下降的最快方向】</p><p><img src="/2023/03/30/CS224/image-20230405100142457.png" alt="vs"></p><p>引入非线性：提高表达力</p><p>MLP：整合线性和非线性</p><h3 id="4-2Deep-Learning-for-Graphs"><a href="#4-2Deep-Learning-for-Graphs" class="headerlink" title="4.2Deep Learning for Graphs"></a>4.2Deep Learning for Graphs</h3><p>set up：</p><p><img src="/2023/03/30/CS224/image-20230405101219610.png" alt="setting"></p><p>naive approach：input【邻接矩阵+node feature 能表示成grid的形式】需要 O(|V|)的参数，不适用于不同大小的图。对节点顺序敏感（我们需要一个即使改变了节点顺序，结果也不会变的模型）</p><p>图上无法定义固定的locality或滑动窗口，而且图是permutation invariant【order是不固定的】</p><p>Graph Convolutional Networks：通过节点邻居定义其计算图，传播并转换信息，计算出节点表示（可以说是用邻居信息来表示一个节点）</p><p>核心思想：通过聚合邻居来生成节点嵌入→通过神经网络聚合信息【每个结点都能定义】</p><p>！在每一层中结点都有不同的嵌入【节点在每一层都有不同的表示向量，每一层节点嵌入是邻居上一层节点嵌入再加上它自己（相当于添加了自环）的聚合。】 k 跳对应k layers；聚合顺序无关紧要 a set of elements</p><p><img src="/2023/03/30/CS224/image-20230405110059635.png" alt="fomular"></p><p>模型上可以学习的参数【只取决于嵌入的大小，而非图形的大小】：W 邻居聚合权重；B 转换结点本身隐藏向量的权重</p><p>网络中所有的结点都使用相同的更新公式。</p><p>使用matrix表示：H 来自上一层结点的嵌入</p><p><img src="/2023/03/30/CS224/image-20230405144050132.png" alt="matrix"></p><p><img src="/2023/03/30/CS224/image-20230405144207898.png" alt="matrix"></p><p>训练：</p><p>有监督使用标签；无监督使用图结构【即相似度】</p><p>定义邻域聚合函数→定义loss函数→set of datasets→为新结点生成嵌入【inductive】</p><h2 id="5-General-Perspective-on-GNNS"><a href="#5-General-Perspective-on-GNNS" class="headerlink" title="5.General Perspective on GNNS"></a>5.General Perspective on GNNS</h2><h3 id="5-1design-space"><a href="#5-1design-space" class="headerlink" title="5.1design space"></a>5.1design space</h3><p><strong>aggregation</strong></p><p><strong>message</strong></p><p> <strong>GNN layers connectivity</strong>即组合layers的方式</p><p><strong>manipulation</strong>：使原始输入图和应用在GNN中的计算图不完全相同（即对原始输入进行一定处理后，再得到GNN中应用的计算图）</p><p><strong>learning</strong></p><h3 id="5-2-single-GNN-layer"><a href="#5-2-single-GNN-layer" class="headerlink" title="5.2 single GNN layer"></a>5.2 single GNN layer</h3><p><strong>message</strong>：self message【来自上一层结点的嵌入】+neighborhoods‘→转换消息 </p><p><strong>aggregation</strong>：聚合消息【阶不变】eg：求和、平均、最大值</p><p>在这两步上都可以用非线性函数（激活函数）来增加其表现力。</p><p><img src="/2023/03/30/CS224/image-20230405155120889.png" alt="gcn"></p><p><img src="/2023/03/30/CS224/image-20230405155141048.png" alt="graphsage"></p><p><strong>GCN：</strong></p><p>message：对上一层的节点嵌入用本层的权重矩阵进行转换，用节点度数进行归一化<br>agg：sum信息，应用激活函数</p><p><strong>GraphSAGE</strong>在GCN上的延展：1.聚合函数可以是任意的:AGG 2.从结点本身获得消息和邻居信息3.L2归一化</p><p>2 stages→agg：聚合neighborhood信息；将上一层信息与节点本身信息进行聚合</p><p>message：在agg过程中一起实现</p><p><strong>GAT</strong>：【邻居的重要性】</p><p>在GCN和GraphSAGE中，直接基于图结构信息（节点度数）显式定义注意力权重，相当于认为节点的所有邻居都同样重要（注意力权重一样大）。</p><p>GAT中注意力权重：attention mechanism：alpha→用两个节点上一层的节点嵌入计算其注意力系数</p><p>mechanism：alpha的形式：可以使用一个layer 难以收敛【改进：多头注意力 三种功能的注意力再汇总，随机初始化，使用不同的函数，每一个alpha收敛到一个局部最小值】</p><h3 id="5-3-stacking-GNN-layers"><a href="#5-3-stacking-GNN-layers" class="headerlink" title="5.3 stacking  GNN layers"></a>5.3 stacking  GNN layers</h3><p>over smoothing：如果GNN层数太多，不同节点的嵌入向量会收敛到同一个值。随着K layers增加和hops的增加，nodes感受野增加也很快。图信号越来越弱。</p><p><img src="/2023/03/30/CS224/image-20230405163916026.png" alt="visualization"></p><p>【idea：如何解决GNN的 over smoothing】分析解决问题所需的必要感受野；设置GNN层数 L 略大于我们想要的感受野</p><p>增加浅层GNN的表现力：将message和agg过程变成深度神经网络→mlp+pre-layer+post-layer</p><p>增加跳层连接：靠前的GNN层可能能更好地区分节点，最终节点嵌入中增加靠前GNN层的影响力，实现方法是在GNN中直接添加捷径，保存上一层节点的嵌入向量。</p><h2 id="5-Applications-of-Graph-Neural-Networks"><a href="#5-Applications-of-Graph-Neural-Networks" class="headerlink" title="5.Applications of Graph Neural Networks"></a>5.Applications of Graph Neural Networks</h2><p>由原始图定义的计算图 →针对这一假设改进【输入图并非一定是最佳计算图】</p><p>node level：输入图可能缺少功能、属性，或者难以encode；→进行特征增强</p><p>graph structure：过于稀疏，消息传递需要多次，效率低。过于密集，消息传递代价高，或者图过大GPU难以使用。</p><h3 id="5-1-Graph-Augmentation-for-GNNs"><a href="#5-1-Graph-Augmentation-for-GNNs" class="headerlink" title="5.1 Graph Augmentation for GNNs"></a>5.1 Graph Augmentation for GNNs</h3><p><strong>Feature Augmentation：</strong>某些结构缺少或者很难学习</p><p>例如在化学中判断结点是否处于一个循环中。在计算图中看起来是一样的。3和4个结点的循环，因为度数相同（都是2），所以无论环上有多少个节点，GNN都会得到相同的计算图（二叉树），无法分别。解决方法是为循环增加one-hot编码。</p><p>constant node feature&amp; one-hot feature</p><p><img src="/2023/03/30/CS224/image-20230405191802807.png" alt="vs"></p><p>其他常用于数据增强的特征：clustering coefficient，centrality，page rank</p><p><strong>Structure Augmentation</strong></p><p>稀疏图：</p><p>增加虚拟边，邻接矩阵的幂次相加【相当于和朋友的朋友成为了朋友，两跳也相连】</p><p>增加虚拟结点，两结点相距过远，牵线搭桥，使得网络不用太大</p><p>稠密图：</p><p>random sample 折衷，计算效率增加但是会使得信息存在丢失</p><h3 id="5-2-training"><a href="#5-2-training" class="headerlink" title="5.2 training"></a>5.2 <strong>training</strong></h3><p><strong>不同粒度下的prediction head</strong>：节点级别，边级别，图级别</p><p>node-level：使用node embedding做预测 →k classification/regression 将d维嵌入映射到k维输出</p><p>edge-level：使用两个node embedding做预测：可以是像注意力机制那样一个点对另一个点剪→alpha对应边，再使用linear将2d维映射到k维输出；也可以是1 way→点积 k way→多头注意力</p><p>graph-level：每个node聚合 pooling。小图上表现很好→mean方法：结果不受节点数量的影响；sum方法：关心图的大小等信息。在大图上的global pooling方法可能会面临丢失信息的问题，效果不好。解决方案：分层聚合，子集比较。【idea：diffPool 分层、差分池】</p><p>【缺陷：global pooling采用大量信息】</p><p><strong>prediction&amp;labels</strong></p><p>有监督学习supervise learning：直接给出标签（如一个分子图是药的概率）node-level：引文论文属于的学科 edge-level：交易中是否有欺诈行为 graph-level：药物的毒性<br>无监督学习unsupervised learning / self-supervised learning：使用图自身的信号（如链接预测：预测两节点间是否有边）无需外部标签【node-level：节点统计量（如clustering coefficient<a href="https://blog.csdn.net/PolarisRisingWar/article/details/118001121#fn3" target="_blank" rel="noopener">3</a>, PageRank<a href="https://blog.csdn.net/PolarisRisingWar/article/details/118001121#fn4" target="_blank" rel="noopener">4</a> 等）edge-level：链接预测（隐藏两节点间的边，预测此处是否存在链接）graph-level：图统计量（如预测两个图是否同构）】</p><p><strong>loss</strong></p><p>分类 CE</p><p><img src="/2023/03/30/CS224/image-20230405202957261.png" alt="CE"></p><p>回归 MSE</p><p><img src="/2023/03/30/CS224/image-20230405203011210.png" alt="MSE"></p><p><strong>data split</strong>【idea：关于图结构的data split】</p><p>训练集是可以用来调整GNN参数的。验证集可以用来调整训练的策略，超参。</p><p>fixed split：只切分一次数据集，此后一直使用这种切分方式<br>random split：随机切分数据集，应用多次随机切分后计算结果的平均值</p><p>图结构的特殊性，如果直接像普通数据一样切分图数据集，不能保证测试集隔绝于训练集。【测试集里面的数据可能与训练集里面的数据有边相连，在message passing的过程中就会互相影响，导致信息泄露。】</p><p>解决方案：tranductive setting：输入全图在所有split中可见【测试集、验证集、训练集在同一个图上，整个数据集由一张图构成】→仅切分（节点）标签【仅适用于节点/边预测任务】。inductive setting：去掉各split之间的链接，得到多个互相无关的图。【测试集、验证集、训练集分别在不同图上，整个数据集由多个图构成】这样不同split之间的节点就不会互相影响。→因此可以泛化【适用于节点/边/<u>图预测任务</u>】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Course" scheme="https://betterwyl.github.io/tags/Course/"/>
    
  </entry>
  
  <entry>
    <title>Radar</title>
    <link href="https://betterwyl.github.io/2022/12/02/radar/"/>
    <id>https://betterwyl.github.io/2022/12/02/radar/</id>
    <published>2022-12-02T05:45:43.000Z</published>
    <updated>2022-12-02T09:35:44.404Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="2d0dda2b33d6168e9e2f26d4e504bba8f152d2985289768dcd4cc960a6836abf">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:please wechat me】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="学习记录【Radar】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90Radar%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>coding指南</title>
    <link href="https://betterwyl.github.io/2022/11/27/code/"/>
    <id>https://betterwyl.github.io/2022/11/27/code/</id>
    <published>2022-11-27T03:43:22.000Z</published>
    <updated>2022-12-02T09:33:30.222Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="031d46d0238bbc530e53b12a6439af625252488041025fb8687d17ee19bd19e0">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:please wechat me】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="code已复现备注" scheme="https://betterwyl.github.io/tags/code%E5%B7%B2%E5%A4%8D%E7%8E%B0%E5%A4%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>Inductive link prediction in knowledge graph——A survey</title>
    <link href="https://betterwyl.github.io/2022/11/20/survey/"/>
    <id>https://betterwyl.github.io/2022/11/20/survey/</id>
    <published>2022-11-20T14:21:39.000Z</published>
    <updated>2022-11-20T15:12:09.683Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="This-is-the-official-repository-of-Inductive-link-prediction-in-knowledge-graph-A-Survey-a-comprehensive-survey-with-regard-to-inductive-link-prediction-the-context-of-knowledge-graph"><a href="#This-is-the-official-repository-of-Inductive-link-prediction-in-knowledge-graph-A-Survey-a-comprehensive-survey-with-regard-to-inductive-link-prediction-the-context-of-knowledge-graph" class="headerlink" title="This is the official repository of Inductive link prediction in knowledge graph: A Survey , a comprehensive survey with regard to inductive link prediction the context of knowledge graph."></a>This is the official repository of Inductive link prediction in knowledge graph: A Survey , a comprehensive survey with regard to inductive link prediction the context of knowledge graph.</h2><h1 id="Please-feel-free-to-CONTACT-us-or-COMMENT-below-the-article"><a href="#Please-feel-free-to-CONTACT-us-or-COMMENT-below-the-article" class="headerlink" title="Please feel free to CONTACT us or COMMENT below the article."></a>Please feel free to CONTACT us or COMMENT below the article.</h1><h2 id="Authors-Hongyu-Sun-Jian-Luo-Pengcheng-Li-Yue-Shen-Yulin-Wang"><a href="#Authors-Hongyu-Sun-Jian-Luo-Pengcheng-Li-Yue-Shen-Yulin-Wang" class="headerlink" title="Authors: Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang"></a>Authors: Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang</h2><ul><li>H. Sun, J. Luo, Y. Shen and Y. Wang are with the Department of Electronic Engineering and Information Science, University of Science and Technology of China (USTC), Hefei, China.<br>E-mail: <a href="mailto:sunhongyu@mail.ustc.edu.cn">sunhongyu@mail.ustc.edu.cn</a>; <a href="mailto:jianluo@mail.ustc.edu.cn">jianluo@mail.ustc.edu.cn</a>; <a href="mailto:yueshen@mail.ustc.edu.cn">yueshen@mail.ustc.edu.cn</a>; <a href="mailto:yulinwang@mail.ustc.edu.cn">yulinwang@mail.ustc.edu.cn</a>.</li><li>P. Li is with the Institute of Advanced Techonology, USTC, Hefei, China.<br>E-mail: <a href="mailto:pechola@mail.ustc.edu.cn">pechola@mail.ustc.edu.cn</a>.</li></ul><h2 id="If-you-find-this-repository-useful-in-your-research-please-consider-citing-our-work"><a href="#If-you-find-this-repository-useful-in-your-research-please-consider-citing-our-work" class="headerlink" title="If you find this repository useful in your research, please consider citing our work:"></a>If you find this repository useful in your research, please consider citing our work:</h2><p>@inproceedings{2022_excellent_survey,<br>title={Inductive link prediction in knowledge graph: A survey},<br>booktitle = {ustc_greatest_survey},<br>volume = {1958},<br>pages = {666},<br>year = {2022},<br>publisher={ustc_greatest_team},<br>author = {Hongyu Sun, Jian Luo, Pengcheng Li,Yue Shen,Yulin Wang},<br>}</p><div class="row">    <embed src="mydocument.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Workshop</title>
    <link href="https://betterwyl.github.io/2022/11/12/vnietest/"/>
    <id>https://betterwyl.github.io/2022/11/12/vnietest/</id>
    <published>2022-11-12T05:45:43.000Z</published>
    <updated>2022-12-02T09:33:08.245Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="057d4399f8a546af0cc717afb7a958d1695b784a7d92e50334f6c1e56b055aa0">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Enter the password to view the article.【hint:please wechat me】</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">This is an encrypted article. If you want to read it, please contact me.</summary>
    
    
    
    
    <category term="Workshop （小群讨论分享版）" scheme="https://betterwyl.github.io/tags/Workshop-%EF%BC%88%E5%B0%8F%E7%BE%A4%E8%AE%A8%E8%AE%BA%E5%88%86%E4%BA%AB%E7%89%88%EF%BC%89/"/>
    
  </entry>
  
  <entry>
    <title>3D检测</title>
    <link href="https://betterwyl.github.io/2022/11/11/3D%E6%A3%80%E6%B5%8B/"/>
    <id>https://betterwyl.github.io/2022/11/11/3D%E6%A3%80%E6%B5%8B/</id>
    <published>2022-11-11T14:20:15.000Z</published>
    <updated>2022-11-20T14:55:23.987Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>形状学习策略与不利天气、遮挡和截断等导致的点云质量恶化相结合</p><h2 id="Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】"><a href="#Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】" class="headerlink" title="Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】"></a>Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】</h2><p>数据集：KITTI<br>分析形状缺失的情况<br>外部遮挡；信号缺失【这边视为能观察到的，只是丢了】，表现为空相素【我的理解是过于稀疏】；自遮挡，远侧部分被近侧遮挡【每个物体都不可避免地会发生自遮挡导致的形状错失】；<br><strong>创新点：</strong><br>BtcDet是第一个针对受遮挡影响的对象形状的3D对象检测器<br>Complete object shapes= observed objects shapes ∪ the occluded object shapes<br>根据估计的占用概率P（OS）进行目标检测<br>形状占用网络<br>assemble the approximated complete shapes<br>假设：<br>大多数前景对象类似于数量有限的形状原型；【处理：利用source point来补全target】<br>H(A,B) →源B是否覆盖目标A的大部分区域 最终A形状=源B中得分最高的Top3+目标A原始的点云。源对象来自训练集的其他帧。<br>车辆和骑自行车者，大致对称【处理：借助标记的边界框，镜像补充它们】<br>识别observed objects shapes ∪ the occluded object shapes【是在球面坐标系中完成】<br>使用均匀间隔的球形网格对点云进行体素化，以便可以通过任何LiDAR点后面的球形体素精确地形成遮挡区域。【就是这个S可能会存在很多voxel中】非空体素若有包含形状S则为1，会形成一个occupancy 的概率。<br>用了一个占有率网络<br>Occlusion-Aware Proposal Refinement【plus】<br>将占有率投影到RPN的各层feature中→feature拼接→ROI pooling→优化的bbox</p><p>PIXOR ing</p><p>————————————————2022-6-26更新——————————————————<br>MOT论文</p><h2 id="MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries"><a href="#MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries" class="headerlink" title="MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries"></a>MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries</h2><p>代码：<a href="https://github.com/a1600012888/MUTR3D" target="_blank" rel="noopener">https://github.com/a1600012888/MUTR3D</a>  nuScenes数据集<br>端到端的多相机3D跟踪框架【解决问题：多相机进行3D跟踪时，会出现检测精度降低、复杂场景中的遮挡和模糊、边界对象丢失】<br>创新点：模拟一个对象的整个轨迹的3D状态，关联【空间和外观相似性】对象到3D轨迹中。<br>Metric：评估当前3D跟踪器中的运动模型：平均跟踪速度误差(ATVE)和跟踪速度误差(TVE)。可以测量被跟踪物体的估计运动的误差</p><p>自回归的方式逐帧更新自身→解码器头从每帧中的每条轨迹查询中预测一个候选对象，并且在来自同一轨迹查询的不同帧中解码的预测被直接关联→<br>损失：新查询和旧查询<br>新真值目标作为查询的回归目标，在新生查询的候选目标之间执行匹配。旧查询：先前帧的活跃查询。、跟踪当前帧中之前出现的目标，它在第一次成功检测到真值目标后被分配。<br>查询是有生命周期的，在代码中设置阈值。【查询更新：使用来自历史帧的特性来更新跟踪查询。】</p><p>两个指标：TVE是在MOTA最高的召回时的平均速度误差【可作为当前3D 跟踪器中运动模型的质量评价】</p><h2 id="Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving"><a href="#Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving" class="headerlink" title="Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving"></a>Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving</h2><p>数据集：nuScenes 3D<br>创新点：3D单目Detection和3D MOT一体；异构线索整合【编码外观 几何特征】<br>信息跨帧传播、估计相似度以生成三维轨迹、整合世界坐标系中的几何相对关系以估计速度、属性和框平滑度优化。<br>1单目检测方法：KM3D+其他检测头平行的Re-ID头<br>2异构线索整合：对外观、几何和运动信息的兼容表示进行了编码<br>3时空信息流：？？？？<br>空间信息流：3D探测器的主中心头提取图像中的中心点，+外观特征和几何特征， MLP层连接以生成其输入。<br>时间信息流模块→多头交叉注意力？<br>Loss：<br>单目3D检测损失LMono3D、跟踪损失Ltracking和时间一致性损失LCons</p><h2 id="MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection"><a href="#MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection"></a>MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</h2><p>创新点：去除中心限制，基于深度信息引导的三维物体检测方法<br><a href="https://github.com/ZrrSkywalker/MonoDETR" target="_blank" rel="noopener">https://github.com/ZrrSkywalker/MonoDETR</a><br>数据集：KITTI3D</p><p>特征提取：<br>视觉：多尺度特征图 下采样8 16 32<br>深度：下采样16。两个 3×3 卷积。对象的同一 2D 框内的像素分配有对象的相同深度标签。对于同时在多个框内的像素，选择离相机最近的物体的深度标签。<br>视觉编码器3块，深度编码器1块。<br>每个编码器块由一个自注意力层和一个前馈神经网络 (FFN) 组成</p><p>前景深度图方法：参考Categorical depth distribution network for monocular 3d object detection. 深度离散化为 k + 1 个 bin，其中第一个 bin 表示前景深度，最后一个表示背景。线性递增离散化（LID）【更远物体的深度估计固有地会产生更大的误差。[dmin, dmax]】</p><p>Depth-guided Decoder【不需要引入额外的深度估计的监督，3D GT boxes的 Z 值→深度值。估计深度属于某一范围得概率，然后根据其属于某一范围得索引返回连续的深度值。】<br>检测头：<br>在深度引导变换器之后，深度感知对象→基于 MLP 的头中以进行 3D 属性估计。在推理过程中，输出3D 边界框。分别计算每个查询的损失，将无序查询与真实对象标签匹配。<br>损失：<br>六个属性损失：两组。L2D 和 L3D。<br>第一组外观：对象类别、2D 大小和投影的 3D 中心。<br>第二组3D 空间属性：由深度、3D 大小和方向组成。【。】【在训练开始时，网络通常预测的 3D 属性不如 2D 属性准确，L3D 的值不稳定会干扰匹配过程。只使用 L2D 作为匹配每个查询标签对的匹配成本。】</p><p>————————————————2022-3-23更新——————————————————</p><h2 id="SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation"><a href="#SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation" class="headerlink" title="SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation"></a>SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</h2><p>核心内容：单个关键点估计与回归3 d 变量相结合，预测每个被检测物体的3d包围盒<br>在图像平面上估计投影的三维点。并行地增加一个三维参数回归分支。</p><p>损失函数： 3 d 盒子的8个角点【包含一个分类和回归分支。】分离了每个参数在三维包围盒编码阶段和回归损失函数中的贡献</p><p>当前方法：会较依赖于（rcnn 、rpn）+一个生成伪点云深度<br>缺点是：多阶段具有复杂性、引入噪声<br>改进方向有：几何推理、数据合成、3d-2d后处理</p><p>骨干网：DLA-34| 特点：实现深层聚合<br>采用这个网络的改进： bn替换为gn<br>3D检测网络：<br>关键点分支:物体由关键点表示【投影三维中心，图像中的8个点左乘内参矩阵】<br>回归分支:待回归的参数全部编码成差的方式去学习</p><p>损失函数<br>分类损失cls_loss：采用带惩罚因子的Focal Loss</p><p>———————————————2021-11-14更新————————————————————</p><h2 id="GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving"><a href="#GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving" class="headerlink" title="GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving"></a>GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</h2><p>作者：香港中文大学；商汤科技 王晓刚团队  CVPR2019<br>数据集：kitti<br>解决的问题是：<br>1仅使用二维边界框进行特征提取时，由于信息缺失，会出现表示模糊的问题。<br>【一个二维框有多种三维盒子表示。矛盾在于：相同特征输入又要分类器给出不同置信度；残差损失难估计】2 改进回归残差的loss 残差分类<br>创新点：<br>    在2D检测基础上，高效地获得3D长方体的方法【包含了大小方向的粗信息，该长方体即 “guidance”】<br>    提取3D盒子可见表面的潜在的3D信息，解决仅使用二维盒子的特征时的特征模糊问题。<br>    改进方法：考虑质量感知损失的离散分类方法比直接回归方法具有更好的精度。<br>流程：<br>image→CNN检测器→2D box &amp; orientation【2D框检测和方向预测】→3D guidance→guidance投影【2D框和可见表面提取特征】→3Dsubnet→refined 3D box【细化】<br>2D+O<br>3D box尺寸估计：汽车的尺寸具有低方差和单峰的特性。可以人为设定尺寸，初始化检测出3D box尺寸。<br>物体3D盒的顶部中心在2D平面上有一个稳定的投影，非常接近2D边界盒的顶部中点，而3D底部中心有一个类似的稳定投影，位于2D边界盒的上方和附近。<br>3d box 在相机坐标系下的粗计算：</p><p>高h估算已知，归一化后已知。因此d深度已知。<br>【d相当于一个比例系数现在，可以算出真实的三维空间坐标 3dbox 中心=0.5×（Cb+Ct）】</p><p>【这些以后可以得到一个粗位置。】<br>表面特征提取：<br>对α的角度进行讨论。前后左右会改变。上面必看到。<br>对可见的三个面做仿射变换，在指定特征层提取三个面特征加上2d box提取特征融合，最终特征回归真实的3d box。<br>残差回归改进为分类公式，细化3d box。【将残差值划分区间，计算标准差，以标准差为刻度，作为一个区间划分标志。】区间问题看成多重的二元分类问题。【意思是：如果有一个2Dbox 他不能和gt匹配→证明说他在区间概率为0。置信度低，那就是一个背景】<br>移位特征部分：【每个残差区间用最相关的投影特征？？？】<br>质量感知损失<br>分类预测的置信度能反映对应类的目标框的质量，目标框越准确得分越高。<br> 重合高的得分高</p><p>实验部分：<br>该方法和DEEP3DBOX方法【学习记录V4】相比，后者的AP高。该方法在2D检测上性能没有调到最好，核心工作是在3D部分。和Mono3D【特征很多很复杂的那篇。陈晓智√学习记录V4】、3DOP【双目的】、DeepMANTA比较。<br>分析：<br>该方法不擅长处理图像边界上的对象(通常带有遮挡或截断)。1利用了表面特征中潜在的三维结构信息，消除了仅使用二维边界框所带来的表示模糊。2残差回归问题重新表述为分类。利用质量意识损失增强模型的识别能力。4没有任何额外的数据或标签进行训练。</p><h2 id="Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image"><a href="#Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image" class="headerlink" title="Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image"></a>Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</h2><p>作者：一些外国人 研究方向是目标检测的           CVPR2017<br>创新点：<br>    定位车辆部件，特征点→可以预测隐藏部分的位置，23D之间的匹配，恢复3D车辆信息<br>    由粗到细的多任务卷积神经网络。借鉴RPN 网络产生2Dbox 。output：六个任务共享特征向量：region proposal , detection , 2D box regression , part location , part visibility , 3D template prediction.<br>    对图中的3D框的车辆的其他信息打标签</p><p>3D形状和模板数据集<br>数据集很庞大：103个汽车模型，每一个汽车模型都有36个特征部件的三维坐标，还有一个1*3数组代表长宽高信息。<br>真实的输入图像的每一个车辆建模 【里面有超多信息的】<br>在2D 和3D中的表示【框和部件】。V是可见性【是否遮挡啥的】<br>核心方法：Deep MANTA网络部分</p><p>从粗糙到精细化的前向传播：【均基于第一个卷积层的feature maps输入】使用三个网络【第一个网络就是RPN；第二三个网络？有点不懂】→结果更精确【原因：1克服大的物体大小变化，提供更高的准确度；2保持高分辨率，用于检测难以检测的车辆；】<br>多任务预测：第三个网络：每一个bounding box，同时输出其对应的：S，V，T【怎么能一下子输出这么多东西，奇怪？？？】 NMS去除冗余边框。<br>网络推断部分：利用一个就是前面的Deep MANTA网络的输出+3D部件数据+3D模板数据【方法：从103中遍历一个最相似的+23D匹配】<br>损失部分：<br>LOSS：【需要最小化五个损失函数】三个子网络<br>net1：LRPN<br>net2 | net3：+L检测损失函数 【分类损失（是车还是背景）+边框回归损失】<br>net2 | net3：+L特征部件定位<br>net3：+Lvis【车辆部件可见性】<br>net3：+L 模板相似度<br>【可以看出net3 Loss多：越往后越精细的意思吧】<br>半自动打标签：需要一个3D CAD的车辆模型数据集。作者手动标记了每一个车辆模型相应的N = 36个特征部件点的位置，并连线得到车辆模型的特征轮廓。对于每一个真实车辆的3D边框，通过算法计算出跟车辆模型的3D边框相似性。<br>选中最高的进行映射【该车辆模型及相应的特征部件点和轮廓的位置都可以映射到原图像真实车辆的位置去。】<br>实验部分：<br>数据集：KITTI<br>CNN：GoogLenet和VGG16。【预先用ImageNet数据集训练得到初始参数。VGG16效果较好】<br>NET1：在RPN网络中作者使用了7种长宽比，10种倍数作为参数。【该特征图上每一个点都可以产生70个anchor box。】<br>对比：【指标：AOS和AP】<br>第一个是不使用特征改进，且和RPN一样使用最后一层低分辨率的特征图的模型（类似于原先的RPN）<br>第二个是不使用特征改进，但是使用第一次卷积后的特征图的模型<br>第三个是Deep MANTA<br>3D定位正确性：ALP 通过阈值判定法<br>3D模板预测，特征部件定位，可见性评估：也是设定一些阈值。</p><h2 id="3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020"><a href="#3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020" class="headerlink" title="3DSSD: Point-based 3D Single Stage Object Detector 2020"></a>3DSSD: Point-based 3D Single Stage Object Detector 2020</h2><p>文章主要内容：点云数据进行单阶段3D检测的模型，速度快。<br><a href="https://github.com/dvlab-research/3DSSD" target="_blank" rel="noopener">https://github.com/dvlab-research/3DSSD</a><br>这篇文章的代码写得太好了，很清楚。<br>前人研究：点云数据投影到图像中、体素表达 缺点是会丢失信息。<br>现在研究：直接用点云数据<br>分析了这个PointRCNN的问题【3D Object Proposal Generation and Detection from Point Cloud<br>这篇文章还没看，但很重要的样子。】第一阶段 获取proposal (SA&amp;FP)前景点 第二阶段 refinement。【要解决耗时长的问题，特别是第一阶段，去除 FP层和第二阶段】<br>F-FPS方法和D-FPS方法<br>SA层的下采样步骤中用到了D-FPS方法【这个方法结果基本覆盖了整个场景，缺点就是前景点少了。→F-FPS前景点多+范围大。有利于回归】 SA中会进行特征聚合。【点和点周围的聚合，背景过少，不利于分类】。<br>→两个方法融合</p><p>CG layer【是对SA模块的变形：只用F-FPS中的提取的点作为初始中心点，初始中心点移动到它们对应的实例中→候选点。】预测头之前加入额外层来提取特征。三个步骤：中心点选择、周围点提取和语义特征生成。点集D-FPS 和F-FPS 的点将它们的归一化位置和语义特征group起来作为输入→mlp 层提取特征。<br>Anchor-Free回归：【2d中的centernet 根据关键点】<br>每个候选点到对应实例的距离、实例大小以及角度<br>Anchor-based方法缺点：要生成的anchor太多啦。要设计很多大小和方向的框【2d中的SSD、frcnn都是】<br>centerness对齐策略：1、点是不是在目标里。2、点到六个面距离，一个公式。 →1×2<br>【点云的点都在目标表面，他们的中心标记将非常的小且相似，很难从其他点中得出好的预测】<br>损失函数：分类、回归和shift损失。</p><p>基于模板匹配 ：打分</p><h2 id="3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection"><a href="#3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection" class="headerlink" title="3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection"></a>3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection</h2><p>文章主要内容：输入Stereo图像对作为来估计深度，将图像平面中的像素坐标重新投影回3D空间来计算点云。该方法优势：召回率高，能给出准确率高的对象框。<br>提出一个生成候选的能量最小化函数。能量最小化函数由 对象大小、地面、深度信息特征【点云密度、到地面的距离、有无遮挡、free space可行驶的区域？】<br>3Dbbox中圈出高密度的点云区域。限制：点云不能垂直延伸到bbox外，且这个bbox附近点云高度要矮一些。</p><p>KITTI图像包含许多小对象、严重遮挡、高饱和区域和阴影。不适用之前的目标检测网络。<br>能量函数各个分量含义：<br>pcd：box内体素是不是有点云。【相当于一个比例，这边很奇怪 难道不是box内点云比例越大越好吗？也许可以这么理解，尽可能把他们框起来，如果box全部都有说明box小了】<br>fs：可行驶空间不能有box 如果一个体素是在box内 那么肯定不会在可行驶区域内【最小化盒子内部的可行驶区域】<br>Height Prior：box内点云高度越接近这个对象的平均高度。要这个体素内有点云才参与计算。<br>Height Contrast：表示包围box附近的点云的高度应低于box内点云的高度。【定义附近的概念：+0.6m】<br>根据这个能量最小化生成2000个框 +nms+贪婪算法 得到最终box<br>地面估计：RANSAC【迭代的方式从一组包含离群的被观测数据中估算出数学模型的参数】将平面拟合到估计的地面像素来估计地面。<br>通过SVM来学习权重【这边有点看不明白】，使用IOU作为损失函数。<br>目标检测和方向估计网络<br>基于Fast R-CNN在最后一个卷积层之后添加一个上下文分支和一个方向回归损失来扩展这个基本网络，以共同学习对象的位置和方向。<br>平滑L1损失进行方向回归。</p><h2 id="Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving"><a href="#Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="Stereo R-CNN based 3D Object Detection for Autonomous Driving"></a>Stereo R-CNN based 3D Object Detection for Autonomous Driving</h2><p>文章主要内容：检测关联左右图像中的对象，基于关键点【mask r cnn】和box约束的3Dbox估计+密集3D盒子法(让他更精确)。<br>基于Faster R-CNN的工作。在本文中通过评估多比例特征地图上的锚来修改金字塔特征的原始RPN【Similar with FPN 意思是这边采用多个feature map】。<br>问题：RPN和FPN对比。<br>之前知道Faster R-CNN 是由Fast R-CNN+RPN构成的。一张图通过RPN获得一堆proposal，【proposal都是在原图上画个框，映射到一个feature map，池化变成统一尺度，之后再做分类和回归。】<br>在FPN中，对多个feature map分别做分类和回归，获得到一堆proposal【feature map这次有多张图】 </p><p>双目的RPN过程<br>左右特征图concat起来。目标是左右的并集，和anchor IOU大于0.7 IOU小于0.3定义正负标签。<br>FPN在Faster R-CNN中FPN产生的每一个尺度的feature map都要送进RPN做一次proposal的提取。<br>六个【我觉得论文这边写错了 这样好奇怪哩 应该是两个uv坐标offset + wh回看之前的是一个点坐标+wh 一共是四个 】<br>左右候选框都是由同一个 anchor 生成，共享类别置信度得分，它们就可以一一对应起来。我们在左右 RoI上分别使用NMS，选取最高的2000个候选框用于训练。测试时，选取300个候选框。</p><p>在RPN之后→RoiAlign的操作→ 获取FPN的左和右featuremap →concat相应的特征→fc层得到对象类别、立体边界框、维度和角度【使用θ表示车辆相对于摄像机框架的方向，使用β表示物体相对于摄像机中心的方位 sin/cos值】<br>keypoint的检测。Mask R-CNN的结构进行关键点的预测。4个3D keypoint，即车辆底部的3D corner point，同时将这4个点投影到图像，得到4个keypoint 起到一定约束作用。<br>3D框恢复。是从2D的框和关键点来恢复的。7个点 left左上右下 right左右 +keypoint：推导<br>Dense 3D Box Alignment：最小化左右视图refine？【这个地方有点不理解，先跳过】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【3D】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%903D%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>range-view</title>
    <link href="https://betterwyl.github.io/2022/07/29/range%20view/"/>
    <id>https://betterwyl.github.io/2022/07/29/range%20view/</id>
    <published>2022-07-29T10:17:47.000Z</published>
    <updated>2022-11-20T14:59:43.036Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>源于看了一篇基于rangeview的mot论文 觉得range的方法可操作性强且耕耘不多。</p><p>————————————————2022-10-2更新——————————————————</p><h2 id="Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation"><a href="#Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation" class="headerlink" title="Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation"></a>Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation</h2><p>创新点：<br>RV-PV-BEV；<br>dilated convolution 2d【residual block】；<br>two-stage RCNN【解决bev视图没有高度的问题】<br>Input：range image 【编码距离、坐标、强度→输入距离图像5 × h × w】<br>处理：Kitti3D相机前视图中的目标【90°场景 5 × 48 × 512】；waymo：64×2650<br>扩张残余块（DRB）<br>问题产生：尺度变化感知不清【使用range image送入到2Dcnn】。<br>将扩张卷积插入正常残差块：【代码中有三个不同的扩张率。最后用1×1融合】</p><p>RV-PV-BEV<br>问题是：Range image会重叠，因此特征提取要在BEV中进行。适应不同尺度。<br>流程是：Range image 上记录特征点对应投影到BEV平面，相同的点使用平均池化。<br>【如果在开始时投影，BEV将作为主要特征提取器。】</p><p>3D RoI Pooling<br>问题是：range or bev都无法明确地学习3D边界框高度方向上的特征。<br>解决方案是：使用相对位置进行高度上的编码。<br>【固定数量、不同的栅格包含目标的不同部分。由栅格的空间关系，信息可通过相对位置编码。】此处有个将三维全部展平为一维然后连接的处理。<br>损失函数：</p><p>Kitti数据集处理</p><p>Waymo：</p><p>固定栅格：12×12×12  128 proposals with a 1:1 ratio<br>优势： [30，50]m和[50，75]m  运行速度为22 fps<br>我的想法：range view的方法照道理快一些【pvrcnn 12fps】，猜测是没有分割出前景点会不会在BEV部分计算量大【关注：前景点分割方法→影响速度】。相对于rangedet其结构还是比较复杂 。</p><h2 id="RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection"><a href="#RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection" class="headerlink" title="RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection"></a>RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection</h2><p>ICCV2021 from 中科院自动化所&amp;图森<br>代码：<a href="https://github.com/TuSimple/RangeDet" target="_blank" rel="noopener">https://github.com/TuSimple/RangeDet</a><br>和BEV视图的区别，信息稠密的更有效利用【体现在NMS的改进】，近大远小会有尺寸变化【体现在MKC和改进型FPN】<br>创新点：<br>1Meta-Kernel Convolution<br>2 Range Conditioned Pyramid Assignment<br>3 Weighted NMS<br>Meta-Kernel Convolution【解决使用卷积会把密集信息忽略的问题，比如在一个rangeimage中，两个点靠近，而他们实际的距离可能很远，那就丢掉了这部分信息】</p><p>Range Conditioned Pyramid Assignment<br>【使用ResNet中的BasicBlock将其卷积核替换了一下此处有点不理解，结合blog】Meta-Kernel Convolution：将卷积核的权重变得可调整。使用中心点与邻域点的差值，放大了检测了属于点和点之间的特征差异。】<br>依据：距离范围的远近：近距离label 局部特征图；远距离的label分配全局的特征图。【思想FPN金字塔】<br>Weighted NMS<br>每个pixel都会预测一个box，一个truth可以被很多pixel预测。那不能全部删掉，而是采用较高score的框进行加权。<br>作者使<br>我的思考：该论文具体问题具体分析提供改进的方法。挺巧妙的。【3D→2D的这样逆过程可以使用一些2D的方法，从而来实现一些2D中已经实现任务，在分割、跟踪上？】</p><h2 id="【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021"><a href="#【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021" class="headerlink" title="【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021"></a>【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021</h2><p>代码：<a href="https://github.com/PRBonn/LiDAR-MOS" target="_blank" rel="noopener">https://github.com/PRBonn/LiDAR-MOS</a><br>数据集：SemanticKITTI<br>创新点：将rangeview用于mos任务；使用了时域上的信息，即残差；<br>Input：<br>3D LiDAR 扫描生成的range image+残差图像【当前帧和先前帧之间的距离的残差→d 是关于r的】<br>r将第k帧的点云旋转至当前帧l第i个像素上的距离值<br>d 归一化表示</p><p>Output：当前帧中的一个标签范围 【红的表示移动物体】<br>流程：3D点云序列的投影图+残差图像<br>1投影公式<br>2使用到SLAM中获得过去时间序列的雷达信息→残差计算【T代表着每个序列的相对变换 传感器得到？有点不理解来源 数据集里的吗】</p><p>结合SLAM读数【其实这边有点不懂，SLAM知识缺失】和残差图像→现有的分割网络通过利用残差图像中的时间信息来区分运动物体和背景上的像素。二进制表示<br>如何将上述两步信息融合 【就是整套需要变换和重新投影流程】<br>1之前的扫描序列转化为当前的2重新投影到当前范围视图3计算距离</p><p>CNN结构【现成的】<br>使用三个网络比对RangeNet++ MINet SalsaNext<br>指标IOU：移动物体<br>实验部分：<br>在slam上添加噪声测试其稳定性<br>【自己生产自己比较，当前较少该方向的成果，作者结合了之前方法重新做实验比较】<br>semantic segmentation  SalsaNext【这个不太理解，加了一堆东西】<br>scene flow 对平移向量设置阈值判断是否移动<br>我的思考：该论文引入与之前帧的残差作为一个特征，若物体移动较慢很可能识别为静物。能不能设置区间或者其他方法放大时间上的信息，【改进点：针对速度小的物体分割效果提升】。一个问题：相对本车静止，他实际上也是运动的。【回答：在实际场景 趋势显露】<br>LiDAR 的 MOS 的实现并不多 ？实验比较的依据可靠性</p><h2 id="Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet"><a href="#Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet" class="headerlink" title="Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet"></a>Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet</h2><p>代码：<a href="https://github.com/haomo-ai/motionseg3d" target="_blank" rel="noopener">https://github.com/haomo-ai/motionseg3d</a><br>数据集： KITTI+自己标注<br>创新点：<br>1双分支结构，【两部分变成并联结构，使用SalsaNext，在此基础上add】。<br>2解决range-view信息没有有效利用问题：加了Meta-Kernel Module<br>通过中心点的相对坐标计算 3×3 邻域的权重，然后使用 1×1Conv 聚合邻域特征来更新中心特征。<br>这么做的目的是：细化结果，并减少对象边界周围出现的伪影。<br>其他tricks：加注意力机制；减少resblock<br>由一个用于编码外观特征的距离图像分支和一个用于编码时间运动的残差图像分支<br>网路LOSS：每个类别频率交叉熵和 【The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-<br>union measure in neural networks,” in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2018】pixel和point都可以用的<br>我的思考：顾名思义，双分支。这篇文章对LMNet的改进可以理解为串联改并联了。同时，他用了一个没见过的损失函数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【3D】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%903D%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>YOLO系列</title>
    <link href="https://betterwyl.github.io/2021/12/25/yolo%E7%B3%BB%E5%88%97/"/>
    <id>https://betterwyl.github.io/2021/12/25/yolo%E7%B3%BB%E5%88%97/</id>
    <published>2021-12-25T14:56:18.000Z</published>
    <updated>2022-11-20T14:54:33.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>———————————————————2021-12-25更新————————————————</p><h2 id="YOLOv3-An-Incremental-Improvement-技术报告"><a href="#YOLOv3-An-Incremental-Improvement-技术报告" class="headerlink" title="YOLOv3: An Incremental Improvement 技术报告"></a>YOLOv3: An Incremental Improvement 技术报告</h2><p>    backbone Darknet-19→Darknet-53 精度速度权衡<br>【该全卷积网络是没有全连接层的 也是其可以兼容任意尺度图像的原因→FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数】<br>兼容任意尺度图像→输入要是32倍数<br>怎么变成416？强行缩放、加灰边</p><p>小目标性能提高<br>多尺度融合 ：不同尺度anchor 每个尺度3个anchor<br>255是85（coco80类+xywhc+边框置信度）×3（anchor）<br>【下采样32倍：13×13×255每一个代表原图32。】<br>【FPN特征金字塔】<br>concat：拼接 【上采样2倍完26×26+原先下采样26×26的拼接。厚度是不一样的。】<br>损失函数<br>正样本：与gt 的IOU最大的那个anchor 所在尺度的grid cell去负责【与之前区别 不看中心点落在哪个gridcell里面】<br>注意的是：如果高于某个阈值的话那就不是正样本而是不参与，小于某个阈值的话那就是负样本。<br>正样本的坐标【遍历所有cell 还有anchor】<br>+正样本的置信度和类别（标签为1）使用二元交叉熵损失函数BCE<br>+负样本置信度（标签为0）<br>测试：1、计算预测框信息2、设置阈值，去掉得分低的3、多分类nms重叠大的消除</p><h2 id="YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><p>网络结构：<br>backbone Darknet-53→CSP Darknet-53<br>生成特征图后再合在一起作为下一个网络输入【厚度越来越大了】<br>特征提取网络改进 SPP和PANet<br>Bag of freebies<br>数据增强：1像素级别：处理光度失真【调整图像的亮度、对比度、色调、饱和度和噪声】几何失真【添加了随机缩放、裁剪、翻转和旋转】2假设遮挡：随机融合或者使用零像素值3style transfer GAN 风格迁移<br>不同类别之间存在数据不平衡：单阶段Focal loss双阶段难例挖掘<br>本论文使用：<br>Mosaic混合了四张训练图片<br>SAT：【不太理解。？网上：它在前后两阶段上进行操作。在第一阶段，神经网络代替原始的图片而非网络的权重。用这种方式，神经网络自己进行对抗训练，代替原始的图片去创建图片中此处没有期望物体的描述。在第二阶段，神经网络使用常规的方法进行训练，在修改之后的图片上进检测物体。】<br>网络正则化的方法：Dropout、Dropblock<br>CIOU：将覆盖区域，中心点距离和纵横比考虑在内。<br>Bag of specials 需要增加推理的过程<br>增大感受野 SPP【本文使用】 ASPP RFB<br>注意力机制 SE SAM【将空间注意改成点注意】<br>特征集成方法ASFF BiFPN PANET【本文使用 是一种拼接 不是相加!!】<br>激活函数【代码可以直接调用库】MISH ReLU leaky-ReLU parameter-ReLU ReLU6 SELU<br>后处理方法：soft NMS【物体的遮挡】、DIoU NMS【添加中心距离的信息】<br>通过均值和方差对网络激活进行归一化【本论文使用CmBN】<br>理解：这篇文章讨论和验证了目标检测中每个部分的多种策略，对这些策略进行选择和部分改进，然后组合在一起，基于yolov3去改动，比较各模块选择什么方法能让目标检测性能最优化。有注重对各模块在单GPU上的性能比较。</p><h2 id="Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）"><a href="#Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）" class="headerlink" title="Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）"></a>Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）</h2><p>abstract关键词：类别不平衡<br>自动选择这些困难的样例可以使训练更加有效和高效<br>【意思是之前的region proposals生成正负比例不平衡。假如说10个里面9个负样本，那直接判别负样本正确率非常高。】<br>VOC2007和2012中的mAP分别为：78.9%，76.3%<br>一个概念：启发式超参数搜索 【使用循环神经网络来生成参数→在训练过程中不变的数 当然会增加时间复杂度】<br>原因：<br>Fast R-CNN允许更新整个卷积网络 SPP net、MR-CNN固定住卷积网络、也没使用SVM。<br>train部分：<br>在Fast R-CNN SPP net  MR-CNN把RoI与真实框的交叉比(IOU)大于等于0.5即判定为目标RoI。本文去掉小于等于的设置背景的。<br>Fast R-CNN在一个 mini-batch中，它们之间的比例是1：3。本文中是去掉了这个比例。<br>没有采用设定背景和目标样本数的比例方式处理数据的类别不平衡【如果哪个类别不平衡，那么这个类别的损失就会比较大，这样被采样的可能性也比较大】<br>方法：<br>    之前使用的方法：<br>只选出那些 hard negative 的样本进行训练→Hard Negative Mining Method<br>SVM + Hard Negative Mining Method<br>基于SVM及检测器训练时需要分类器对样本进行分类，把其中错误分类的样本放入负样本集合再继续训练分类器直到模型收敛。<br>困难样本挖掘。【会比都是简单的效果好。】<br>缺陷：端到端难以操作；需要迭代训练的时候又另外腾出时间来生成这种hard negative，每迭代几次就固定模型一次，速度慢。<br>一般用svm才会用这个方法。但是fastrcnn和fasterrcnn都没使用。→想另外的方法<br>    本文：<br>SGD回传 对样本进行一个重新选择【选择困难样本或者对困难样本赋予更高的权重】<br>SGD是以mini-batch为单位来更新模型的：对于每个mini-batch，先从数据集中取N张，然后每张图片采样B/N个RoIs<br>重合率比较大的ROI之间的损失也比较相似→解决办法：使用了 NMS(非最大值抑制) 算法：把损失按高到低排序→选择最高的损失→计算其他 ROI与这个 ROI的 IoU→移除 IoU 大于一定阈值的 ROI，然后反复上述流程直到选择了 B/N 个 ROIs。<br>    提出一个是修改loss层：【缺点：将没选择的ROI的loss设置为0。但是这种做法并不高效，因为即便很多ROI的loss都是0，也就是不需要更新梯度，但是这样仍需要给每个ROI都分配存储空间，并且每个ROI都需要后向传播】</p><p>    两个相同的 ROI网络，一个只可读【前向传递的时候分配空间】，另一个可读可修改【同时为前向和后向分配空间】。<br>经过ROI plooling层生成feature map，然后进入只读的ROI network得到所有ROI的loss；然后是hard ROI sampler结构根据损失排序选出hard example，并把这些hard example作为第二个ROI network的输入。</p><h2 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）"></a>You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）</h2><p>abstract关键词：<br>视作回归问题。<br>一个单一的神经网络预测bbox和类概率。<br>由于整个检测pipeline是一个单一的网络，可以直接对检测性能进行端到端的优化。<br>快！每秒45帧。实时 是之前其他实时物体检测系统mAP的两倍以上<br>避免背景错误，产生false positives。<br>【对比之前的：是通过region proposal产生大量的可能包含待检测物体的bounding box，再用分类器去判断每个 bounding box里是否包含有物体，以及物体所属类别的概率。分开处理较难优化】<br>核心思想：<br>视为回归问题。利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别<br>流程：<br>input：resize图像到448 * 448 →网络→NMS<br>Unified Detection<br>栅格 各管各的<br>image→S*S的栅格 每个栅格负责检测中心落在该栅格中的物体<br>每一个栅格预测B个bounding boxes&amp;置信度得分<br>x y w h IOU+C【conditional class probability在一个栅格包含一个Object的前提下，它属于某个类的概率】<br>conditional class probability信息是针对每个栅格的。<br>confidence信息是针对每个bbox的。<br>【上述两者相乘：包含bounding box中预测的class的 probability信息，也反映了bounding box是否含有Object和bounding box坐标的准确度】<br>一个图 ：S×S×(B×5+C)<br>网络设计：<br>24个卷积层和2个全连接层<br>【卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。】</p><p>YOLO借鉴GoogLeNet分类网络结构。没使用inception module。使用 1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>训练：<br>Pretrain网络：上述网络中的前20 个卷积层+average-pooling layer+全连接层【ImageNet 1000-class的分类任务数据集】<br>Pretrain的结果的前20层卷积层应用到检测中，+剩下的4个卷积层及2个全连接。<br>损失函数：<br>如果一些栅格中没有物体，那么就会将这些栅格中的bounding box的confidence 置为0，相比于较少的有物体的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。</p><p>坐标预测：xywh<br>含有物体的bbox IOU预测 confidence<br>不含有物体的bbox IOU预测 confidence<br>类别预测：Class 有没有中心落在网格中 有就预测概率的意思。</p><p>看起来可以直接用7×7×30理解。30前20个代表的是预测的种类，后10代表两个预测框及其置信度(5x2) 7*7<br>每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box predictor来负责预测【当前哪一个predictor预测的bounding box与ground truth box的IOU最大，这个 predictor就负责物体检测→每个predictor可以专门的负责特定的物体检测→训练后预测效果更好】</p><p>局限性：<br>YOLO对小物体的检测效果不好【小物体，因为一个栅格只能预测2个物体。而且小物体IOU影响较大】<br>YOLO容易产生物体的定位误差</p><h2 id="YOLO9000-Better-Faster-Stronger-CVPR-2017"><a href="#YOLO9000-Better-Faster-Stronger-CVPR-2017" class="headerlink" title="YOLO9000: Better, Faster, Stronger (CVPR 2017)"></a>YOLO9000: Better, Faster, Stronger (CVPR 2017)</h2><p>abstract关键词：<br>保持原有速度，精度上提升<br>一种目标分类与检测的联合训练方法：YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测<br>改进措施：<br>    Batch Normalization：批处理规范化<br>每一个卷积层后添加batch normalization，通过这一方法，mAP获得了2%的提升。batch normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。<br>    High Resolution Classifier：<br>在ImageNet上对448×448上的分类网络进行了10次微调。我们在检测时对产生的网络进行微调。模型在检测数据集上finetune之前已经适用高分辨率输入。这种高分辨率的分类网络mAP增加了近4%。<br>    Convolutional With Anchor Boxes：<br>YOLOv1采用的是：全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体。【YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差】<br>YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。【RPN预测的是边界框相对于先验框的偏移】YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。每个位置的各个anchor box【注重形状】都单独预测一套分类概率值。【影响：mAP有稍微下降】</p><p>input： 416×416<br>特征图大小：13×13【维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。】<br>最初的YOLO输入尺寸为448×448，加入anchor boxes后，输入尺寸为416×416。模型只包含卷积层和pooling 层，因此可以随时改变输入尺寸。<br>作者在训练时，每隔几轮便改变模型输入尺寸，以使模型对不同尺寸图像具有鲁棒性。每个10batches，模型随机选择一种新的输入图像尺寸（320,352,…608，32的倍数，因为模型下采样因子为32），改变模型输入尺寸，继续训练。<br>Dimension Clusters<br>维度聚类【k-means聚类方法】。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标。5种大小的box维度来进行定位预测，这与手动精选的box维度不同。结果中扁长的框较少，而瘦高的框更多【就和尺度没关系了】125=5×25<br>Direct location prediction<br>直接目标框预测【不是预测偏移了 每个anchor 检测周围】</p><p>细粒度特征拥有较细粒度特征的层变形【？】<br>多尺度训练：训练Yolo v2时不固定image size，而是每训练10个epochs随机选取【32倍数】<br>网络：New Network: Darknet-19（特征提取器）<br>19个卷积层和5个maxpooling层 3×3 卷积层 【其中使用1×1卷积层】 2×2maxpooling<br>改进：方法wordtree</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN系列 （实习摸鱼中）</title>
    <link href="https://betterwyl.github.io/2021/11/17/R-CNN%E7%B3%BB%E5%88%97/"/>
    <id>https://betterwyl.github.io/2021/11/17/R-CNN%E7%B3%BB%E5%88%97/</id>
    <published>2021-11-17T03:56:22.000Z</published>
    <updated>2022-11-27T04:05:41.776Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来锐捷实习了，我每天的任务是写两个脚本，但奈何我。。。<br>又不想虚度光阴，认真看看好了。<br>这里伙食确实不错。</p><h1 id="R-CNN-系列"><a href="#R-CNN-系列" class="headerlink" title="R-CNN 系列"></a>R-CNN 系列</h1><h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014" class="headerlink" title="R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)"></a>R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)</h2><p><strong>背景</strong>：首次用深度学习CNN的方式进行目标检测的尝试。之前都是使用传统提取特征，比较好的是SIFT和HOG。【看代码复习】<br><strong>创新点</strong>：<br>使用候选区域，与滑动窗口方法相比，CNN处理的图像窗口减少了两个数量级。<br>CNN提取特征：RCNN使用的CNN网络是AlexNet。<br>有监督pre-training+domain-specific finetuning处理label少的情况<br>使用bounding box regression进行修正【？具体】<br>目标检测：<br><strong>生成候选区域</strong>：<br>选择性搜索【一张图像生成约2K个候选区域 （Selective Search）】目的是为了改善传统提取特征方法中机从左到右、从上到下枚举式的低效。<br>Selective Search：过分割，将图像分成小区域；合并可能性最高的相邻两个区域<br><a href="http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a><br>【相关论文 颜色、纹理相近；尺度要均匀：不能大鱼吃小鱼；形状】；输出所有候选区域。</p><p>基于上述方法搜出的候选框是矩形的，而且是大小各不相同。为了要得到固定尺寸的图片输入到CNN中，进行缩放。作者比较了两种方法：1 各向异性缩放：不管比例 直接缩到227*227 ；2 各项同性缩放：将边界拓展成正方形 然后不在框里面的直接用框外的颜色均值代替填充；用固定背景颜色填充<br>本文：采用各向异性缩放、padding=16的精度最高</p><p>如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5就把这个候选框标注成物体类别，否则就是背景。<br><strong>特征提取</strong>：<br>对每个候选区域，使用深度卷积网络提取特征 （CNN）<br>比较：VGG和AlexNet。前者精度高但是计算量是后者的7倍。<br>物体检测的一个难点：物体标签训练数据少，若直接采用随机初始化CNN参数的方法，训练数据量是不够——采用的是有监督的预训练。<br>AlexNet原本是做图像分类任务，为做目标检测任务，替换掉AlexNet的最后一层的全连接层（4096 * 1000）。<br>1pre-train：采用了迁移学习的思想： ImageNet训练的CNN，先进行网络图片分类训练。<em>该数据库有大量的标注数据，共包含了1000种类别物体，预训练阶段CNN模型的output 1000个神经元。<br>网络优化：采用随机梯度下降法，学习速率大小为0.001<br>2fine-tune：在小型目标数据集（PASAC VOC）对上面得到的model进行改动。将模型的最后一层修改类别数。（20+1background）<br>RCNN的结构实际是5个卷积层、2个全连接层。<br>input: 2000</em> 227 * 227 * 3<br>output:  2000 * 4096 * 1<br>从每个候选区域中提取4096维特征向量。特征是通过前向传播通过五个卷积层和两个全连接层减去平均的224X224 RGB图像来计算的<br><strong>线性SVM分类器</strong>：<br>训练过的对应类别的SVM给特征向量中的每个类进行打分，每个类别对应一个二分类SVM。output: 2000*N（N目标的类别）作者测试了IOU阈值各种方案数值，通过训练发现，IOU阈值为0.3效果最好。IoU&gt;0.3的region proposal的特征向量作为正例，其余作为负例。<br>减少bbox：非极大值抑制法<br>测试时：2000×4096维特征与N个SVM组成的权值矩阵4096×N相乘，每一列即每一类进行非极大值抑制剔除重叠建议框</p><p>【CNN做特征提取（提取fc7层数据），再把提取的特征用于训练svm分类器原因：CNN容易过拟合，需要大量训练数据，因此CNN训练数据做了比较宽松的标注，一个bounding box正样本可能只包含物体的一部分，用于训练CNN。SVM适用于少样本训练，对于训练样本数据的IOU要求比较严格，只有当bounding box把整个物体都包含进去了，才把它标注为物体类别】<br><strong>Bounding Box 回归</strong>：<br>就是得到的候选框可能与ground truth相差比较大。解决：利用回归的方法重新预测了一个新的矩形框，借此来进一步修正bounding box的大小和位置<br>边界框回归是利用平移变换和尺度变换来实现映射。<br>使用相对坐标差：【比例值是恒定不变的；对坐标偏移量除以宽高即做尺度归一化：尺寸较大的目标框的坐标偏移量较大，尺寸较小的目标框的坐标偏移量较小】<br>IoU大于0.6时，边界框回归可视为线性变换：【log(1+x)/x  x趋近0 整个趋近于1】<br>AlexNet第5个池化层得到的特征即将送入全连接层的输入特征的线型函数。<br><strong>存在问题</strong>：R-CNN需要两次进行跑CNN model，第一次得到classification的结果，第二次才能得到(nms+b-box regression)bounding-box<br>三个模块（CNN特征提取、SVM分类和边框修正）是分别训练的，并且在训练的时候，对于存储空间的消耗大。检测速度慢，47s/per image。</p><h2 id="Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明"><a href="#Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明" class="headerlink" title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明"></a>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明</h2><p><strong>核心贡献</strong>：在R-CNN的基础上提出了空间金字塔变换层，速度、精度提升。<br>【CNN网络后面接的FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数，故需要固定长度的输入限制网络的输入大小<br>CNN网络会有大量的重复计算，造成的计算冗余】<br>R-CNN：输入需要对候选区域做填充到固定大小【对候选区域做填充缩放操作，可能会让几何失真、有冗余信息，这都会造成识别精度损失】;每个候选区域都要塞到CNN内提取特征向量【一张图片有2000个候选区域，也就是一张图片需要经过2000次CNN的前向传播，这2000重复计算过程会有大量的计算冗余，耗费大量的时间。】<br><img src="https://upload-images.jianshu.io/upload_images/3940902-0db3d0e4fa819bf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/444/format/webp" alt="比较"><br>SPP-net：针对候选框的重复计算部分，候选区域到全图的特征映射之间的对应关系。【直接获取到候选区域的特征向量，不需要重复使用CNN提取特征】<br>使用空间金字塔变换层将接收任意大小的图像输入，输出固定长度的输出向量。<br>【相当于从中间截断了，那前面的CNN可以接受不同尺寸图像了。都是为了保证全连接层的一个固定】<br>空间金字塔变换层<br>以不同的大小的bin块来提取特征的过程。【成列向量与下一层全链接层相连。这样就消除了输入尺度不一致的影响。】<br>不同大小侯选区域在feature map上的映射塞给SPP层<br>SPP layer分成1x1(塔底)，2x2(塔中)，4x4（塔顶）三张子图，对每个子图的每个区域作max pooling。输出都是(16+4+1) 每个块提取出一个特征21维特征向量。然后×256。【其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出神经元】</p><h2 id="Fast-R-CNN-CVPR-2015"><a href="#Fast-R-CNN-CVPR-2015" class="headerlink" title="Fast R-CNN(CVPR 2015)"></a>Fast R-CNN(CVPR 2015)</h2><p><strong>创新点</strong>：<br>SSP→RoI池化层：避免对每个候选区域提取特征，避免大量重复计算。<br>将分类与定位两大任务融入一个网络中来，获得了比R-CNN快的训练测试速度。边框回归直接加入到CNN网络中训练，损失部分采用多任务损失：<br><strong>方法</strong>：<br>利用选择性搜索获取图像中的推荐区域→将原始图片利用VGG16网络进行提取特征，之后把图像尺寸、推荐区域位置信息和特区得到的特征图送入RoI池化层，进而获取每个推荐区域对应的特征图。<br>input：待处理的整张图像；候选区域<br>网络分成两个并行分支，一个对推荐区域进行分类，一个对推荐区域的位置信息做预测。<br><img src="https://img-blog.csdn.net/20180527160553808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dlbnRlbHlhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="FastRCNN"><br><strong>RoI池化层</strong>:<br>【对SSP改进部分，SSP有不同尺度的特征图。这边简化了一下下采样到统一尺度】RoI只采用单一尺度进行池化→VGG16 后产生一个7×7×512维度的特征向量作为全连接层的输入【每个RoI区域的卷积特征分成4×4个bin，然后对每个bin内采用max pooling，这样就得到一共16维的特征向量。】<br>RoI pooling解决了SPP无法进行权值更新的问题。【？解答：SPP是将所有的特征图上的RoI保存下来，然后选择进行的网络微调，不会更新SSP layer之前的层，就相当于和前面的断开了，前面的特征图都不共享。Fast就是从输入到选择RoI都是同一批图，这样就能效率高的反向传播了。】<br>两个好处：将图像中的RoI区域定位到卷积特征中的对应位置；将这个对应后的卷积特征区域通过池化操作固定到特定长度的特征，然后将该特征送入全连接层<br>采用SVD对全连接层分解:<br>一张图像约产生2000个RoI，近一半的前向传递时间都花在计算全连接层上。SVD对全连接层进行变换来提高运算速度。截断SVD可以减少30%以上的检测时间，mAP只下降很小(0.3个百分点)。<br>一个大的矩阵可以近似分解为三个小矩阵的乘积，分解后的矩阵的元素数目远小于原始矩阵的元素数目，从而达到减少计算量的目的。→对全连接层的权值矩阵进行SVD分解。<br><strong>多任务的损失</strong><br>Fast R-CNN直接使用Softmax替代SVM分类，利用Softmax Loss 和Smooth L1 Loss对分类概率和边框回归联合训练<br>【Fast R-CNN网络主要有2个网络分支，一个网络分支负责输出推荐区域的分类概率，另一个网络分支负责输出每个推荐区域位置信息偏移量。】</p><h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><p><strong>RCNN FastRCNN总结对比</strong><br>二者的ROI区域生成：单独模块 选择性搜索<br>RCNN把分类和坐标分成两个网络，fastRCNN加上特征提取融合在一起了（Deepnet）。<br><strong>创新点</strong>：<br>使用RPN生成检测框，提高检测框生成速度。区域生成网络(RPN)+Fast RCNN<br>产生建议窗口的CNN和目标检测的CNN共享<br>核心方法：<br>使用卷积层提取特征图：<br>conv x13 relux13 poolingx4<br>conv层：kernel_size=3，pad=1，stride=1<br>对卷积进行填充：【相当于保持尺度不变 假设输入MxN 进来后(M+2)x(N+2) 那么3x3输出后还是MxN】<br>pooling层：kernel_size=2，pad=0，stride=2<br>【假设输入MxN 进来后(M+2)x(N+2) 那么2x2步长2输出后(M/2)x(N/2)→四次pool就变成了1/16 特征图上面密集的点对应到原始图像上面有16个像素的间隔】 最后conv5输出通道数有256（针对ZF ：VGG16是512-d,ZF是256-d）<br>【800/16 x 600/16=50 x 38 特征一共是50 x 38 x 256】</p><p><strong>RPN网络生成检测框</strong>：【相当于目标定位，二分类】<br>input: (M/16)x(N/16) 先经过一次3x3卷积 output: 50 x 38 x 256<br>分成两条线：<br>softmax判断正负样例<br>bb回归修正→proposals精准化<br>每一个点都负责原图中对应位置的9种尺寸框的检测→50 x 38 x 9 个anchor<br>【anchors多尺度方法：9个尺度，三种形状→长宽比1:1 1:2 2:1】<br>【可理解为在原图尺度上，设置了许多候选Anchor。通过CNN判断标记有目标的positive anchor和没目标的negative anchor】<br><strong>正负样本怎么划分</strong>：<br>1    对每个标定的ground truth区域，与其重叠比例最大的anchor记为正样本。【一个gt对应一个正样本】<br>2    剩余的anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本【每个ground truth可能会对应多个正样本anchor。但每个正样本anchor 只可能对应一个grand truth 一对多关系】。<br>如果其与任意一个标定的重叠比例都小于0.3，记为负样本。<br>3    剩余的anchor、跨越图像边界的anchor丢弃<br>计算anchor box与ground truth之间的偏移量：ground truth box与预测的anchor box之间的差异<br><strong>损失</strong>：rpn_loss_cls【softmax】、rpn_loss_bbox【smooth L1】、rpn_cls_prob【用于下一层的nms非最大值抑制操作】</p><p>p表示anchor i预测为物体的概率<br>p×正样本=1负样本=0 【回归只有在正样本时候才会被使用！】<br>t表示正样本anchor到预测区域的4个平移缩放参数<br>t×正样本anchor到Ground Truth的4个平移缩放参数</p><p>生成anchors →softmax分类器提取positvie anchors →bbox回归positive anchors →Proposal Layer生成proposals</p><p><strong>Roi Pooling</strong>：【共享信息】<br>input：<br>特征图和proposals 提取proposal feature maps→后续全连接层判定目标类别<br>候选框的特征图水平和垂直分为7份，对每一份进行最大池化处理。49维送入全连接层。【即使大小不一样的候选区，输出大小都一样，实现了固定长度的输出】<br>分类：<br>利用已经获得的proposal feature maps，通过全连接层与softmax计算每个proposal具体属于的类别，输出cls_prob概率向量；再次利用bounding box regression获得每个proposal的位置偏移量bbox_预测，用于回归更加精确的目标检测框。</p><p><strong>训练部分</strong>：<br>1    使用ImageNet模型初始化，独立训练一个RPN网络。<br>2    使用上面RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络。【两个网络每一层的参数完全不共享】<br>3    使用上面的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的学习率设置为0，仅仅更新RPN特有的那些网络层，重新训练。两个网络已经共享了所有公共的卷积层<br>4    仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个network，继续训练，微调 Fast-RCNN特有的网络层实现网络内部预测proposal并实现检测的功能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
  <entry>
    <title>Anchor-free系列</title>
    <link href="https://betterwyl.github.io/2021/10/26/anchorfree/"/>
    <id>https://betterwyl.github.io/2021/10/26/anchorfree/</id>
    <published>2021-10-26T01:35:22.000Z</published>
    <updated>2022-11-20T14:58:55.156Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="CornerNet-2个角点：左上角和右下角角点"><a href="#CornerNet-2个角点：左上角和右下角角点" class="headerlink" title="CornerNet 2个角点：左上角和右下角角点"></a>CornerNet 2个角点：左上角和右下角角点</h2><p>input：image经过一个ConvNet→生成feature map<br>hourglass network: 捕获图片在多个尺度下的特征。1、降采样操作缩小输入的大小；2、上采样恢复到输入图像大小<br>常用：使用多个pipeline分别单独处理不同尺度下的信息，网络的后面部分再组合这些特征。<br>上面分支module负责预测左上角corner，下面分支module负责预测右下角corner<br>需要group操作。<br>heatmaps预测：Corners点概率；<br>中心点不一定局限在某一个位置上 而是一个区域→中心点落在半径r范围内设置一个iou→圆圈内的点的数值是以圆心往外呈二维的高斯分布；【对不同负样本点的损失函数采取不同权重值的原因：白色虚线是一个预测框，预测框的两个角点和ground truth并不重合，但是该预测框大概框住了目标，因此是有用的预测框，所以要有一定权重的损失返回】<br>loss：带有惩罚因子的Focal loss 降低接近真值损失 进一步解决样本不均衡<br>embeddings预测：解决的是配对问题：一个目标的两个角点，二者的embedding vector之间的距离应该很小。每个点所属的目标中心点；采用L1范数，距离大于0.5或者两个点来自不同类别的目标的都不能构成一对。<br>offsets：表示在取整计算时丢失的精度信息，输入图像到特征图之间会有尺寸缩小。anchor-based方法是算和anchor偏移。针对小尺寸目标的回归。这是针对角点的。</p><p>corner pooling：怎么知道这个点就是角点？特征点肯定是最大的。红色的部分。找到值最大的网格然后确保该网格左边的网格全部都能变成最大值，也就是把水平方向最明显的特征向左延续；竖直方向最明显的特征向上延续；这样当两幅heatmap相加时→两者最明显特征的路线相重叠，这样加出来的值肯定也是最大的，因此就能推测出左上角关键点的位置。</p><p>测试：<br>在得到预测角点后，会对这些角点做NMS操作，选择前100个左上角角点和100个右下角角点。测试图像采用0值填充方式得到指定大小作为网络的输入，而不是采用resize，另外同时测试图像的水平翻转图并融合二者的结果。最后通过soft-nms操作去除冗余框，只保留前100个预测框</p><h2 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a>ExtremeNet</h2><p>检测目标的4个极值点（即最上点、最下点、最左点、最右点）和一个中心点<br>需要group操作。针对CornerNet预测的角点经常落在目标外部，没有足够的目标特征改进。<br>backbone：Hourglass Network<br>heatmap：*5 4个极值点+1个中心点。<br>根据顶、底、左、右四个点集，从四个点集中各抽取一个得到四个极值，计算几何中心坐标，找到该中心坐标在中心点heatmap中的得分，如果高于阈值，那么这四个极值点组成的bounding box返回一个最终得分→五个点得分的平均值。<br>Ghost box抑制：解决比如要找2的中心，有可能找的是更大的这个框而不是2的box。如果box，其里面所有的包围框的得分超过了其本身得分的3倍→修正为原来的1/2</p><p>Edge aggregation：解决极值点不唯一问题。对于左边右边极值点在竖直方向聚合；顶部底部极值点在水平方向聚合。【沿着聚合方向，将第一个单调下降区间内的点的score按一定权重累加到原极值点上，并在达到局部最小值的时候停止聚合】</p><h2 id="Objects-as-Points【Centernet】"><a href="#Objects-as-Points【Centernet】" class="headerlink" title="Objects as Points【Centernet】"></a>Objects as Points【Centernet】</h2><p>只预测中心点。通过检测物体的中心点以及中心点对应的w,h来实现检测。不需要group操作<br>input：512<br>backbone: DLA 沙漏型<br>hourglass network 姿态检测<br>output：fm128 降了4倍 → 三个head<br>三个head：<br>1、    heatmap【功能是预测中心点】：128<em>128</em>class；同CornerNet<br>关于高斯圆的半径确定，iou overlap 情况讨论：三种情况的半径，预测的全覆盖；gt全覆盖；交错 overlap=0.7作为临界值，取最小值作为高斯核的半径R。<br>2、128<em>128</em>2；【对应location宽、高】使用L1<br>3、28<em>128</em>2；【中心点：细化调整 offset  x y】引入偏置的损失值，降4倍后取证会带来误差。<br>o是预测的偏移值数量 R表示Heatmap的缩放因子 p~是缩放后取证的坐标</p><p>为什么没有给每个类别预测宽高？考虑物体中心点不会重合<br>3D目标检测：<br>1、depth不好直接回归； 在特征点估计网络上添加了一个深度计算通道，L1 loss。参考文章：Depth map prediction from a single image using a multi-scale deep network.<br>2、l w h L1 loss<br>3、方向也很难回归；用两个bins来呈现方向→方向用8个标量值来编码的形式，每个bin有4个值。对于一个bin，两个值用作softmax分类，其余两个值回归到在每个bin中的角度。参考文章：3d bounding box estimation using deep learning and geometry.√</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="学习记录【目标检测】" scheme="https://betterwyl.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E3%80%91/"/>
    
  </entry>
  
</feed>
