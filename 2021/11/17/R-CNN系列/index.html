<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>R-CNN系列 （实习摸鱼中） · Betterwyl</title><meta name="description" content="来锐捷实习了，我每天的任务是写两个脚本，但奈何我。。。又不想虚度光阴，认真看看好了。这里伙食确实不错。
R-CNN 系列R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CV"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:170px;" alt="favicon"><h3 title=""><a href="/">Betterwyl</a></h3><div class="description"><p>The man is lazy, nothing left.  /  review&query</p></div></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/about">About me</a></li><li> <a href="/">Article List</a></li><li> <a href="/archives">Repository</a></li><li> <a href="/tags">Tags List</a></li><li> <a href="/guestbook">Guest book</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>R-CNN系列 （实习摸鱼中）</a></h3></div><div class="post-content"><p>来锐捷实习了，我每天的任务是写两个脚本，但奈何我。。。<br>又不想虚度光阴，认真看看好了。<br>这里伙食确实不错。</p>
<h1 id="R-CNN-系列"><a href="#R-CNN-系列" class="headerlink" title="R-CNN 系列"></a>R-CNN 系列</h1><h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-CVPR-2014" class="headerlink" title="R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)"></a>R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)</h2><p><strong>背景</strong>：首次用深度学习CNN的方式进行目标检测的尝试。之前都是使用传统提取特征，比较好的是SIFT和HOG。【看代码复习】<br><strong>创新点</strong>：<br>使用候选区域，与滑动窗口方法相比，CNN处理的图像窗口减少了两个数量级。<br>CNN提取特征：RCNN使用的CNN网络是AlexNet。<br>有监督pre-training+domain-specific finetuning处理label少的情况<br>使用bounding box regression进行修正【？具体】<br>目标检测：<br><strong>生成候选区域</strong>：<br>选择性搜索【一张图像生成约2K个候选区域 （Selective Search）】目的是为了改善传统提取特征方法中机从左到右、从上到下枚举式的低效。<br>Selective Search：过分割，将图像分成小区域；合并可能性最高的相邻两个区域<br><a href="http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a><br>【相关论文 颜色、纹理相近；尺度要均匀：不能大鱼吃小鱼；形状】；输出所有候选区域。</p>
<p>基于上述方法搜出的候选框是矩形的，而且是大小各不相同。为了要得到固定尺寸的图片输入到CNN中，进行缩放。作者比较了两种方法：1 各向异性缩放：不管比例 直接缩到227*227 ；2 各项同性缩放：将边界拓展成正方形 然后不在框里面的直接用框外的颜色均值代替填充；用固定背景颜色填充<br>本文：采用各向异性缩放、padding=16的精度最高</p>
<p>如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5就把这个候选框标注成物体类别，否则就是背景。<br><strong>特征提取</strong>：<br>对每个候选区域，使用深度卷积网络提取特征 （CNN）<br>比较：VGG和AlexNet。前者精度高但是计算量是后者的7倍。<br>物体检测的一个难点：物体标签训练数据少，若直接采用随机初始化CNN参数的方法，训练数据量是不够——采用的是有监督的预训练。<br>AlexNet原本是做图像分类任务，为做目标检测任务，替换掉AlexNet的最后一层的全连接层（4096 * 1000）。<br>1pre-train：采用了迁移学习的思想： ImageNet训练的CNN，先进行网络图片分类训练。<em>该数据库有大量的标注数据，共包含了1000种类别物体，预训练阶段CNN模型的output 1000个神经元。<br>网络优化：采用随机梯度下降法，学习速率大小为0.001<br>2fine-tune：在小型目标数据集（PASAC VOC）对上面得到的model进行改动。将模型的最后一层修改类别数。（20+1background）<br>RCNN的结构实际是5个卷积层、2个全连接层。<br>input: 2000</em> 227 * 227 * 3<br>output:  2000 * 4096 * 1<br>从每个候选区域中提取4096维特征向量。特征是通过前向传播通过五个卷积层和两个全连接层减去平均的224X224 RGB图像来计算的<br><strong>线性SVM分类器</strong>：<br>训练过的对应类别的SVM给特征向量中的每个类进行打分，每个类别对应一个二分类SVM。output: 2000*N（N目标的类别）作者测试了IOU阈值各种方案数值，通过训练发现，IOU阈值为0.3效果最好。IoU&gt;0.3的region proposal的特征向量作为正例，其余作为负例。<br>减少bbox：非极大值抑制法<br>测试时：2000×4096维特征与N个SVM组成的权值矩阵4096×N相乘，每一列即每一类进行非极大值抑制剔除重叠建议框</p>
<p>【CNN做特征提取（提取fc7层数据），再把提取的特征用于训练svm分类器原因：CNN容易过拟合，需要大量训练数据，因此CNN训练数据做了比较宽松的标注，一个bounding box正样本可能只包含物体的一部分，用于训练CNN。SVM适用于少样本训练，对于训练样本数据的IOU要求比较严格，只有当bounding box把整个物体都包含进去了，才把它标注为物体类别】<br><strong>Bounding Box 回归</strong>：<br>就是得到的候选框可能与ground truth相差比较大。解决：利用回归的方法重新预测了一个新的矩形框，借此来进一步修正bounding box的大小和位置<br>边界框回归是利用平移变换和尺度变换来实现映射。<br>使用相对坐标差：【比例值是恒定不变的；对坐标偏移量除以宽高即做尺度归一化：尺寸较大的目标框的坐标偏移量较大，尺寸较小的目标框的坐标偏移量较小】<br>IoU大于0.6时，边界框回归可视为线性变换：【log(1+x)/x  x趋近0 整个趋近于1】<br>AlexNet第5个池化层得到的特征即将送入全连接层的输入特征的线型函数。<br><strong>存在问题</strong>：R-CNN需要两次进行跑CNN model，第一次得到classification的结果，第二次才能得到(nms+b-box regression)bounding-box<br>三个模块（CNN特征提取、SVM分类和边框修正）是分别训练的，并且在训练的时候，对于存储空间的消耗大。检测速度慢，47s/per image。</p>
<h2 id="Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明"><a href="#Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-SPP-net-ECCV-2014-何恺明" class="headerlink" title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明"></a>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(SPP-net)ECCV 2014 何恺明</h2><p><strong>核心贡献</strong>：在R-CNN的基础上提出了空间金字塔变换层，速度、精度提升。<br>【CNN网络后面接的FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数，故需要固定长度的输入限制网络的输入大小<br>CNN网络会有大量的重复计算，造成的计算冗余】<br>R-CNN：输入需要对候选区域做填充到固定大小【对候选区域做填充缩放操作，可能会让几何失真、有冗余信息，这都会造成识别精度损失】;每个候选区域都要塞到CNN内提取特征向量【一张图片有2000个候选区域，也就是一张图片需要经过2000次CNN的前向传播，这2000重复计算过程会有大量的计算冗余，耗费大量的时间。】<br><img src="https://upload-images.jianshu.io/upload_images/3940902-0db3d0e4fa819bf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/444/format/webp" alt="比较"><br>SPP-net：针对候选框的重复计算部分，候选区域到全图的特征映射之间的对应关系。【直接获取到候选区域的特征向量，不需要重复使用CNN提取特征】<br>使用空间金字塔变换层将接收任意大小的图像输入，输出固定长度的输出向量。<br>【相当于从中间截断了，那前面的CNN可以接受不同尺寸图像了。都是为了保证全连接层的一个固定】<br>空间金字塔变换层<br>以不同的大小的bin块来提取特征的过程。【成列向量与下一层全链接层相连。这样就消除了输入尺度不一致的影响。】<br>不同大小侯选区域在feature map上的映射塞给SPP层<br>SPP layer分成1x1(塔底)，2x2(塔中)，4x4（塔顶）三张子图，对每个子图的每个区域作max pooling。输出都是(16+4+1) 每个块提取出一个特征21维特征向量。然后×256。【其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出神经元】</p>
<h2 id="Fast-R-CNN-CVPR-2015"><a href="#Fast-R-CNN-CVPR-2015" class="headerlink" title="Fast R-CNN(CVPR 2015)"></a>Fast R-CNN(CVPR 2015)</h2><p><strong>创新点</strong>：<br>SSP→RoI池化层：避免对每个候选区域提取特征，避免大量重复计算。<br>将分类与定位两大任务融入一个网络中来，获得了比R-CNN快的训练测试速度。边框回归直接加入到CNN网络中训练，损失部分采用多任务损失：<br><strong>方法</strong>：<br>利用选择性搜索获取图像中的推荐区域→将原始图片利用VGG16网络进行提取特征，之后把图像尺寸、推荐区域位置信息和特区得到的特征图送入RoI池化层，进而获取每个推荐区域对应的特征图。<br>input：待处理的整张图像；候选区域<br>网络分成两个并行分支，一个对推荐区域进行分类，一个对推荐区域的位置信息做预测。<br><img src="https://img-blog.csdn.net/20180527160553808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dlbnRlbHlhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="FastRCNN"><br><strong>RoI池化层</strong>:<br>【对SSP改进部分，SSP有不同尺度的特征图。这边简化了一下下采样到统一尺度】RoI只采用单一尺度进行池化→VGG16 后产生一个7×7×512维度的特征向量作为全连接层的输入【每个RoI区域的卷积特征分成4×4个bin，然后对每个bin内采用max pooling，这样就得到一共16维的特征向量。】<br>RoI pooling解决了SPP无法进行权值更新的问题。【？解答：SPP是将所有的特征图上的RoI保存下来，然后选择进行的网络微调，不会更新SSP layer之前的层，就相当于和前面的断开了，前面的特征图都不共享。Fast就是从输入到选择RoI都是同一批图，这样就能效率高的反向传播了。】<br>两个好处：将图像中的RoI区域定位到卷积特征中的对应位置；将这个对应后的卷积特征区域通过池化操作固定到特定长度的特征，然后将该特征送入全连接层<br>采用SVD对全连接层分解:<br>一张图像约产生2000个RoI，近一半的前向传递时间都花在计算全连接层上。SVD对全连接层进行变换来提高运算速度。截断SVD可以减少30%以上的检测时间，mAP只下降很小(0.3个百分点)。<br>一个大的矩阵可以近似分解为三个小矩阵的乘积，分解后的矩阵的元素数目远小于原始矩阵的元素数目，从而达到减少计算量的目的。→对全连接层的权值矩阵进行SVD分解。<br><strong>多任务的损失</strong><br>Fast R-CNN直接使用Softmax替代SVM分类，利用Softmax Loss 和Smooth L1 Loss对分类概率和边框回归联合训练<br>【Fast R-CNN网络主要有2个网络分支，一个网络分支负责输出推荐区域的分类概率，另一个网络分支负责输出每个推荐区域位置信息偏移量。】</p>
<h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><p><strong>RCNN FastRCNN总结对比</strong><br>二者的ROI区域生成：单独模块 选择性搜索<br>RCNN把分类和坐标分成两个网络，fastRCNN加上特征提取融合在一起了（Deepnet）。<br><strong>创新点</strong>：<br>使用RPN生成检测框，提高检测框生成速度。区域生成网络(RPN)+Fast RCNN<br>产生建议窗口的CNN和目标检测的CNN共享<br>核心方法：<br>使用卷积层提取特征图：<br>conv x13 relux13 poolingx4<br>conv层：kernel_size=3，pad=1，stride=1<br>对卷积进行填充：【相当于保持尺度不变 假设输入MxN 进来后(M+2)x(N+2) 那么3x3输出后还是MxN】<br>pooling层：kernel_size=2，pad=0，stride=2<br>【假设输入MxN 进来后(M+2)x(N+2) 那么2x2步长2输出后(M/2)x(N/2)→四次pool就变成了1/16 特征图上面密集的点对应到原始图像上面有16个像素的间隔】 最后conv5输出通道数有256（针对ZF ：VGG16是512-d,ZF是256-d）<br>【800/16 x 600/16=50 x 38 特征一共是50 x 38 x 256】</p>
<p><strong>RPN网络生成检测框</strong>：【相当于目标定位，二分类】<br>input: (M/16)x(N/16) 先经过一次3x3卷积 output: 50 x 38 x 256<br>分成两条线：<br>softmax判断正负样例<br>bb回归修正→proposals精准化<br>每一个点都负责原图中对应位置的9种尺寸框的检测→50 x 38 x 9 个anchor<br>【anchors多尺度方法：9个尺度，三种形状→长宽比1:1 1:2 2:1】<br>【可理解为在原图尺度上，设置了许多候选Anchor。通过CNN判断标记有目标的positive anchor和没目标的negative anchor】<br><strong>正负样本怎么划分</strong>：<br>1    对每个标定的ground truth区域，与其重叠比例最大的anchor记为正样本。【一个gt对应一个正样本】<br>2    剩余的anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本【每个ground truth可能会对应多个正样本anchor。但每个正样本anchor 只可能对应一个grand truth 一对多关系】。<br>如果其与任意一个标定的重叠比例都小于0.3，记为负样本。<br>3    剩余的anchor、跨越图像边界的anchor丢弃<br>计算anchor box与ground truth之间的偏移量：ground truth box与预测的anchor box之间的差异<br><strong>损失</strong>：rpn_loss_cls【softmax】、rpn_loss_bbox【smooth L1】、rpn_cls_prob【用于下一层的nms非最大值抑制操作】</p>
<p>p表示anchor i预测为物体的概率<br>p×正样本=1负样本=0 【回归只有在正样本时候才会被使用！】<br>t表示正样本anchor到预测区域的4个平移缩放参数<br>t×正样本anchor到Ground Truth的4个平移缩放参数</p>
<p>生成anchors →softmax分类器提取positvie anchors →bbox回归positive anchors →Proposal Layer生成proposals</p>
<p><strong>Roi Pooling</strong>：【共享信息】<br>input：<br>特征图和proposals 提取proposal feature maps→后续全连接层判定目标类别<br>候选框的特征图水平和垂直分为7份，对每一份进行最大池化处理。49维送入全连接层。【即使大小不一样的候选区，输出大小都一样，实现了固定长度的输出】<br>分类：<br>利用已经获得的proposal feature maps，通过全连接层与softmax计算每个proposal具体属于的类别，输出cls_prob概率向量；再次利用bounding box regression获得每个proposal的位置偏移量bbox_预测，用于回归更加精确的目标检测框。</p>
<p><strong>训练部分</strong>：<br>1    使用ImageNet模型初始化，独立训练一个RPN网络。<br>2    使用上面RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络。【两个网络每一层的参数完全不共享】<br>3    使用上面的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的学习率设置为0，仅仅更新RPN特有的那些网络层，重新训练。两个网络已经共享了所有公共的卷积层<br>4    仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个network，继续训练，微调 Fast-RCNN特有的网络层实现网络内部预测proposal并实现检测的功能。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-11-17</span><i class="fa fa-tag"></i><a class="tag" href="/tags/学习记录【目标检测】/" title="学习记录【目标检测】">学习记录【目标检测】 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2021/11/17/R-CNN系列/,Betterwyl,R-CNN系列 （实习摸鱼中）,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/12/25/yolo%E7%B3%BB%E5%88%97/" title="YOLO系列">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/10/26/anchorfree/" title="Anchor-free系列">Next</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>