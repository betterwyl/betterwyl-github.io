<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>小方法 · Yulin Wang</title><meta name="description" content="Norm方法学习Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:130px;" alt="favicon"><h3 title=""><a href="/">Yulin Wang</a></h3><div class="description"><p>The man is lazy, nothing left.</p></div></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">Article List</a></li><li> <a href="/about">About me</a></li><li> <a href="/archives">Repository</a></li><li> <a href="/tags">Tags List</a></li><li> <a href="/guestbook">Guest book</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>小方法</a></h3></div><div class="post-content"><h1 id="Norm方法学习"><a href="#Norm方法学习" class="headerlink" title="Norm方法学习"></a>Norm方法学习</h1><p>Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。<br>Batch Normalization的目的：使整个训练样本集feature map满足均值为0，方差为1的分布规律。【在图像输入的时候进行预处理会使得前面部分的网络接收到满足某一分布的特征矩阵】<br>理论上：计算出整个训练集的fm进行标准化处理，但是这样工作量很大。→针对batch进行处理，可以知道batch越大效果越好。<br>在训练过程中要去不断的计算每个batch的均值和方差，并使用移动平均(moving average)的方法记录统计的均值和方差，在训练完后我们可以近似认为所统计的均值和方差就等于整个训练集的均值和方差。然后在我们验证以及预测过程中，就使用统计得到的均值和方差进行标准化处理。<br>计算的feature map每个维度（channel）的均值和方差。<br><a href="https://img-blog.csdnimg.cn/20200221215813522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200221215813522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70</a></p>
<p><a href="https://img-blog.csdnimg.cn/20200226145423805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200226145423805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70</a></p>
<p>在视觉领域，其实最常用的还是BN，但BN也有缺点，通常需要比较大的Batch Size。<br>Group Normalization<br>batch size的大小对GN并没有影响，所以当batch size设置较小时，可以采用GN<br>和batch_size无关，我们直接看对于一个样本的情况。假设某层输出得到x，沿channel方向均分成num_groups份，也是对每一份求均值和方差。</p>
<h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><p>pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。<br>pooling目的是为了保持某种不变性（旋转、平移、伸缩等）<br>常用的有mean-pooling，max-pooling，Stochastic-pooling：<br>mean-pooling，正向传播对邻域内特征点只求平均，优缺点：能很好的保留背景，但容易使得图片变模糊。反向传播特征值根据领域大小被平均，然后传给每个索引位置<br>max-pooling，正向传播对邻域内特征点取最大并记住最大值的索引位置，以方便反向传播能更好的保留纹理信息。反向传播：将特征值填充到正向传播中，值最大的索引位置，其他位置补0。<br>Stochastic-pooling：只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大。接着按照概率值来随机选择，一般情况概率大的，容易被选择到，比如选择到了概率值为0.3的时候，那么（1，2，3，4）池化之后的值为3。使用stochastic pooling时(即test过程)，其推理过程也很简单，对矩阵区域求加权平均即可，比如上面图中，池化输出值为：1×0.1+2×0.2+3×0.3+4×0.4=3。在反向传播求导时，只需保留前向传播已经记录被选中节点的位置的值，其它值都为0,</p>
<p> global average pooling（全局平均池化）：全局平均池化一般是用来替换全连接层。在分类网络中，全连接层几乎成了标配，在最后几层，feature maps会被reshape成向量，接着对这个向量做乘法，最终降低其维度，然后输入到softmax层中得到对应的每个类别的得分，过多的全连接层，不仅会使得网络参数变多，也会产生过拟合现象，针对过拟合现象，全连接层一般会搭配dropout操作。而全局平均池化则直接把整幅feature maps（它的个数等于类别个数）进行平均池化，然后输入到softmax层中得到对应的每个类别的得分。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-09-10</span><i class="fa fa-tag"></i><a class="tag" href="/tags/学习记录【深度学习tips】/" title="学习记录【深度学习tips】">学习记录【深度学习tips】 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2021/09/10/DL贴士/,Yulin Wang,小方法,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/09/23/%E8%BD%A6%E9%81%93%E7%BA%BF%E8%AE%BA%E6%96%87%E6%96%BD%E5%B7%A5%E4%B8%AD/" title="车道线论文阅读 （施工多年版本）">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/08/17/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95/" title="传统方法">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>