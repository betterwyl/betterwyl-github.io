<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>YOLO系列 · Betterwyl</title><meta name="description" content="———————————————————2021-12-25更新————————————————
YOLOv3: An Incremental Improvement 技术报告    backbone Darknet-19→Darknet-53 精度速度权衡【该全卷积网络是没有全连接层的 也是其可以"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Betterwyl</a></h3><div class="description"><p>The man is lazy, nothing left.  /  review&query</p></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div><div class="beian"><a href="http://www.beian.miit.gov.cn/" target="_blank"></a><span style="height:10px;margin-left: 10px;">|</span><img src="/images/gongan.png" style="height:10px;margin-left: 10px;position: relative;top: 1px;"><span style="margin-left: 2px;"></span></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/about">关于</a></li><li><a href="/guestbook">留言</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>YOLO系列</a></h3></div><div class="post-content"><p>———————————————————2021-12-25更新————————————————</p>
<h2 id="YOLOv3-An-Incremental-Improvement-技术报告"><a href="#YOLOv3-An-Incremental-Improvement-技术报告" class="headerlink" title="YOLOv3: An Incremental Improvement 技术报告"></a>YOLOv3: An Incremental Improvement 技术报告</h2><p>    backbone Darknet-19→Darknet-53 精度速度权衡<br>【该全卷积网络是没有全连接层的 也是其可以兼容任意尺度图像的原因→FC层需要固定的输入大小：FC层在设计时就固定了神经元的个数】<br>兼容任意尺度图像→输入要是32倍数<br>怎么变成416？强行缩放、加灰边</p>
<p>小目标性能提高<br>多尺度融合 ：不同尺度anchor 每个尺度3个anchor<br>255是85（coco80类+xywhc+边框置信度）×3（anchor）<br>【下采样32倍：13×13×255每一个代表原图32。】<br>【FPN特征金字塔】<br>concat：拼接 【上采样2倍完26×26+原先下采样26×26的拼接。厚度是不一样的。】<br>损失函数<br>正样本：与gt 的IOU最大的那个anchor 所在尺度的grid cell去负责【与之前区别 不看中心点落在哪个gridcell里面】<br>注意的是：如果高于某个阈值的话那就不是正样本而是不参与，小于某个阈值的话那就是负样本。<br>正样本的坐标【遍历所有cell 还有anchor】<br>+正样本的置信度和类别（标签为1）使用二元交叉熵损失函数BCE<br>+负样本置信度（标签为0）<br>测试：1、计算预测框信息2、设置阈值，去掉得分低的3、多分类nms重叠大的消除</p>
<h2 id="YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><p>网络结构：<br>backbone Darknet-53→CSP Darknet-53<br>生成特征图后再合在一起作为下一个网络输入【厚度越来越大了】<br>特征提取网络改进 SPP和PANet<br>Bag of freebies<br>数据增强：1像素级别：处理光度失真【调整图像的亮度、对比度、色调、饱和度和噪声】几何失真【添加了随机缩放、裁剪、翻转和旋转】2假设遮挡：随机融合或者使用零像素值3style transfer GAN 风格迁移<br>不同类别之间存在数据不平衡：单阶段Focal loss双阶段难例挖掘<br>本论文使用：<br>Mosaic混合了四张训练图片<br>SAT：【不太理解。？网上：它在前后两阶段上进行操作。在第一阶段，神经网络代替原始的图片而非网络的权重。用这种方式，神经网络自己进行对抗训练，代替原始的图片去创建图片中此处没有期望物体的描述。在第二阶段，神经网络使用常规的方法进行训练，在修改之后的图片上进检测物体。】<br>网络正则化的方法：Dropout、Dropblock<br>CIOU：将覆盖区域，中心点距离和纵横比考虑在内。<br>Bag of specials 需要增加推理的过程<br>增大感受野 SPP【本文使用】 ASPP RFB<br>注意力机制 SE SAM【将空间注意改成点注意】<br>特征集成方法ASFF BiFPN PANET【本文使用 是一种拼接 不是相加!!】<br>激活函数【代码可以直接调用库】MISH ReLU leaky-ReLU parameter-ReLU ReLU6 SELU<br>后处理方法：soft NMS【物体的遮挡】、DIoU NMS【添加中心距离的信息】<br>通过均值和方差对网络激活进行归一化【本论文使用CmBN】<br>理解：这篇文章讨论和验证了目标检测中每个部分的多种策略，对这些策略进行选择和部分改进，然后组合在一起，基于yolov3去改动，比较各模块选择什么方法能让目标检测性能最优化。有注重对各模块在单GPU上的性能比较。</p>
<h2 id="Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）"><a href="#Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining（CVPR2016）" class="headerlink" title="Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）"></a>Training Region-based Object Detectors with Online Hard Example Mining（CVPR2016）</h2><p>abstract关键词：类别不平衡<br>自动选择这些困难的样例可以使训练更加有效和高效<br>【意思是之前的region proposals生成正负比例不平衡。假如说10个里面9个负样本，那直接判别负样本正确率非常高。】<br>VOC2007和2012中的mAP分别为：78.9%，76.3%<br>一个概念：启发式超参数搜索 【使用循环神经网络来生成参数→在训练过程中不变的数 当然会增加时间复杂度】<br>原因：<br>Fast R-CNN允许更新整个卷积网络 SPP net、MR-CNN固定住卷积网络、也没使用SVM。<br>train部分：<br>在Fast R-CNN SPP net  MR-CNN把RoI与真实框的交叉比(IOU)大于等于0.5即判定为目标RoI。本文去掉小于等于的设置背景的。<br>Fast R-CNN在一个 mini-batch中，它们之间的比例是1：3。本文中是去掉了这个比例。<br>没有采用设定背景和目标样本数的比例方式处理数据的类别不平衡【如果哪个类别不平衡，那么这个类别的损失就会比较大，这样被采样的可能性也比较大】<br>方法：<br>    之前使用的方法：<br>只选出那些 hard negative 的样本进行训练→Hard Negative Mining Method<br>SVM + Hard Negative Mining Method<br>基于SVM及检测器训练时需要分类器对样本进行分类，把其中错误分类的样本放入负样本集合再继续训练分类器直到模型收敛。<br>困难样本挖掘。【会比都是简单的效果好。】<br>缺陷：端到端难以操作；需要迭代训练的时候又另外腾出时间来生成这种hard negative，每迭代几次就固定模型一次，速度慢。<br>一般用svm才会用这个方法。但是fastrcnn和fasterrcnn都没使用。→想另外的方法<br>    本文：<br>SGD回传 对样本进行一个重新选择【选择困难样本或者对困难样本赋予更高的权重】<br>SGD是以mini-batch为单位来更新模型的：对于每个mini-batch，先从数据集中取N张，然后每张图片采样B/N个RoIs<br>重合率比较大的ROI之间的损失也比较相似→解决办法：使用了 NMS(非最大值抑制) 算法：把损失按高到低排序→选择最高的损失→计算其他 ROI与这个 ROI的 IoU→移除 IoU 大于一定阈值的 ROI，然后反复上述流程直到选择了 B/N 个 ROIs。<br>    提出一个是修改loss层：【缺点：将没选择的ROI的loss设置为0。但是这种做法并不高效，因为即便很多ROI的loss都是0，也就是不需要更新梯度，但是这样仍需要给每个ROI都分配存储空间，并且每个ROI都需要后向传播】</p>
<p>    两个相同的 ROI网络，一个只可读【前向传递的时候分配空间】，另一个可读可修改【同时为前向和后向分配空间】。<br>经过ROI plooling层生成feature map，然后进入只读的ROI network得到所有ROI的loss；然后是hard ROI sampler结构根据损失排序选出hard example，并把这些hard example作为第二个ROI network的输入。</p>
<h2 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection-（YOLO-V1）" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）"></a>You Only Look Once: Unified, Real-Time Object Detection （YOLO V1）</h2><p>abstract关键词：<br>视作回归问题。<br>一个单一的神经网络预测bbox和类概率。<br>由于整个检测pipeline是一个单一的网络，可以直接对检测性能进行端到端的优化。<br>快！每秒45帧。实时 是之前其他实时物体检测系统mAP的两倍以上<br>避免背景错误，产生false positives。<br>【对比之前的：是通过region proposal产生大量的可能包含待检测物体的bounding box，再用分类器去判断每个 bounding box里是否包含有物体，以及物体所属类别的概率。分开处理较难优化】<br>核心思想：<br>视为回归问题。利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别<br>流程：<br>input：resize图像到448 * 448 →网络→NMS<br>Unified Detection<br>栅格 各管各的<br>image→S*S的栅格 每个栅格负责检测中心落在该栅格中的物体<br>每一个栅格预测B个bounding boxes&amp;置信度得分<br>x y w h IOU+C【conditional class probability在一个栅格包含一个Object的前提下，它属于某个类的概率】<br>conditional class probability信息是针对每个栅格的。<br>confidence信息是针对每个bbox的。<br>【上述两者相乘：包含bounding box中预测的class的 probability信息，也反映了bounding box是否含有Object和bounding box坐标的准确度】<br>一个图 ：S×S×(B×5+C)<br>网络设计：<br>24个卷积层和2个全连接层<br>【卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。】</p>
<p>YOLO借鉴GoogLeNet分类网络结构。没使用inception module。使用 1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>训练：<br>Pretrain网络：上述网络中的前20 个卷积层+average-pooling layer+全连接层【ImageNet 1000-class的分类任务数据集】<br>Pretrain的结果的前20层卷积层应用到检测中，+剩下的4个卷积层及2个全连接。<br>损失函数：<br>如果一些栅格中没有物体，那么就会将这些栅格中的bounding box的confidence 置为0，相比于较少的有物体的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。</p>
<p>坐标预测：xywh<br>含有物体的bbox IOU预测 confidence<br>不含有物体的bbox IOU预测 confidence<br>类别预测：Class 有没有中心落在网格中 有就预测概率的意思。</p>
<p>看起来可以直接用7×7×30理解。30前20个代表的是预测的种类，后10代表两个预测框及其置信度(5x2) 7*7<br>每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box predictor来负责预测【当前哪一个predictor预测的bounding box与ground truth box的IOU最大，这个 predictor就负责物体检测→每个predictor可以专门的负责特定的物体检测→训练后预测效果更好】</p>
<p>局限性：<br>YOLO对小物体的检测效果不好【小物体，因为一个栅格只能预测2个物体。而且小物体IOU影响较大】<br>YOLO容易产生物体的定位误差</p>
<h2 id="YOLO9000-Better-Faster-Stronger-CVPR-2017"><a href="#YOLO9000-Better-Faster-Stronger-CVPR-2017" class="headerlink" title="YOLO9000: Better, Faster, Stronger (CVPR 2017)"></a>YOLO9000: Better, Faster, Stronger (CVPR 2017)</h2><p>abstract关键词：<br>保持原有速度，精度上提升<br>一种目标分类与检测的联合训练方法：YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测<br>改进措施：<br>    Batch Normalization：批处理规范化<br>每一个卷积层后添加batch normalization，通过这一方法，mAP获得了2%的提升。batch normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。<br>    High Resolution Classifier：<br>在ImageNet上对448×448上的分类网络进行了10次微调。我们在检测时对产生的网络进行微调。模型在检测数据集上finetune之前已经适用高分辨率输入。这种高分辨率的分类网络mAP增加了近4%。<br>    Convolutional With Anchor Boxes：<br>YOLOv1采用的是：全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体。【YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差】<br>YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。【RPN预测的是边界框相对于先验框的偏移】YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。每个位置的各个anchor box【注重形状】都单独预测一套分类概率值。【影响：mAP有稍微下降】</p>
<p>input： 416×416<br>特征图大小：13×13【维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。】<br>最初的YOLO输入尺寸为448×448，加入anchor boxes后，输入尺寸为416×416。模型只包含卷积层和pooling 层，因此可以随时改变输入尺寸。<br>作者在训练时，每隔几轮便改变模型输入尺寸，以使模型对不同尺寸图像具有鲁棒性。每个10batches，模型随机选择一种新的输入图像尺寸（320,352,…608，32的倍数，因为模型下采样因子为32），改变模型输入尺寸，继续训练。<br>Dimension Clusters<br>维度聚类【k-means聚类方法】。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标。5种大小的box维度来进行定位预测，这与手动精选的box维度不同。结果中扁长的框较少，而瘦高的框更多【就和尺度没关系了】125=5×25<br>Direct location prediction<br>直接目标框预测【不是预测偏移了 每个anchor 检测周围】</p>
<p>细粒度特征拥有较细粒度特征的层变形【？】<br>多尺度训练：训练Yolo v2时不固定image size，而是每训练10个epochs随机选取【32倍数】<br>网络：New Network: Darknet-19（特征提取器）<br>19个卷积层和5个maxpooling层 3×3 卷积层 【其中使用1×1卷积层】 2×2maxpooling<br>改进：方法wordtree</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-12-25</span><i class="fa fa-tag"></i><a class="tag" href="/tags/学习记录【目标检测】/" title="学习记录【目标检测】">学习记录【目标检测】 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2021/12/25/yolo系列/,Betterwyl,YOLO系列,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2022/07/29/range%20view/" title="range-view">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/11/17/R-CNN%E7%B3%BB%E5%88%97/" title="R-CNN系列 （实习摸鱼中）">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>