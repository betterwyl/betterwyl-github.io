<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>range-view · Betterwyl</title><meta name="description" content="源于看了一篇基于rangeview的mot论文 觉得range的方法可操作性强且耕耘不多。
————————————————2022-10-2更新——————————————————
Range RCNN: Towards Fast and Accurate 3D Object Detection "><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:200px;"><h3 title=""><a href="/">Betterwyl</a></h3><div class="description"><p>The man is lazy, nothing left.  /  review&query</p></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/about">About me</a></li><li> <a href="/">Article List</a></li><li> <a href="/archives">Repository</a></li><li> <a href="/tags">Tags List</a></li><li> <a href="/guestbook">Guest book</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>range-view</a></h3></div><div class="post-content"><p>源于看了一篇基于rangeview的mot论文 觉得range的方法可操作性强且耕耘不多。</p>
<p>————————————————2022-10-2更新——————————————————</p>
<h2 id="Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation"><a href="#Range-RCNN-Towards-Fast-and-Accurate-3D-Object-Detection-with-Range-Image-Representation" class="headerlink" title="Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation"></a>Range RCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation</h2><p>创新点：<br>RV-PV-BEV；<br>dilated convolution 2d【residual block】；<br>two-stage RCNN【解决bev视图没有高度的问题】<br>Input：range image 【编码距离、坐标、强度→输入距离图像5 × h × w】<br>处理：Kitti3D相机前视图中的目标【90°场景 5 × 48 × 512】；waymo：64×2650<br>扩张残余块（DRB）<br>问题产生：尺度变化感知不清【使用range image送入到2Dcnn】。<br>将扩张卷积插入正常残差块：【代码中有三个不同的扩张率。最后用1×1融合】</p>
<p>RV-PV-BEV<br>问题是：Range image会重叠，因此特征提取要在BEV中进行。适应不同尺度。<br>流程是：Range image 上记录特征点对应投影到BEV平面，相同的点使用平均池化。<br>【如果在开始时投影，BEV将作为主要特征提取器。】</p>
<p>3D RoI Pooling<br>问题是：range or bev都无法明确地学习3D边界框高度方向上的特征。<br>解决方案是：使用相对位置进行高度上的编码。<br>【固定数量、不同的栅格包含目标的不同部分。由栅格的空间关系，信息可通过相对位置编码。】此处有个将三维全部展平为一维然后连接的处理。<br>损失函数：</p>
<p>Kitti数据集处理</p>
<p>Waymo：</p>
<p>固定栅格：12×12×12  128 proposals with a 1:1 ratio<br>优势： [30，50]m和[50，75]m  运行速度为22 fps<br>我的想法：range view的方法照道理快一些【pvrcnn 12fps】，猜测是没有分割出前景点会不会在BEV部分计算量大【关注：前景点分割方法→影响速度】。相对于rangedet其结构还是比较复杂 。</p>
<h2 id="RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection"><a href="#RangeDet-In-Defense-of-Range-View-for-LiDAR-based-3D-Object-Detection" class="headerlink" title="RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection"></a>RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection</h2><p>ICCV2021 from 中科院自动化所&amp;图森<br>代码：<a href="https://github.com/TuSimple/RangeDet" target="_blank" rel="noopener">https://github.com/TuSimple/RangeDet</a><br>和BEV视图的区别，信息稠密的更有效利用【体现在NMS的改进】，近大远小会有尺寸变化【体现在MKC和改进型FPN】<br>创新点：<br>1Meta-Kernel Convolution<br>2 Range Conditioned Pyramid Assignment<br>3 Weighted NMS<br>Meta-Kernel Convolution【解决使用卷积会把密集信息忽略的问题，比如在一个rangeimage中，两个点靠近，而他们实际的距离可能很远，那就丢掉了这部分信息】</p>
<p>Range Conditioned Pyramid Assignment<br>【使用ResNet中的BasicBlock将其卷积核替换了一下此处有点不理解，结合blog】Meta-Kernel Convolution：将卷积核的权重变得可调整。使用中心点与邻域点的差值，放大了检测了属于点和点之间的特征差异。】<br>依据：距离范围的远近：近距离label 局部特征图；远距离的label分配全局的特征图。【思想FPN金字塔】<br>Weighted NMS<br>每个pixel都会预测一个box，一个truth可以被很多pixel预测。那不能全部删掉，而是采用较高score的框进行加权。<br>作者使<br>我的思考：该论文具体问题具体分析提供改进的方法。挺巧妙的。【3D→2D的这样逆过程可以使用一些2D的方法，从而来实现一些2D中已经实现任务，在分割、跟踪上？】</p>
<h2 id="【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021"><a href="#【LMNet】Moving-Object-Segmentation-in-3D-LiDAR-Data-A-Learning-based-Approach-Exploiting-Sequential-Data-RAL2021" class="headerlink" title="【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021"></a>【LMNet】Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data  RAL2021</h2><p>代码：<a href="https://github.com/PRBonn/LiDAR-MOS" target="_blank" rel="noopener">https://github.com/PRBonn/LiDAR-MOS</a><br>数据集：SemanticKITTI<br>创新点：将rangeview用于mos任务；使用了时域上的信息，即残差；<br>Input：<br>3D LiDAR 扫描生成的range image+残差图像【当前帧和先前帧之间的距离的残差→d 是关于r的】<br>r将第k帧的点云旋转至当前帧l第i个像素上的距离值<br>d 归一化表示</p>
<p>Output：当前帧中的一个标签范围 【红的表示移动物体】<br>流程：3D点云序列的投影图+残差图像<br>1投影公式<br>2使用到SLAM中获得过去时间序列的雷达信息→残差计算【T代表着每个序列的相对变换 传感器得到？有点不理解来源 数据集里的吗】</p>
<p>结合SLAM读数【其实这边有点不懂，SLAM知识缺失】和残差图像→现有的分割网络通过利用残差图像中的时间信息来区分运动物体和背景上的像素。二进制表示<br>如何将上述两步信息融合 【就是整套需要变换和重新投影流程】<br>1之前的扫描序列转化为当前的2重新投影到当前范围视图3计算距离</p>
<p>CNN结构【现成的】<br>使用三个网络比对RangeNet++ MINet SalsaNext<br>指标IOU：移动物体<br>实验部分：<br>在slam上添加噪声测试其稳定性<br>【自己生产自己比较，当前较少该方向的成果，作者结合了之前方法重新做实验比较】<br>semantic segmentation  SalsaNext【这个不太理解，加了一堆东西】<br>scene flow 对平移向量设置阈值判断是否移动<br>我的思考：该论文引入与之前帧的残差作为一个特征，若物体移动较慢很可能识别为静物。能不能设置区间或者其他方法放大时间上的信息，【改进点：针对速度小的物体分割效果提升】。一个问题：相对本车静止，他实际上也是运动的。【回答：在实际场景 趋势显露】<br>LiDAR 的 MOS 的实现并不多 ？实验比较的依据可靠性</p>
<h2 id="Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet"><a href="#Efficient-Spatial-Temporal-Information-Fusion-for-LiDAR-Based-3D-Moving-Object-Segmentation-IROS2022工作基于LMNet" class="headerlink" title="Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet"></a>Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation  IROS2022工作基于LMNet</h2><p>代码：<a href="https://github.com/haomo-ai/motionseg3d" target="_blank" rel="noopener">https://github.com/haomo-ai/motionseg3d</a><br>数据集： KITTI+自己标注<br>创新点：<br>1双分支结构，【两部分变成并联结构，使用SalsaNext，在此基础上add】。<br>2解决range-view信息没有有效利用问题：加了Meta-Kernel Module<br>通过中心点的相对坐标计算 3×3 邻域的权重，然后使用 1×1Conv 聚合邻域特征来更新中心特征。<br>这么做的目的是：细化结果，并减少对象边界周围出现的伪影。<br>其他tricks：加注意力机制；减少resblock<br>由一个用于编码外观特征的距离图像分支和一个用于编码时间运动的残差图像分支<br>网路LOSS：每个类别频率交叉熵和 【The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-<br>union measure in neural networks,” in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2018】pixel和point都可以用的<br>我的思考：顾名思义，双分支。这篇文章对LMNet的改进可以理解为串联改并联了。同时，他用了一个没见过的损失函数。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-07-29</span><i class="fa fa-tag"></i><a class="tag" href="/tags/学习记录【3D】/" title="学习记录【3D】">学习记录【3D】 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2022/07/29/range view/,Betterwyl,range-view,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2022/11/11/3D%E6%A3%80%E6%B5%8B/" title="3D检测">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/12/25/yolo%E7%B3%BB%E5%88%97/" title="YOLO系列">Next</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>