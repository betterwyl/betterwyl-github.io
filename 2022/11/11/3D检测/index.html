<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>3D检测 · Betterwyl</title><meta name="description" content="————————————————2022-11-11更新——————————————————在军军老师发的综述里看到的interest in：形状学习策略与不利天气、遮挡和截断等导致的点云质量恶化相结合
Behind the Curtain: Learning Occluded Shapes for"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.png"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:200px;" alt="favicon"><h3 title=""><a href="/">Betterwyl</a></h3><div class="description"><p>The man is lazy, nothing left.  /  review&query</p></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="mailto:yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> “from 2020” </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>3D检测</a></h3></div><div class="post-content"><p><p>————————————————2022-11-11更新——————————————————<br>在军军老师发的综述里看到的<br>interest in：<br>形状学习策略与不利天气、遮挡和截断等导致的点云质量恶化相结合</p>
<h2 id="Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】"><a href="#Behind-the-Curtain-Learning-Occluded-Shapes-for-3D-Object-Detection【AAAI2022】" class="headerlink" title="Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】"></a>Behind the Curtain: Learning Occluded Shapes for 3D Object Detection【AAAI2022】</h2><p>数据集：KITTI<br>分析形状缺失的情况<br>外部遮挡；信号缺失【这边视为能观察到的，只是丢了】，表现为空相素【我的理解是过于稀疏】；自遮挡，远侧部分被近侧遮挡【每个物体都不可避免地会发生自遮挡导致的形状错失】；<br><strong>创新点：</strong><br>BtcDet是第一个针对受遮挡影响的对象形状的3D对象检测器<br>Complete object shapes= observed objects shapes ∪ the occluded object shapes<br>根据估计的占用概率P（OS）进行目标检测<br>形状占用网络<br>assemble the approximated complete shapes<br>假设：<br>大多数前景对象类似于数量有限的形状原型；【处理：利用source point来补全target】<br>H(A,B) →源B是否覆盖目标A的大部分区域 最终A形状=源B中得分最高的Top3+目标A原始的点云。源对象来自训练集的其他帧。<br>车辆和骑自行车者，大致对称【处理：借助标记的边界框，镜像补充它们】<br>识别observed objects shapes ∪ the occluded object shapes【是在球面坐标系中完成】<br>使用均匀间隔的球形网格对点云进行体素化，以便可以通过任何LiDAR点后面的球形体素精确地形成遮挡区域。【就是这个S可能会存在很多voxel中】非空体素若有包含形状S则为1，会形成一个occupancy 的概率。<br>用了一个占有率网络<br>Occlusion-Aware Proposal Refinement【plus】<br>将占有率投影到RPN的各层feature中→feature拼接→ROI pooling→优化的bbox</p>
<p>PIXOR ing</p>
<p>————————————————2022-6-26更新——————————————————<br>MOT论文</p>
<h2 id="MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries"><a href="#MUTR3D-A-Multi-camera-Tracking-Framework-via-3D-to-2D-Queries" class="headerlink" title="MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries"></a>MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries</h2><p>代码：<a href="https://github.com/a1600012888/MUTR3D" target="_blank" rel="noopener">https://github.com/a1600012888/MUTR3D</a>  nuScenes数据集<br>端到端的多相机3D跟踪框架【解决问题：多相机进行3D跟踪时，会出现检测精度降低、复杂场景中的遮挡和模糊、边界对象丢失】<br>创新点：模拟一个对象的整个轨迹的3D状态，关联【空间和外观相似性】对象到3D轨迹中。<br>Metric：评估当前3D跟踪器中的运动模型：平均跟踪速度误差(ATVE)和跟踪速度误差(TVE)。可以测量被跟踪物体的估计运动的误差</p>
<p>自回归的方式逐帧更新自身→解码器头从每帧中的每条轨迹查询中预测一个候选对象，并且在来自同一轨迹查询的不同帧中解码的预测被直接关联→<br>损失：新查询和旧查询<br>新真值目标作为查询的回归目标，在新生查询的候选目标之间执行匹配。旧查询：先前帧的活跃查询。、跟踪当前帧中之前出现的目标，它在第一次成功检测到真值目标后被分配。<br>查询是有生命周期的，在代码中设置阈值。【查询更新：使用来自历史帧的特性来更新跟踪查询。】</p>
<p>两个指标：TVE是在MOTA最高的召回时的平均速度误差【可作为当前3D 跟踪器中运动模型的质量评价】</p>
<h2 id="Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving"><a href="#Time-3D-End-to-End-Joint-Monocular-3D-Object-Detection-and-Tracking-for-Autonomous-Driving" class="headerlink" title="Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving"></a>Time 3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving</h2><p>数据集：nuScenes 3D<br>创新点：3D单目Detection和3D MOT一体；异构线索整合【编码外观 几何特征】<br>信息跨帧传播、估计相似度以生成三维轨迹、整合世界坐标系中的几何相对关系以估计速度、属性和框平滑度优化。<br>1单目检测方法：KM3D+其他检测头平行的Re-ID头<br>2异构线索整合：对外观、几何和运动信息的兼容表示进行了编码<br>3时空信息流：？？？？<br>空间信息流：3D探测器的主中心头提取图像中的中心点，+外观特征和几何特征， MLP层连接以生成其输入。<br>时间信息流模块→多头交叉注意力？<br>Loss：<br>单目3D检测损失LMono3D、跟踪损失Ltracking和时间一致性损失LCons</p>
<h2 id="MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection"><a href="#MonoDETR-Depth-guided-Transformer-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection"></a>MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</h2><p>创新点：去除中心限制，基于深度信息引导的三维物体检测方法<br><a href="https://github.com/ZrrSkywalker/MonoDETR" target="_blank" rel="noopener">https://github.com/ZrrSkywalker/MonoDETR</a><br>数据集：KITTI3D</p>
<p>特征提取：<br>视觉：多尺度特征图 下采样8 16 32<br>深度：下采样16。两个 3×3 卷积。对象的同一 2D 框内的像素分配有对象的相同深度标签。对于同时在多个框内的像素，选择离相机最近的物体的深度标签。<br>视觉编码器3块，深度编码器1块。<br>每个编码器块由一个自注意力层和一个前馈神经网络 (FFN) 组成</p>
<p>前景深度图方法：参考Categorical depth distribution network for monocular 3d object detection. 深度离散化为 k + 1 个 bin，其中第一个 bin 表示前景深度，最后一个表示背景。线性递增离散化（LID）【更远物体的深度估计固有地会产生更大的误差。[dmin, dmax]】</p>
<p>Depth-guided Decoder【不需要引入额外的深度估计的监督，3D GT boxes的 Z 值→深度值。估计深度属于某一范围得概率，然后根据其属于某一范围得索引返回连续的深度值。】<br>检测头：<br>在深度引导变换器之后，深度感知对象→基于 MLP 的头中以进行 3D 属性估计。在推理过程中，输出3D 边界框。分别计算每个查询的损失，将无序查询与真实对象标签匹配。<br>损失：<br>六个属性损失：两组。L2D 和 L3D。<br>第一组外观：对象类别、2D 大小和投影的 3D 中心。<br>第二组3D 空间属性：由深度、3D 大小和方向组成。【。】【在训练开始时，网络通常预测的 3D 属性不如 2D 属性准确，L3D 的值不稳定会干扰匹配过程。只使用 L2D 作为匹配每个查询标签对的匹配成本。】</p>
<p>————————————————2022-3-23更新——————————————————</p>
<h2 id="SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation"><a href="#SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation" class="headerlink" title="SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation"></a>SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</h2><p>核心内容：单个关键点估计与回归3 d 变量相结合，预测每个被检测物体的3d包围盒<br>在图像平面上估计投影的三维点。并行地增加一个三维参数回归分支。</p>
<p>损失函数： 3 d 盒子的8个角点【包含一个分类和回归分支。】分离了每个参数在三维包围盒编码阶段和回归损失函数中的贡献</p>
<p>当前方法：会较依赖于（rcnn 、rpn）+一个生成伪点云深度<br>缺点是：多阶段具有复杂性、引入噪声<br>改进方向有：几何推理、数据合成、3d-2d后处理</p>
<p>骨干网：DLA-34| 特点：实现深层聚合<br>采用这个网络的改进： bn替换为gn<br>3D检测网络：<br>关键点分支:物体由关键点表示【投影三维中心，图像中的8个点左乘内参矩阵】<br>回归分支:待回归的参数全部编码成差的方式去学习</p>
<p>损失函数<br>分类损失cls_loss：采用带惩罚因子的Focal Loss</p>
<p>———————————————2021-11-14更新————————————————————</p>
<h2 id="GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving"><a href="#GS3D-An-Efficient-3D-Object-Detection-Framework-for-Autonomous-Driving" class="headerlink" title="GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving"></a>GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</h2><p>作者：香港中文大学；商汤科技 王晓刚团队  CVPR2019<br>数据集：kitti<br>解决的问题是：<br>1仅使用二维边界框进行特征提取时，由于信息缺失，会出现表示模糊的问题。<br>【一个二维框有多种三维盒子表示。矛盾在于：相同特征输入又要分类器给出不同置信度；残差损失难估计】2 改进回归残差的loss 残差分类<br>创新点：<br>    在2D检测基础上，高效地获得3D长方体的方法【包含了大小方向的粗信息，该长方体即 “guidance”】<br>    提取3D盒子可见表面的潜在的3D信息，解决仅使用二维盒子的特征时的特征模糊问题。<br>    改进方法：考虑质量感知损失的离散分类方法比直接回归方法具有更好的精度。<br>流程：<br>image→CNN检测器→2D box &amp; orientation【2D框检测和方向预测】→3D guidance→guidance投影【2D框和可见表面提取特征】→3Dsubnet→refined 3D box【细化】<br>2D+O<br>3D box尺寸估计：汽车的尺寸具有低方差和单峰的特性。可以人为设定尺寸，初始化检测出3D box尺寸。<br>物体3D盒的顶部中心在2D平面上有一个稳定的投影，非常接近2D边界盒的顶部中点，而3D底部中心有一个类似的稳定投影，位于2D边界盒的上方和附近。<br>3d box 在相机坐标系下的粗计算：</p>
<p>高h估算已知，归一化后已知。因此d深度已知。<br>【d相当于一个比例系数现在，可以算出真实的三维空间坐标 3dbox 中心=0.5×（Cb+Ct）】</p>
<p>【这些以后可以得到一个粗位置。】<br>表面特征提取：<br>对α的角度进行讨论。前后左右会改变。上面必看到。<br>对可见的三个面做仿射变换，在指定特征层提取三个面特征加上2d box提取特征融合，最终特征回归真实的3d box。<br>残差回归改进为分类公式，细化3d box。【将残差值划分区间，计算标准差，以标准差为刻度，作为一个区间划分标志。】区间问题看成多重的二元分类问题。【意思是：如果有一个2Dbox 他不能和gt匹配→证明说他在区间概率为0。置信度低，那就是一个背景】<br>移位特征部分：【每个残差区间用最相关的投影特征？？？】<br>质量感知损失<br>分类预测的置信度能反映对应类的目标框的质量，目标框越准确得分越高。<br> 重合高的得分高</p>
<p>实验部分：<br>该方法和DEEP3DBOX方法【学习记录V4】相比，后者的AP高。该方法在2D检测上性能没有调到最好，核心工作是在3D部分。和Mono3D【特征很多很复杂的那篇。陈晓智√学习记录V4】、3DOP【双目的】、DeepMANTA比较。<br>分析：<br>该方法不擅长处理图像边界上的对象(通常带有遮挡或截断)。1利用了表面特征中潜在的三维结构信息，消除了仅使用二维边界框所带来的表示模糊。2残差回归问题重新表述为分类。利用质量意识损失增强模型的识别能力。4没有任何额外的数据或标签进行训练。</p>
<h2 id="Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image"><a href="#Deep-MANTA-A-Coarse-to-fine-Many-Task-Network-for-joint-2D-and-3D-vehicle-analysis-from-monocular-image" class="headerlink" title="Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image"></a>Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</h2><p>作者：一些外国人 研究方向是目标检测的           CVPR2017<br>创新点：<br>    定位车辆部件，特征点→可以预测隐藏部分的位置，23D之间的匹配，恢复3D车辆信息<br>    由粗到细的多任务卷积神经网络。借鉴RPN 网络产生2Dbox 。output：六个任务共享特征向量：region proposal , detection , 2D box regression , part location , part visibility , 3D template prediction.<br>    对图中的3D框的车辆的其他信息打标签</p>
<p>3D形状和模板数据集<br>数据集很庞大：103个汽车模型，每一个汽车模型都有36个特征部件的三维坐标，还有一个1*3数组代表长宽高信息。<br>真实的输入图像的每一个车辆建模 【里面有超多信息的】<br>在2D 和3D中的表示【框和部件】。V是可见性【是否遮挡啥的】<br>核心方法：Deep MANTA网络部分</p>
<p>从粗糙到精细化的前向传播：【均基于第一个卷积层的feature maps输入】使用三个网络【第一个网络就是RPN；第二三个网络？有点不懂】→结果更精确【原因：1克服大的物体大小变化，提供更高的准确度；2保持高分辨率，用于检测难以检测的车辆；】<br>多任务预测：第三个网络：每一个bounding box，同时输出其对应的：S，V，T【怎么能一下子输出这么多东西，奇怪？？？】 NMS去除冗余边框。<br>网络推断部分：利用一个就是前面的Deep MANTA网络的输出+3D部件数据+3D模板数据【方法：从103中遍历一个最相似的+23D匹配】<br>损失部分：<br>LOSS：【需要最小化五个损失函数】三个子网络<br>net1：LRPN<br>net2 | net3：+L检测损失函数 【分类损失（是车还是背景）+边框回归损失】<br>net2 | net3：+L特征部件定位<br>net3：+Lvis【车辆部件可见性】<br>net3：+L 模板相似度<br>【可以看出net3 Loss多：越往后越精细的意思吧】<br>半自动打标签：需要一个3D CAD的车辆模型数据集。作者手动标记了每一个车辆模型相应的N = 36个特征部件点的位置，并连线得到车辆模型的特征轮廓。对于每一个真实车辆的3D边框，通过算法计算出跟车辆模型的3D边框相似性。<br>选中最高的进行映射【该车辆模型及相应的特征部件点和轮廓的位置都可以映射到原图像真实车辆的位置去。】<br>实验部分：<br>数据集：KITTI<br>CNN：GoogLenet和VGG16。【预先用ImageNet数据集训练得到初始参数。VGG16效果较好】<br>NET1：在RPN网络中作者使用了7种长宽比，10种倍数作为参数。【该特征图上每一个点都可以产生70个anchor box。】<br>对比：【指标：AOS和AP】<br>第一个是不使用特征改进，且和RPN一样使用最后一层低分辨率的特征图的模型（类似于原先的RPN）<br>第二个是不使用特征改进，但是使用第一次卷积后的特征图的模型<br>第三个是Deep MANTA<br>3D定位正确性：ALP 通过阈值判定法<br>3D模板预测，特征部件定位，可见性评估：也是设定一些阈值。</p>
<h2 id="3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020"><a href="#3DSSD-Point-based-3D-Single-Stage-Object-Detector-2020" class="headerlink" title="3DSSD: Point-based 3D Single Stage Object Detector 2020"></a>3DSSD: Point-based 3D Single Stage Object Detector 2020</h2><p>文章主要内容：点云数据进行单阶段3D检测的模型，速度快。<br><a href="https://github.com/dvlab-research/3DSSD" target="_blank" rel="noopener">https://github.com/dvlab-research/3DSSD</a><br>这篇文章的代码写得太好了，很清楚。<br>前人研究：点云数据投影到图像中、体素表达 缺点是会丢失信息。<br>现在研究：直接用点云数据<br>分析了这个PointRCNN的问题【3D Object Proposal Generation and Detection from Point Cloud<br>这篇文章还没看，但很重要的样子。】第一阶段 获取proposal (SA&amp;FP)前景点 第二阶段 refinement。【要解决耗时长的问题，特别是第一阶段，去除 FP层和第二阶段】<br>F-FPS方法和D-FPS方法<br>SA层的下采样步骤中用到了D-FPS方法【这个方法结果基本覆盖了整个场景，缺点就是前景点少了。→F-FPS前景点多+范围大。有利于回归】 SA中会进行特征聚合。【点和点周围的聚合，背景过少，不利于分类】。<br>→两个方法融合</p>
<p>CG layer【是对SA模块的变形：只用F-FPS中的提取的点作为初始中心点，初始中心点移动到它们对应的实例中→候选点。】预测头之前加入额外层来提取特征。三个步骤：中心点选择、周围点提取和语义特征生成。点集D-FPS 和F-FPS 的点将它们的归一化位置和语义特征group起来作为输入→mlp 层提取特征。<br>Anchor-Free回归：【2d中的centernet 根据关键点】<br>每个候选点到对应实例的距离、实例大小以及角度<br>Anchor-based方法缺点：要生成的anchor太多啦。要设计很多大小和方向的框【2d中的SSD、frcnn都是】<br>centerness对齐策略：1、点是不是在目标里。2、点到六个面距离，一个公式。 →1×2<br>【点云的点都在目标表面，他们的中心标记将非常的小且相似，很难从其他点中得出好的预测】<br>损失函数：分类、回归和shift损失。</p>
<p>基于模板匹配 ：打分</p>
<h2 id="3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection"><a href="#3DOP-3D-Object-Proposals-using-Stereo-Imagery-forAccurate-Object-Class-Detection" class="headerlink" title="3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection"></a>3DOP  3D Object Proposals using Stereo Imagery forAccurate Object Class Detection</h2><p>文章主要内容：输入Stereo图像对作为来估计深度，将图像平面中的像素坐标重新投影回3D空间来计算点云。该方法优势：召回率高，能给出准确率高的对象框。<br>提出一个生成候选的能量最小化函数。能量最小化函数由 对象大小、地面、深度信息特征【点云密度、到地面的距离、有无遮挡、free space可行驶的区域？】<br>3Dbbox中圈出高密度的点云区域。限制：点云不能垂直延伸到bbox外，且这个bbox附近点云高度要矮一些。</p>
<p>KITTI图像包含许多小对象、严重遮挡、高饱和区域和阴影。不适用之前的目标检测网络。<br>能量函数各个分量含义：<br>pcd：box内体素是不是有点云。【相当于一个比例，这边很奇怪 难道不是box内点云比例越大越好吗？也许可以这么理解，尽可能把他们框起来，如果box全部都有说明box小了】<br>fs：可行驶空间不能有box 如果一个体素是在box内 那么肯定不会在可行驶区域内【最小化盒子内部的可行驶区域】<br>Height Prior：box内点云高度越接近这个对象的平均高度。要这个体素内有点云才参与计算。<br>Height Contrast：表示包围box附近的点云的高度应低于box内点云的高度。【定义附近的概念：+0.6m】<br>根据这个能量最小化生成2000个框 +nms+贪婪算法 得到最终box<br>地面估计：RANSAC【迭代的方式从一组包含离群的被观测数据中估算出数学模型的参数】将平面拟合到估计的地面像素来估计地面。<br>通过SVM来学习权重【这边有点看不明白】，使用IOU作为损失函数。<br>目标检测和方向估计网络<br>基于Fast R-CNN在最后一个卷积层之后添加一个上下文分支和一个方向回归损失来扩展这个基本网络，以共同学习对象的位置和方向。<br>平滑L1损失进行方向回归。</p>
<h2 id="Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving"><a href="#Stereo-R-CNN-based-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="Stereo R-CNN based 3D Object Detection for Autonomous Driving"></a>Stereo R-CNN based 3D Object Detection for Autonomous Driving</h2><p>文章主要内容：检测关联左右图像中的对象，基于关键点【mask r cnn】和box约束的3Dbox估计+密集3D盒子法(让他更精确)。<br>基于Faster R-CNN的工作。在本文中通过评估多比例特征地图上的锚来修改金字塔特征的原始RPN【Similar with FPN 意思是这边采用多个feature map】。<br>问题：RPN和FPN对比。<br>之前知道Faster R-CNN 是由Fast R-CNN+RPN构成的。一张图通过RPN获得一堆proposal，【proposal都是在原图上画个框，映射到一个feature map，池化变成统一尺度，之后再做分类和回归。】<br>在FPN中，对多个feature map分别做分类和回归，获得到一堆proposal【feature map这次有多张图】 </p>
<p>双目的RPN过程<br>左右特征图concat起来。目标是左右的并集，和anchor IOU大于0.7 IOU小于0.3定义正负标签。<br>FPN在Faster R-CNN中FPN产生的每一个尺度的feature map都要送进RPN做一次proposal的提取。<br>六个【我觉得论文这边写错了 这样好奇怪哩 应该是两个uv坐标offset + wh回看之前的是一个点坐标+wh 一共是四个 】<br>左右候选框都是由同一个 anchor 生成，共享类别置信度得分，它们就可以一一对应起来。我们在左右 RoI上分别使用NMS，选取最高的2000个候选框用于训练。测试时，选取300个候选框。</p>
<p>在RPN之后→RoiAlign的操作→ 获取FPN的左和右featuremap →concat相应的特征→fc层得到对象类别、立体边界框、维度和角度【使用θ表示车辆相对于摄像机框架的方向，使用β表示物体相对于摄像机中心的方位 sin/cos值】<br>keypoint的检测。Mask R-CNN的结构进行关键点的预测。4个3D keypoint，即车辆底部的3D corner point，同时将这4个点投影到图像，得到4个keypoint 起到一定约束作用。<br>3D框恢复。是从2D的框和关键点来恢复的。7个点 left左上右下 right左右 +keypoint：推导<br>Dense 3D Box Alignment：最小化左右视图refine？【这个地方有点不理解，先跳过】</p>
</p><div class="tip">honest&brave girl<br>作者: Yulin,Wang</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-11-11</span><i class="fa fa-tag"></i><a class="tag" href="/tags/学习记录【3D】/" title="学习记录【3D】">学习记录【3D】 </a><span class="leancloud_visitors"></span><span>大约5945个字, 19分钟49秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2022/11/11/3D检测/,Betterwyl,3D检测,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2022/07/29/range%20view/" title="range-view">下一篇</a></li></ul></div><script src="/js/visitors.js"></script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script></body></html>