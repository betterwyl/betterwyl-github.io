<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>Graph Transformer · Yulin Wang</title><meta name="description" content="Graph TransformerPositional and structural encodings位置编码（PE）意在提供给定节点在空间中的位置的信息。如果两个节点在图或子图中的位置相邻，那么其位置编码应该相近。通常做法是计算一对节点之间的距离，或特征向量。
Local PE-node fea"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:130px;" alt="favicon"><h3 title=""><a href="/">Yulin Wang</a></h3><div class="description"><p>The man is lazy, nothing left.</p></div></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">Article List</a></li><li> <a href="/about">About me</a></li><li> <a href="/archives">Repository</a></li><li> <a href="/tags">Tags List</a></li><li> <a href="/guestbook">Guest book</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Graph Transformer</a></h3></div><div class="post-content"><h1 id="Graph-Transformer"><a href="#Graph-Transformer" class="headerlink" title="Graph Transformer"></a>Graph Transformer</h1><h2 id="Positional-and-structural-encodings"><a href="#Positional-and-structural-encodings" class="headerlink" title="Positional and structural encodings"></a>Positional and structural encodings</h2><p><strong>位置编码（PE）</strong>意在提供给定节点在空间中的位置的信息。如果两个节点在图或子图中的位置相邻，那么其位置编码应该相近。通常做法是计算一对节点之间的距离，或特征向量。</p>
<p>Local PE-node features-Within a cluster: 1、随机游走矩阵非对角元求和  2、节点到聚类中心距离</p>
<p>Global PE-node features -Within a graph  ：1、Eigenvectors of the Adjacency, Laplacian or distance matrices  2、与graph中心的聚类。3、连通成分的独特定义 4、SignNet  </p>
<p>Relative PE-edge features  ：1、成对距离：最短路径、核、随机游走、贪心等 或者使用全局or局部PE 。2、PEGlayer：位置编码→用于连接预测任务</p>
<p><strong>结构编码（SE）</strong>旨在提供一个图结构或子图结构的嵌入，以增强GNN的表达能力和泛化能力。如果两个节点拥有相同的子图结构，亦或两个图相似，那么它们的结构编码也应该相似。</p>
<p>Local SE-node features-m radius 【划定半径范围、相当于子图】：1、度2、随机游走矩阵对角元3、预定义一些结构</p>
<p>Global SE-graph features  【两张图相似则globalSE也相似】</p>
<p>Relative SE-edge features  【和Local SE相关的边的嵌入】</p>
<p><strong>1-Weisfeiler-Leman test</strong></p>
<p>Weisfeiler-Lehman算法在大多数图上会得到一个独一无二的特征集合，这意味着图上的每一个节点都有着独一无二的角色定位（例外在于网格，链式结构等等）。因此，对于大多数非规则的图结构，得到的特征可以作为图是否同构的判别依据——WL Test（例如两个图是否是同质的，取决于节点的排列方式）。</p>
<h2 id="Rethinking-Graph-Transformers-with-Spectral-Attention"><a href="#Rethinking-Graph-Transformers-with-Spectral-Attention" class="headerlink" title="Rethinking Graph Transformers with Spectral Attention"></a>Rethinking Graph Transformers with Spectral Attention</h2><p>可学习的位置编码</p>
<h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><h3 id="Multi-Headed-Attention-MHA"><a href="#Multi-Headed-Attention-MHA" class="headerlink" title="Multi-Headed Attention (MHA)"></a>Multi-Headed Attention (MHA)</h3><p>1、 Prepare for multi-head attention：每个head 的vector表示做线性变换。</p>
<p>forward：</p>
<p>input： <code>[seq_len, batch_size, d_model]</code> or <code>[batch_size, d_model]</code></p>
<p>在最后一个维使用线性变换，分为多个heads</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.linear &#x3D; nn.Linear(d_model, heads * d_k, bias&#x3D;bias)</span><br><span class="line">head_shape &#x3D; x.shape[:-1] #最后一个维为：d_model</span><br><span class="line">x &#x3D; self.linear(x)</span><br><span class="line">x &#x3D; x.view(*head_shape, self.heads, self.d_k)</span><br></pre></td></tr></table></figure>

<p>2、MultiHeadAttention</p>
<p>每一个q、k、v都输入到preparationmultiheadattention的module中取准备映射数据。</p>
<p>计算分数：query和key</p>
<p>为每个head计算mask：[seq_len_q, seq_len_k, batch_size]</p>
<p>forward：input：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input：[query: torch.Tensor,key: torch.Tensor,value: torch.Tensor,mask: Optional[torch.Tensor] &#x3D; None]</span><br><span class="line">query、key和value的原始数据形式 [seq_len, batch_size, d_model]→[seq_len, batch_size, heads, d_k]</span><br><span class="line">计算query和key相乘的得→[seq_len, seq_len, batch_size, heads] .</span><br></pre></td></tr></table></figure>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-08-27</span><i class="fa fa-tag"></i><a class="tag" href="/tags/Workshop/" title="Workshop">Workshop </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2023/08/27/Graph-Transformer-learning/,Yulin Wang,Graph Transformer,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2023/10/15/RAGNN/" title="NAGNN">Post Anterior</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2023/06/14/radarwork/" title="My Work">Próximo post</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>