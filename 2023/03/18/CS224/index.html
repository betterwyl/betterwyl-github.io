<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Yulin,Wang"><title>CS224W · Betterwyl</title><meta name="description" content="1.Introduction1.1intro图的表示学习：大致来说就是将原始的结点（或链接、或图）表示为向量（嵌入embedding），图中相似的结点会被embed得靠近（指同一实体，在结点空间上相似，在向量空间上就也应当相似）。
1.2Applications of Graph MLnode le"><meta name="keywords" content="Recording"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:170px;" alt="favicon"><h3 title=""><a href="/">Betterwyl</a></h3><div class="description"><p>The man is lazy, nothing left.  /  review&query</p></div></div></div></div><ul class="social-links"><li><a href="https://github.com/betterwyl" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li><li><a href="yulinwang@mail.ustc.edu.cn"><i class="fa fa-envelope"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2020 </span><i class="fa fa-star"></i><span> Yulin,Wang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/about">About me</a></li><li> <a href="/">Article List</a></li><li> <a href="/archives">Repository</a></li><li> <a href="/tags">Tags List</a></li><li> <a href="/guestbook">Guest book</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>CS224W</a></h3></div><div class="post-content"><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><h3 id="1-1intro"><a href="#1-1intro" class="headerlink" title="1.1intro"></a>1.1intro</h3><p>图的表示学习：大致来说就是将原始的结点（或链接、或图）表示为向量（嵌入<strong>embedding</strong>），图中相似的结点会被embed得靠近（指同一实体，在结点空间上相似，在向量空间上就也应当相似）。</p>
<h3 id="1-2Applications-of-Graph-ML"><a href="#1-2Applications-of-Graph-ML" class="headerlink" title="1.2Applications of Graph ML"></a>1.2Applications of Graph ML</h3><p>node level：</p>
<p>protein folding：给定氨基酸序列计算预测蛋白质的3D结构。模拟蛋白质位置，预测3Dshape。</p>
<p>edge level：</p>
<p>recommender systems ：pinsage图像+图结构，更好的推荐 ；任务目标是使相似结点嵌入之间的距离比不相似结点嵌入之间的距离更小。</p>
<p>药物组合副作用：预测引擎，蛋白质相互作用网络，预测缺失的边缘。</p>
<p>背景：很多人需要同时吃多种药来治疗多种病症。任务：输入一对药物，预测其有害副作用。</p>
<p>subgraph level：</p>
<p>Google map: traffic prediction测一段路程的长度、耗时等：将路段建模成图，在每个子图上建立预测模型</p>
<p>graph level：</p>
<p>用GNN的图分类任务来从一系列备选图（分子被表示为图，结点是原子，边是化学键）中预测最有可能是抗生素的分子；</p>
<p>预测材料变形：用GNN来预测粒子的下一步活动（组成一个新位置、新图</p>
<h3 id="1-3design-choice："><a href="#1-3design-choice：" class="headerlink" title="1.3design choice："></a>1.3design choice：</h3><p>Bipartite Graph→Folded/Projected Bipartite Graphs</p>
<p>Representing Graphs：Edge list【难表示】；Adjacency list【对图的分析和操作更方便】</p>
<p>结点和边的属性：Weight (e.g., frequency of communication)；Ranking (best friend, second best friend…)；Type (friend, relative, co-worker)；Sign: Friend vs. Foe, Trust vs. Distrust；Properties depending on the structure of the rest of the graph: Number of common friends</p>
<p>Self-edges (self-loops)自环 / Multigraph![](/屏幕截图 2023-04-03 153338.png)</p>
<p>multigraph有时也可被视作是weighted graph，就是说将多边的地方视作一条边的权重（在邻接矩阵上可看出效果是一样的）。但有时也可能就是想要分别处理每一条边，这些边上可能有不同的property和attribute</p>
<p>Connectivity</p>
<p><img src="/image-20230403192951728.png" alt="image-20230403192951728"></p>
<p>connected：任意两个结点都有路径相通 strongly connected components：互相可以访问</p>
<p>disconnected：由2至多个connected components构成。<br>最大的子连接图：giant component<br>isolated node<br>这种图的邻接矩阵可以写成block-diagonal的形式，数字只在connected components之中出现。【聚类中的块对角个数=类别数】</p>
<h2 id="2-Feature-Engineering-for-ML-in-Graphs"><a href="#2-Feature-Engineering-for-ML-in-Graphs" class="headerlink" title="2.Feature Engineering for ML in Graphs"></a>2.Feature Engineering for ML in Graphs</h2><p>传统pipelines【2 steps】：设计并获取所有训练数据上结点/边/图的特征→分类器/模型→新结点作出预测</p>
<p>Features: d-dimensional vectors<br>Objects: Nodes, edges, sets of nodes, entire graphs<br>Objective function: What task are we aiming to solve?</p>
<h3 id="2-1Node-Level-Feature"><a href="#2-1Node-Level-Feature" class="headerlink" title="2.1Node Level Feature"></a>2.1Node Level Feature</h3><p>1.Importance-based features考虑了结点的重要性；</p>
<p>2.Structure-based features捕获节点附近的拓扑属性</p>
<p><strong>eigenvector centrality</strong>：如果结点邻居重要，那么结点本身也重要。与link的数量无关，与link的重要性有关。</p>
<p><img src="/image-20230403200310561.png" alt="image-20230403200310561"></p>
<p><strong>betweenness centrality</strong>：桥梁！如果一个结点处在很多结点对的最短路径上，那么这个结点是重要的。</p>
<p><img src="/image-20230403200532146.png" alt="image-20230403200532146"></p>
<p><strong>closeness centrality</strong>：交通枢纽！到其他结点的路径长度最小。越居中越短。</p>
<p><img src="/image-20230403200844882.png" alt="image-20230403200844882"></p>
<p><strong>clustering coefficient</strong>聚类系数：邻居的联系程度</p>
<p>第1个例子：6 / 6<br>第2个例子：3 / 6<br>第3个例子：0 / 6 </p>
<p><img src="/image-20230403204703668.png" alt="image-20230403204703668"></p>
<p><strong>三角形/三元组</strong>：结点自我网络。共友也会互相认识。</p>
<p>三角形可以拓展到某些预定义的子图pre-specified subgraph</p>
<p><strong>graphlets</strong></p>
<p>2-5个节点的graphlets，可以得到一个长度为73个坐标coordinate的向量GDV，描述该点的局部拓扑结构topology of node’s neighborhood，可以捕获距离为4 hops的互联性interconnectivities。<br>相比节点度数或clustering coefficient，GDV能够描述两个节点之间更详细的节点局部拓扑结构相似性local topological similarity。</p>
<p><img src="/image-20230403213451852.png" alt="image-20230403213451852"></p>
<h3 id="2-2Link-Level-Feature"><a href="#2-2Link-Level-Feature" class="headerlink" title="2.2Link Level Feature"></a>2.2Link Level Feature</h3><p>link level prediction：根据网络现有的link预测新的link</p>
<p>eg基于相似性：算两点间的相似性得分（如用共同邻居衡量相似性），然后将点对进行排序，得分最高的n组点对就是预测结果，与真实值作比</p>
<p>测试阶段：评估无edge的node pairs→rank k→prediction</p>
<p>链接预测任务的两种类型：</p>
<p>随机缺失边【第一种假设可以以蛋白质之间的交互作用举例，缺失的是研究者还没有发现的交互作用。新链接的发现会受到已发现链接的影响。在网络中有些部分被研究得更彻底，有些部分就几乎没有什么了解，不同部分的发现难度不同】；随时间推演边【以社交网络举例，随着时间流转，人们认识更多朋友。】</p>
<p><strong>distance-based feature</strong></p>
<p><img src="/image-20230403215243135.png" alt="image-20230403215243135"></p>
<p><strong>local neighborhood overlap</strong></p>
<p><img src="/image-20230403215255078.png" alt="image-20230403215255078"></p>
<p>common neighbors： 度数高将表现结果更高。</p>
<p>Adamic-Adar index：有点像信息熵，度数低的共友比名人共友价值大。</p>
<p>如果没有共友的情况，则值为0。【缺点：两个点未来仍有可能被连接起来。相距两跳的结点、没有邻居的结点？解决方案global neighborhood overlap】</p>
<p><strong>global neighborhood overlap</strong></p>
<p>获得一对结点的得分。</p>
<p>Katz index：计算node pairs之间所有长度路径的数量 → 对邻接矩阵求幂</p>
<p>邻接矩阵：代表u和v之间长度为1的路径的数量</p>
<p>分解路径：将长度为1转化，通过桥接方法。对起始节点和相邻结点求和，到目标结点。【归纳法】</p>
<p><img src="/image-20230403220039044.png" alt="image-20230403220039044"></p>
<p>观察其对角线，其实就是邻居的数量。</p>
<p><img src="/image-20230403220152827.png" alt="image-20230403220152827"></p>
<p>比较长的距离以比较小的权重。</p>
<p>2.3Graph Level Feature </p>
<p><strong>核方法</strong>：</p>
<p><img src="/image-20230403223205107.png" alt="image-20230403223205107"></p>
<p>eg. BoW for graph：将图表示成一个向量，每个元素代表对应something出现的次数（这个something可以是node, degree, graphlet, color）node degrees</p>
<p><img src="/image-20230404113506849.png" alt="image-20230404113506849"></p>
<p>eg. Graphlet kernel【侧重于node 和 edge，即结构 】直接点积两个图的graphlet count vector得到相似性。对于图尺寸相差较大的情况需进行归一化。计算代价高，k-size对应n^k</p>
<p><img src="/image-20230404113417281.png" alt="image-20230404113417281"></p>
<p>eg. Weisfeiler-Lehman Kernel ：建立特征描述子，邻域结构叠代结点度。</p>
<p>color refinement：</p>
<p><img src="/image-20230404114000690.png" alt="image-20230404114000690"></p>
<p>把邻居颜色聚集起来，使用hash，重新标记颜色。【优化是线性的】</p>
<p>进行K次迭代，整个迭代过程中颜色出现的次数作为Weisfeiler-Lehman graph feature。上述向量点积计算相似性，得到WL kernel。</p>
<h2 id="3-Node-Embeddings"><a href="#3-Node-Embeddings" class="headerlink" title="3.Node Embeddings"></a>3.Node Embeddings</h2><h3 id="3-1encoder-decoder"><a href="#3-1encoder-decoder" class="headerlink" title="3.1encoder-decoder"></a>3.1encoder-decoder</h3><p>graph representation learning：学习到图数据用于机器学习的、与下游任务无关的特征，我们希望这个向量能够抓住数据的结构信息。</p>
<p>将图数据紧密地嵌入到嵌入空间中→自动对结构信息进行编码用于下游任务。</p>
<p>eg. node embedding：deep walk</p>
<p><strong>Encoder and Decoder</strong>：对结点进行编码，保证其在嵌入空间的相似性。在二维的相似性【例如距离】，可反应图的结构。</p>
<p>Encoder：将节点映射为embedding<br>定义一个衡量节点相似度的函数（如衡量在原网络中的节点相似度）<br>Decoder DEC：将embedding对映射为相似度得分。</p>
<p><img src="/image-20230404122425624.png" alt="image-20230404122425624"></p>
<p>ENC(v)=Z · v </p>
<p><img src="/image-20230404123051920.png" alt="image-20230404123051920"></p>
<p>→ Z(d,v)每个节点都有一个列来表示该结点的嵌入。</p>
<p>→v是一个其他元素都为0，对应节点位置的元素为1的向量。</p>
<p>【缺陷：参数过多，难scale up到大型图；Z的大小取决于结点的数量】</p>
<p>DEC：节点相似的不同定义：边；邻居；相似的structural roles</p>
<p>random walks：无监督/自监督→使用结点embedding时，并没有使用结点的label和功能，目标只学习了网络的相似性。</p>
<h3 id="3-2Random-Walk-Approaches-for-Node-Embeddings"><a href="#3-2Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="3.2Random Walk Approaches for Node Embeddings"></a>3.2Random Walk Approaches for Node Embeddings</h3><p>goal：每个结点的嵌入向量z</p>
<p>随机游走的相似性：使用概率表示，从结点u开始随机走动</p>
<p><img src="/image-20230404141113888.png" alt="image-20230404141113888"></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-03-18</span><i class="fa fa-tag"></i><a class="tag" href="/tags/Course/" title="Course">Course </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://betterwyl.github.io/2023/03/18/CS224/,Betterwyl,CS224W,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2023/04/02/debug%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/" title="coding指南 2">Post Anterior</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2022/12/02/radar/" title="Radar">Próximo post</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'xHB6JST92sP5ORC9WNZ7DwXX-gzGzoHsz',
  app_key:'cqHNrhvssKtJ6KzZLln1xtVv',
  placeholder:'...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>